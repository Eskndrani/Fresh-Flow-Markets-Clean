{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Testing Notebook\n",
        "\n",
        "This notebook tests ML models from `New_ML_Models` on test data from the `Testing` folder.\n",
        "\n",
        "## Models to Test:\n",
        "1. **stock_forecaster**: LSTM (time series)\n",
        "2. **revenue**: XGBoost (time series)\n",
        "3. **customer_churn**: Random Forest (classification)\n",
        "4. **campaign**: Random Forest Classifier (classification only)\n",
        "\n",
        "**Note:** Operational Risk testing has been moved to `operational_risk_test.ipynb`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
            "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
            "c:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import json\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    mean_squared_error, mean_absolute_error, r2_score\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Stock test data: 238 rows\n",
            "✓ Revenue test data: 879 rows\n",
            "✓ Customer churn test data: 575 rows\n",
            "✓ Campaign test data: 129 rows\n"
          ]
        }
      ],
      "source": [
        "# Load all test datasets\n",
        "test_data_dir = \"D:/Deloitte/Testing/\"\n",
        "\n",
        "# Load test files\n",
        "stock_test = pd.read_csv(f\"{test_data_dir}Stock_Forecasting_Test.csv\")\n",
        "revenue_test = pd.read_csv(f\"{test_data_dir}revenue_prediction_test.csv\")\n",
        "customer_churn_test = pd.read_csv(f\"{test_data_dir}customer_churn_test.csv\")\n",
        "campaign_test = pd.read_csv(f\"{test_data_dir}campaign_success_test.csv\")\n",
        "\n",
        "print(f\"✓ Stock test data: {len(stock_test)} rows\")\n",
        "print(f\"✓ Revenue test data: {len(revenue_test)} rows\")\n",
        "print(f\"✓ Customer churn test data: {len(customer_churn_test)} rows\")\n",
        "print(f\"✓ Campaign test data: {len(campaign_test)} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Test Stock Forecaster (LSTM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Stock Forecaster LSTM model...\n",
            "✓ Loaded 10 LSTM models\n",
            "\n",
            "✓ Stock forecaster tested on 238 samples\n",
            "  Predictions made: 238\n"
          ]
        }
      ],
      "source": [
        "# Load stock forecaster model\n",
        "print(\"Loading Stock Forecaster LSTM model...\")\n",
        "\n",
        "model_dir = \"D:/Deloitte/New_ML_Models/stock_forecaster/models/lstm_models/\"\n",
        "scaler_dir = \"D:/Deloitte/New_ML_Models/stock_forecaster/models/scalers/\"\n",
        "mse_path = \"D:/Deloitte/New_ML_Models/stock_forecaster/MSE.json\"\n",
        "\n",
        "# Load category mapping\n",
        "with open(\"D:/Deloitte/New_ML_Models/stock_forecaster/full_menu_mapping.json\", 'r', encoding='utf-8') as f:\n",
        "    category_mapping = json.load(f)\n",
        "\n",
        "# Load models and scalers\n",
        "lstm_models = {}\n",
        "scalers = {}\n",
        "categories = [\n",
        "    \"Beverages\", \"Breakfast_&_Brunch\", \"Desserts_&_Sweets\", \n",
        "    \"Handhelds\", \"Main_Courses\", \"Misc_Services\", \n",
        "    \"Other_Uncategorized\", \"Salads_&_Greens\", \n",
        "    \"Sides_&_Snacks\", \"Sushi_&_Asian\"\n",
        "]\n",
        "\n",
        "for cat in categories:\n",
        "    model_path = f\"{model_dir}{cat}_lstm.h5\"\n",
        "    scaler_path = f\"{scaler_dir}{cat}_scaler.joblib\"\n",
        "    if os.path.exists(model_path) and os.path.exists(scaler_path):\n",
        "        lstm_models[cat] = load_model(model_path, compile=False)\n",
        "        scalers[cat] = joblib.load(scaler_path)\n",
        "\n",
        "# Load MSE errors\n",
        "with open(mse_path, 'r', encoding='utf-8') as f:\n",
        "    mse_errors = json.load(f)\n",
        "\n",
        "print(f\"✓ Loaded {len(lstm_models)} LSTM models\")\n",
        "\n",
        "# Test stock forecaster\n",
        "def predict_stock(category_name, month, last_qty):\n",
        "    \"\"\"Predict stock quantity using LSTM\"\"\"\n",
        "    if category_name not in lstm_models:\n",
        "        return None\n",
        "    \n",
        "    model = lstm_models[category_name]\n",
        "    scaler = scalers[category_name]\n",
        "    \n",
        "    # Scale input\n",
        "    qty_scaled = float(scaler.transform([[last_qty]])[0][0])\n",
        "    \n",
        "    # Prepare features for LSTM\n",
        "    features = np.array([[month, qty_scaled]])\n",
        "    features = np.reshape(features, (1, 1, features.shape[1]))\n",
        "    \n",
        "    # Predict\n",
        "    prediction_scaled = model.predict(features, verbose=0)\n",
        "    prediction = float(scaler.inverse_transform(prediction_scaled)[0][0])\n",
        "    \n",
        "    return max(0, prediction)\n",
        "\n",
        "# Test on stock test data\n",
        "stock_results = []\n",
        "for idx, row in stock_test.iterrows():\n",
        "    month = int(row['month']) if pd.notna(row['month']) else 1\n",
        "    lagged_qt = row['lagged_qt'] if pd.notna(row['lagged_qt']) else 0\n",
        "    \n",
        "    # Try to find category (default to first available)\n",
        "    category = list(categories)[0]  # Default category\n",
        "    \n",
        "    try:\n",
        "        pred = predict_stock(category, month, lagged_qt)\n",
        "        stock_results.append({\n",
        "            'month': month,\n",
        "            'lagged_qt': lagged_qt,\n",
        "            'predicted_qt': pred,\n",
        "            'category': category\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error at row {idx}: {e}\")\n",
        "        stock_results.append({\n",
        "            'month': month,\n",
        "            'lagged_qt': lagged_qt,\n",
        "            'predicted_qt': None,\n",
        "            'category': category,\n",
        "            'error': str(e)\n",
        "        })\n",
        "\n",
        "stock_df = pd.DataFrame(stock_results)\n",
        "print(f\"\\n✓ Stock forecaster tested on {len(stock_results)} samples\")\n",
        "print(f\"  Predictions made: {stock_df['predicted_qt'].notna().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Test Revenue Predictor (XGBoost)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Revenue Predictor XGBoost model...\n",
            "✓ Revenue model loaded\n",
            "\n",
            "✓ Revenue predictor tested on 879 samples\n",
            "  Valid predictions: 879\n",
            "  MSE: 3510913.61\n",
            "  MAE: 1087.91\n",
            "  R²: 0.8762\n"
          ]
        }
      ],
      "source": [
        "# Load revenue predictor model\n",
        "print(\"Loading Revenue Predictor XGBoost model...\")\n",
        "\n",
        "revenue_model_path = \"D:/Deloitte/New_ML_Models/revenue_predictor/revenue_predictor_xgb.pkl\"\n",
        "revenue_model = joblib.load(revenue_model_path)\n",
        "\n",
        "print(\"✓ Revenue model loaded\")\n",
        "\n",
        "# Test revenue predictor\n",
        "def predict_revenue(is_weekend, is_holiday, lagged_revenue):\n",
        "    \"\"\"Predict revenue using XGBoost\"\"\"\n",
        "    features = np.array([[is_weekend, is_holiday, lagged_revenue]])\n",
        "    prediction = float(revenue_model.predict(features)[0])\n",
        "    return max(0, prediction)\n",
        "\n",
        "# Test on revenue test data\n",
        "revenue_results = []\n",
        "for idx, row in revenue_test.iterrows():\n",
        "    is_weekend = float(row['is_weekend']) if pd.notna(row['is_weekend']) else 0.0\n",
        "    is_holiday = float(row['is_holiday']) if pd.notna(row['is_holiday']) else 0.0\n",
        "    lagged_revenue = float(row['lagged_revenue']) if pd.notna(row['lagged_revenue']) else 0.0\n",
        "    actual = float(row['val']) if pd.notna(row['val']) else None\n",
        "    \n",
        "    try:\n",
        "        pred = predict_revenue(is_weekend, is_holiday, lagged_revenue)\n",
        "        revenue_results.append({\n",
        "            'is_weekend': is_weekend,\n",
        "            'is_holiday': is_holiday,\n",
        "            'lagged_revenue': lagged_revenue,\n",
        "            'actual_revenue': actual,\n",
        "            'predicted_revenue': pred\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error at row {idx}: {e}\")\n",
        "        revenue_results.append({\n",
        "            'is_weekend': is_weekend,\n",
        "            'is_holiday': is_holiday,\n",
        "            'lagged_revenue': lagged_revenue,\n",
        "            'actual_revenue': actual,\n",
        "            'predicted_revenue': None,\n",
        "            'error': str(e)\n",
        "        })\n",
        "\n",
        "revenue_df = pd.DataFrame(revenue_results)\n",
        "\n",
        "# Calculate metrics if actual values are available\n",
        "if revenue_df['actual_revenue'].notna().sum() > 0:\n",
        "    valid_mask = revenue_df['actual_revenue'].notna() & revenue_df['predicted_revenue'].notna()\n",
        "    if valid_mask.sum() > 0:\n",
        "        actual_vals = revenue_df.loc[valid_mask, 'actual_revenue']\n",
        "        pred_vals = revenue_df.loc[valid_mask, 'predicted_revenue']\n",
        "        \n",
        "        mse = mean_squared_error(actual_vals, pred_vals)\n",
        "        mae = mean_absolute_error(actual_vals, pred_vals)\n",
        "        r2 = r2_score(actual_vals, pred_vals)\n",
        "        \n",
        "        print(f\"\\n✓ Revenue predictor tested on {len(revenue_results)} samples\")\n",
        "        print(f\"  Valid predictions: {valid_mask.sum()}\")\n",
        "        print(f\"  MSE: {mse:.2f}\")\n",
        "        print(f\"  MAE: {mae:.2f}\")\n",
        "        print(f\"  R²: {r2:.4f}\")\n",
        "else:\n",
        "    print(f\"\\n✓ Revenue predictor tested on {len(revenue_results)} samples\")\n",
        "    print(f\"  Predictions made: {revenue_df['predicted_revenue'].notna().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Customer Churn Predictor (Random Forest)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Customer Churn Random Forest model...\n",
            "✓ Customer churn model loaded\n",
            "\n",
            "✓ Customer churn predictor tested on 575 samples\n",
            "  Valid predictions: 575\n",
            "  Accuracy: 0.9948\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.50      0.40         2\n",
            "           1       1.00      1.00      1.00       573\n",
            "\n",
            "    accuracy                           0.99       575\n",
            "   macro avg       0.67      0.75      0.70       575\n",
            "weighted avg       1.00      0.99      1.00       575\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load customer churn model\n",
        "print(\"Loading Customer Churn Random Forest model...\")\n",
        "\n",
        "churn_model_path = \"D:/Deloitte/New_ML_Models/customer_churn/model_bundle.joblib\"\n",
        "churn_bundle = joblib.load(churn_model_path)\n",
        "churn_model = churn_bundle[\"model\"]\n",
        "churn_scaler = churn_bundle[\"scaler\"]\n",
        "\n",
        "print(\"✓ Customer churn model loaded\")\n",
        "\n",
        "# Test customer churn predictor\n",
        "def predict_churn(discount_amount, points_earned, price, waiting_time):\n",
        "    \"\"\"Predict customer churn using Random Forest\"\"\"\n",
        "    x = [[discount_amount, points_earned, price, waiting_time]]\n",
        "    x_scaled = churn_scaler.transform(x)\n",
        "    prediction = churn_model.predict(x_scaled)[0]\n",
        "    probability = churn_model.predict_proba(x_scaled)[0][1]\n",
        "    return prediction, probability\n",
        "\n",
        "# Test on customer churn test data\n",
        "churn_results = []\n",
        "for idx, row in customer_churn_test.iterrows():\n",
        "    discount_amount = float(row['discount_amount']) if pd.notna(row['discount_amount']) else 0.0\n",
        "    points_earned = float(row['points_earned']) if pd.notna(row['points_earned']) else 0.0\n",
        "    price = float(row['price']) if pd.notna(row['price']) else 0.0\n",
        "    waiting_time = float(row['waiting_time']) if pd.notna(row['waiting_time']) else 0.0\n",
        "    actual = int(row['is_churned']) if pd.notna(row['is_churned']) else None\n",
        "    \n",
        "    try:\n",
        "        pred, prob = predict_churn(discount_amount, points_earned, price, waiting_time)\n",
        "        churn_results.append({\n",
        "            'user_id': row.get('user_id', idx),\n",
        "            'discount_amount': discount_amount,\n",
        "            'points_earned': points_earned,\n",
        "            'price': price,\n",
        "            'waiting_time': waiting_time,\n",
        "            'actual_churn': actual,\n",
        "            'predicted_churn': int(pred),\n",
        "            'churn_probability': float(prob)\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error at row {idx}: {e}\")\n",
        "        churn_results.append({\n",
        "            'user_id': row.get('user_id', idx),\n",
        "            'actual_churn': actual,\n",
        "            'predicted_churn': None,\n",
        "            'churn_probability': None,\n",
        "            'error': str(e)\n",
        "        })\n",
        "\n",
        "churn_df = pd.DataFrame(churn_results)\n",
        "\n",
        "# Calculate metrics if actual values are available\n",
        "if churn_df['actual_churn'].notna().sum() > 0:\n",
        "    valid_mask = churn_df['actual_churn'].notna() & churn_df['predicted_churn'].notna()\n",
        "    if valid_mask.sum() > 0:\n",
        "        actual_vals = churn_df.loc[valid_mask, 'actual_churn']\n",
        "        pred_vals = churn_df.loc[valid_mask, 'predicted_churn']\n",
        "        \n",
        "        accuracy = accuracy_score(actual_vals, pred_vals)\n",
        "        \n",
        "        print(f\"\\n✓ Customer churn predictor tested on {len(churn_results)} samples\")\n",
        "        print(f\"  Valid predictions: {valid_mask.sum()}\")\n",
        "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"\\nClassification Report:\")\n",
        "        print(classification_report(actual_vals, pred_vals))\n",
        "else:\n",
        "    print(f\"\\n✓ Customer churn predictor tested on {len(churn_results)} samples\")\n",
        "    print(f\"  Predictions made: {churn_df['predicted_churn'].notna().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test Campaign Success Predictor (Random Forest Classifier)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Campaign Success Classifier Random Forest model...\n",
            "✓ Campaign classifier loaded\n",
            "  Feature columns: ['duration_days', 'discount', 'redemptions', 'redemptions_per_duration']\n",
            "\n",
            "✓ Campaign success predictor tested on 129 samples\n",
            "  Valid predictions: 129\n",
            "  Accuracy: 0.7984\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.97      0.88       103\n",
            "           1       0.50      0.12      0.19        26\n",
            "\n",
            "    accuracy                           0.80       129\n",
            "   macro avg       0.66      0.54      0.54       129\n",
            "weighted avg       0.75      0.80      0.74       129\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load campaign classifier model (only classifier, not regressor)\n",
        "print(\"Loading Campaign Success Classifier Random Forest model...\")\n",
        "\n",
        "campaign_model_dir = \"D:/Deloitte/New_ML_Models/Campaign_ROI_Predictor/models/\"\n",
        "campaign_classifier = joblib.load(os.path.join(campaign_model_dir, 'campaign_success_classifier.pkl'))\n",
        "campaign_scaler = joblib.load(os.path.join(campaign_model_dir, 'campaign_scaler.pkl'))\n",
        "campaign_features = joblib.load(os.path.join(campaign_model_dir, 'campaign_features.pkl'))\n",
        "\n",
        "print(f\"✓ Campaign classifier loaded\")\n",
        "print(f\"  Feature columns: {campaign_features}\")\n",
        "\n",
        "# Test campaign classifier\n",
        "def predict_campaign_success(duration_days, discount, redemptions, redemptions_per_duration):\n",
        "    \"\"\"Predict campaign success using Random Forest Classifier\"\"\"\n",
        "    x = [[duration_days, discount, redemptions, redemptions_per_duration]]\n",
        "    x_scaled = campaign_scaler.transform(x)\n",
        "    prediction = campaign_classifier.predict(x_scaled)[0]\n",
        "    probability = campaign_classifier.predict_proba(x_scaled)[0][1]\n",
        "    return prediction, probability\n",
        "\n",
        "# Test on campaign test data\n",
        "campaign_results = []\n",
        "for idx, row in campaign_test.iterrows():\n",
        "    duration_days = float(row['duration_days']) if pd.notna(row['duration_days']) else 0.0\n",
        "    discount = float(row['discount']) if pd.notna(row['discount']) else 0.0\n",
        "    redemptions = float(row['redemptions']) if pd.notna(row['redemptions']) else 0.0\n",
        "    redemptions_per_duration = float(row['redemptions_per_duration']) if pd.notna(row['redemptions_per_duration']) else 0.0\n",
        "    actual = int(row['val']) if pd.notna(row['val']) else None\n",
        "    \n",
        "    try:\n",
        "        pred, prob = predict_campaign_success(duration_days, discount, redemptions, redemptions_per_duration)\n",
        "        campaign_results.append({\n",
        "            'duration_days': duration_days,\n",
        "            'discount': discount,\n",
        "            'redemptions': redemptions,\n",
        "            'redemptions_per_duration': redemptions_per_duration,\n",
        "            'actual_success': actual,\n",
        "            'predicted_success': int(pred),\n",
        "            'success_probability': float(prob)\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error at row {idx}: {e}\")\n",
        "        campaign_results.append({\n",
        "            'duration_days': duration_days,\n",
        "            'discount': discount,\n",
        "            'redemptions': redemptions,\n",
        "            'redemptions_per_duration': redemptions_per_duration,\n",
        "            'actual_success': actual,\n",
        "            'predicted_success': None,\n",
        "            'success_probability': None,\n",
        "            'error': str(e)\n",
        "        })\n",
        "\n",
        "campaign_df = pd.DataFrame(campaign_results)\n",
        "\n",
        "# Calculate metrics if actual values are available\n",
        "if campaign_df['actual_success'].notna().sum() > 0:\n",
        "    valid_mask = campaign_df['actual_success'].notna() & campaign_df['predicted_success'].notna()\n",
        "    if valid_mask.sum() > 0:\n",
        "        actual_vals = campaign_df.loc[valid_mask, 'actual_success']\n",
        "        pred_vals = campaign_df.loc[valid_mask, 'predicted_success']\n",
        "        \n",
        "        accuracy = accuracy_score(actual_vals, pred_vals)\n",
        "        \n",
        "        print(f\"\\n✓ Campaign success predictor tested on {len(campaign_results)} samples\")\n",
        "        print(f\"  Valid predictions: {valid_mask.sum()}\")\n",
        "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"\\nClassification Report:\")\n",
        "        print(classification_report(actual_vals, pred_vals))\n",
        "else:\n",
        "    print(f\"\\n✓ Campaign success predictor tested on {len(campaign_results)} samples\")\n",
        "    print(f\"  Predictions made: {campaign_df['predicted_success'].notna().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Export Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ All results exported to CSV files:\n",
            "  - Testing/exports/stock_forecaster_results.csv\n",
            "  - Testing/exports/revenue_predictor_results.csv\n",
            "  - Testing/exports/customer_churn_predictor_results.csv\n",
            "  - Testing/exports/campaign_success_predictor_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Create exports directory if it doesn't exist\n",
        "exports_dir = \"Testing/exports\"\n",
        "os.makedirs(exports_dir, exist_ok=True)\n",
        "\n",
        "# Export all results to CSV\n",
        "stock_df.to_csv(f\"{exports_dir}/stock_forecaster_results.csv\", index=False)\n",
        "revenue_df.to_csv(f\"{exports_dir}/revenue_predictor_results.csv\", index=False)\n",
        "churn_df.to_csv(f\"{exports_dir}/customer_churn_predictor_results.csv\", index=False)\n",
        "campaign_df.to_csv(f\"{exports_dir}/campaign_success_predictor_results.csv\", index=False)\n",
        "\n",
        "print(\"✓ All results exported to CSV files:\")\n",
        "print(f\"  - {exports_dir}/stock_forecaster_results.csv\")\n",
        "print(f\"  - {exports_dir}/revenue_predictor_results.csv\")\n",
        "print(f\"  - {exports_dir}/customer_churn_predictor_results.csv\")\n",
        "print(f\"  - {exports_dir}/campaign_success_predictor_results.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TESTING SUMMARY\n",
            "================================================================================\n",
            "\n",
            "1. Stock Forecaster (LSTM):\n",
            "   Total samples: 238\n",
            "   Successful predictions: 238\n",
            "\n",
            "2. Revenue Predictor (XGBoost):\n",
            "   Total samples: 879\n",
            "   Successful predictions: 879\n",
            "   MSE: 3510913.61\n",
            "   MAE: 1087.91\n",
            "\n",
            "3. Customer Churn Predictor (Random Forest):\n",
            "   Total samples: 575\n",
            "   Successful predictions: 575\n",
            "   Accuracy: 0.9948\n",
            "\n",
            "4. Campaign Success Predictor (Random Forest Classifier):\n",
            "   Total samples: 129\n",
            "   Successful predictions: 129\n",
            "   Accuracy: 0.7984\n",
            "\n",
            "================================================================================\n",
            "Testing complete! All results exported to CSV files.\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Display summary statistics\n",
        "print(\"=\"*80)\n",
        "print(\"TESTING SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n1. Stock Forecaster (LSTM):\")\n",
        "print(f\"   Total samples: {len(stock_df)}\")\n",
        "print(f\"   Successful predictions: {stock_df['predicted_qt'].notna().sum()}\")\n",
        "\n",
        "print(f\"\\n2. Revenue Predictor (XGBoost):\")\n",
        "print(f\"   Total samples: {len(revenue_df)}\")\n",
        "print(f\"   Successful predictions: {revenue_df['predicted_revenue'].notna().sum()}\")\n",
        "if revenue_df['actual_revenue'].notna().sum() > 0:\n",
        "    valid = revenue_df['actual_revenue'].notna() & revenue_df['predicted_revenue'].notna()\n",
        "    if valid.sum() > 0:\n",
        "        print(f\"   MSE: {mean_squared_error(revenue_df.loc[valid, 'actual_revenue'], revenue_df.loc[valid, 'predicted_revenue']):.2f}\")\n",
        "        print(f\"   MAE: {mean_absolute_error(revenue_df.loc[valid, 'actual_revenue'], revenue_df.loc[valid, 'predicted_revenue']):.2f}\")\n",
        "\n",
        "print(f\"\\n3. Customer Churn Predictor (Random Forest):\")\n",
        "print(f\"   Total samples: {len(churn_df)}\")\n",
        "print(f\"   Successful predictions: {churn_df['predicted_churn'].notna().sum()}\")\n",
        "if churn_df['actual_churn'].notna().sum() > 0:\n",
        "    valid = churn_df['actual_churn'].notna() & churn_df['predicted_churn'].notna()\n",
        "    if valid.sum() > 0:\n",
        "        print(f\"   Accuracy: {accuracy_score(churn_df.loc[valid, 'actual_churn'], churn_df.loc[valid, 'predicted_churn']):.4f}\")\n",
        "\n",
        "print(f\"\\n4. Campaign Success Predictor (Random Forest Classifier):\")\n",
        "print(f\"   Total samples: {len(campaign_df)}\")\n",
        "print(f\"   Successful predictions: {campaign_df['predicted_success'].notna().sum()}\")\n",
        "if campaign_df['actual_success'].notna().sum() > 0:\n",
        "    valid = campaign_df['actual_success'].notna() & campaign_df['predicted_success'].notna()\n",
        "    if valid.sum() > 0:\n",
        "        print(f\"   Accuracy: {accuracy_score(campaign_df.loc[valid, 'actual_success'], campaign_df.loc[valid, 'predicted_success']):.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Testing complete! All results exported to CSV files.\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
