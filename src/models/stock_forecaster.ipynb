{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c86ffb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from data_loader import *\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "print('âœ“ All libraries imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a136ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dim_add_ons.csv: 9731 rows\n",
      "Successfully loaded dim_campaigns.csv: 641 rows\n",
      "Successfully loaded dim_items.csv: 87266 rows\n",
      "Successfully loaded dim_menu_item_add_ons.csv: 2459 rows\n",
      "Successfully loaded dim_menu_items.csv: 30407 rows\n",
      "Successfully loaded dim_places.csv: 1056 rows\n",
      "Successfully loaded dim_skus.csv: 4 rows\n",
      "Successfully loaded dim_stock_categories.csv: 3 rows\n",
      "Successfully loaded dim_taxonomy_terms.csv: 904 rows\n",
      "Successfully loaded dim_users.csv: 22762 rows\n",
      "Successfully loaded fct_bonus_codes.csv: 6 rows\n",
      "Successfully loaded fct_campaigns.csv: 641 rows\n",
      "Successfully loaded fct_invoice_items.csv: 2918 rows\n",
      "Successfully loaded fct_order_items.csv: 1974592 rows\n",
      "Successfully loaded fct_orders.csv: 371667 rows\n",
      "Successfully loaded most_ordered.csv: 93048 rows\n"
     ]
    }
   ],
   "source": [
    "#we will load datasets first\n",
    "data_loader = DataLoader(\"D:/Deloitte/DIH-X-AUC-Hackathon/data/Old data\")\n",
    "additions_on_food = data_loader.load_csv('dim_add_ons.csv') #it contains the price and status and id and category and if it is deleted or not\n",
    "campaigns_offers = data_loader.load_csv('dim_campaigns.csv') #it contains campaigns (offers) and their places and if it is active or not\n",
    "stock_items_info = data_loader.load_csv('dim_items.csv')  #it contains items and if there is delivery or takeaway and if it is there and \n",
    "#types of additions and if it is displayed for customers\n",
    "menu_additions = data_loader.load_csv('dim_menu_item_add_ons.csv') #specific add_ons to menu and the price and all are active\n",
    "menu_items_info = data_loader.load_csv('dim_menu_items.csv') #it contains price and rating and votes and if it is acitve\n",
    "places_info = data_loader.load_csv(\"dim_places.csv\") #it contains  places and if it is bankrupt and visit_duration and vip threshold\n",
    "#and waiting_time and binding_period and their title\n",
    "stock_internal_measuring_units = data_loader.load_csv(\"dim_skus.csv\") #units for measuring stock\n",
    "stock_categories = data_loader.load_csv(\"dim_stock_categories.csv\") #stock categories ids and titles\n",
    "taxonomy_terms = data_loader.load_csv(\"dim_taxonomy_terms.csv\") #contains ids that translates into vocab (male,female,bar,cafe,lounge)\n",
    "users_info = data_loader.load_csv(\"dim_users.csv\") #if account closed,name,country,currency,points,savings,and the role\n",
    "bonus_codes = data_loader.load_csv(\"fct_bonus_codes.csv\") #bonus ids, points, and duration\n",
    "campaigns_info = data_loader.load_csv(\"fct_campaigns.csv\") #campaigns titles and items, delivery, what original campaing,the place\n",
    "#and redemptions and redemptions per_customer and if it is active or not.\n",
    "#cash_balances = data_loader.load_csv(\"fct_cash_balances.csv\")\n",
    "#cash_balances.head() #it shows transactions and opening and closing balances\n",
    "invoice_items = data_loader.load_csv(\"fct_invoice_items.csv\") #it shows specific items with the amount (tablets)\n",
    "order_items_info = data_loader.load_csv(\"fct_order_items.csv\") #order_tiems and cost, commision, discount, points earned, redeemed,\n",
    "#quanity, price and status and id of orders and title\n",
    "orders_info  = data_loader.load_csv(\"fct_orders.csv\") #orders cash amount, cashier, channel, points earned and redeemed, promise_time,\n",
    "#source and status and total amount and type (takeaway or eat in)\n",
    "most_ordered = data_loader.load_csv(\"most_ordered.csv\") #most ordered items and count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f124ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"list.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(order_items_info['title'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75f4b908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Processed 19089 unique items into 'full_menu_mapping.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def categorize_item(item):\n",
    "    item_lower = item.lower()\n",
    "    \n",
    "    # Logic to assign categories based on keywords\n",
    "    if any(k in item_lower for k in [\"soda\", \"cola\", \"vin\", \"beer\", \"Ã¸l\", \"juice\", \"coffee\", \"kaffe\", \"tea\", \"the\", \"latte\", \"drink\", \"water\", \"vand\", \"lassi\", \"shake\"]):\n",
    "        return \"Beverages\"\n",
    "    elif any(k in item_lower for k in [\"sushi\", \"maki\", \"nigiri\", \"gyoza\", \"tempura\", \"sashimi\", \"edamame\", \"hosomaki\"]):\n",
    "        return \"Sushi & Asian\"\n",
    "    elif any(k in item_lower for k in [\"sandwich\", \"wrap\", \"burger\", \"pita\", \"durum\", \"rulle\"]):\n",
    "        return \"Handhelds\"\n",
    "    elif any(k in item_lower for k in [\"pizza\", \"pasta\", \"steak\", \"chicken\", \"meal\", \"kylling\", \"kebab\", \"curry\", \"bolognese\", \"lasagna\"]):\n",
    "        return \"Main Courses\"\n",
    "    elif any(k in item_lower for k in [\"cake\", \"kage\", \"dessert\", \"mousse\", \"sweet\", \"oreo\", \"cookie\", \"is\", \"gelato\", \"cheesecake\"]):\n",
    "        return \"Desserts & Sweets\"\n",
    "    elif any(k in item_lower for k in [\"brunch\", \"breakfast\", \"morning\", \"morgen\", \"egg\", \"bowl\", \"yogurt\", \"pancake\"]):\n",
    "        return \"Breakfast & Brunch\"\n",
    "    elif any(k in item_lower for k in [\"salat\", \"salad\", \"greens\", \"asparges\"]):\n",
    "        return \"Salads & Greens\"\n",
    "    elif any(k in item_lower for k in [\"fries\", \"fritter\", \"pommes\", \"snacks\", \"dip\", \"oliven\", \"mandler\", \"nuggets\", \"samosa\"]):\n",
    "        return \"Sides & Snacks\"\n",
    "    elif any(k in item_lower for k in [\"klip\", \"powerbank\", \"pose\", \"lighter\", \"levering\", \"personale\", \"deposit\"]):\n",
    "        return \"Misc/Services\"\n",
    "    else:\n",
    "        return \"Other/Uncategorized\"\n",
    "\n",
    "# Read your file and create the mapping\n",
    "item_mapping = {}\n",
    "\n",
    "try:\n",
    "    with open('list.txt', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            item = line.strip()\n",
    "            if item:\n",
    "                # Map the item (Key) to the category (Value)\n",
    "                category = categorize_item(item)\n",
    "                item_mapping[item] = category\n",
    "\n",
    "    # Save to JSON\n",
    "    with open('full_menu_mapping.json', 'w', encoding='utf-8') as json_f:\n",
    "        json.dump(item_mapping, json_f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Success! Processed {len(item_mapping)} unique items into 'full_menu_mapping.json'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: list.txt not found. Please place the script in the same folder as your file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad0985ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: ITEM-LEVEL FEATURE ENGINEERING\n",
      "======================================================================\n",
      "\n",
      "âœ“ FILLING DATE GAPS FOR EACH ITEM...\n",
      "start filling\n",
      "finish filling\n",
      "\n",
      "âœ“ ITEM-LEVEL DATA PREPARED WITH DATE GAPS FILLED\n",
      "   â”œâ”€ Items: 10\n",
      "   â”œâ”€ Total records: 11870 (including 0 zero-sale days)\n",
      "   â”œâ”€ Date range: 2021-02-12 to 2024-05-13\n",
      "   â”œâ”€ Unique days: 1187\n",
      "   â””â”€ Avg daily sales per item: 227.72 units\n",
      "\n",
      "Sample Item Sales Data (with gaps filled):\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: ITEM-LEVEL FEATURE ENGINEERING\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: ITEM-LEVEL FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert timestamp\n",
    "stock_data = order_items_info.copy()\n",
    "stock_data['date'] = pd.to_datetime(stock_data['created'], errors='coerce')\n",
    "stock_data['date_only'] = stock_data['date'].dt.date\n",
    "\n",
    "# Create time-based features\n",
    "stock_data['day_of_week'] = stock_data['date'].dt.dayofweek\n",
    "stock_data['is_weekend'] = stock_data['day_of_week'].isin([5, 6]).astype(int)\n",
    "stock_data['month'] = stock_data['date'].dt.month\n",
    "stock_data = stock_data[stock_data['title'].isin(item_mapping.keys())]\n",
    "stock_data['category'] = stock_data['title'].map(item_mapping)\n",
    "# Holiday features\n",
    "holidays_2024 = [\n",
    "    '2023-01-01', '2023-02-21', '2023-03-30', '2023-04-10', '2023-05-01',\n",
    "    '2023-06-12', '2023-12-25', '2024-01-01', '2024-02-10', '2024-03-30',\n",
    "    '2024-05-01', '2024-12-25'\n",
    "]\n",
    "holidays_dates = pd.to_datetime(holidays_2024)\n",
    "stock_data['is_holiday'] = stock_data['date_only'].astype('datetime64[ns]').isin(holidays_dates).astype(int)\n",
    "\n",
    "# Aggregate by item and date\n",
    "item_sales = stock_data.groupby(['date_only', 'category']).agg({\n",
    "    'quantity': 'sum',\n",
    "    'price': 'mean',\n",
    "    'day_of_week': 'first',\n",
    "    'is_weekend': 'first',\n",
    "    'is_holiday': 'first',\n",
    "    'month': 'first'\n",
    "}).reset_index()\n",
    "#highest ten items for training\n",
    "categories = stock_data['category'].unique()\n",
    "item_sales.columns = ['date', 'category_name', 'quantity_sold', 'avg_price', 'day_of_week', 'is_weekend', 'is_holiday', 'month']\n",
    "item_sales['date'] = pd.to_datetime(item_sales['date'])\n",
    "item_sales = item_sales.sort_values(['category_name', 'date'])\n",
    "\n",
    "# FILL DATE GAPS FOR EACH ITEM\n",
    "print(f\"\\nâœ“ FILLING DATE GAPS FOR EACH ITEM...\")\n",
    "min_date = item_sales['date'].min()\n",
    "max_date = item_sales['date'].max()\n",
    "date_range = pd.date_range(min_date, max_date, freq='D')\n",
    "\n",
    "# Create complete date range for each item\n",
    "complete_dates = []\n",
    "for category in categories:\n",
    "    for date in date_range:\n",
    "        complete_dates.append({'date': date, 'category_name': category})\n",
    "\n",
    "complete_df = pd.DataFrame(complete_dates)\n",
    "\n",
    "# Merge with existing sales data\n",
    "item_sales_filled = complete_df.merge(\n",
    "    item_sales[['date', 'category_name', 'quantity_sold', 'avg_price']],\n",
    "    on=['date', 'category_name'],\n",
    "    how='left'\n",
    ")\n",
    "# Fill missing quantities with previous row values\n",
    "print(\"start filling\")\n",
    "item_sales_filled['quantity_sold'] = item_sales_filled['quantity_sold'].ffill()\n",
    "item_sales_filled['quantity_sold'] = item_sales_filled['quantity_sold'].bfill()\n",
    "print(\"finish filling\")\n",
    "item_sales_filled['avg_price']=item_sales_filled['avg_price'].mean()\n",
    "# Recalculate temporal features for all rows\n",
    "item_sales_filled['day_of_week'] = item_sales_filled['date'].dt.dayofweek\n",
    "item_sales_filled['is_weekend'] = item_sales_filled['day_of_week'].isin([5, 6]).astype(int)\n",
    "item_sales_filled['is_holiday'] = item_sales_filled['date'].dt.date.astype('datetime64[ns]').isin(holidays_dates).astype(int)\n",
    "item_sales_filled['month'] = item_sales_filled['date'].dt.month\n",
    "\n",
    "# Use filled data\n",
    "item_sales = item_sales_filled.sort_values(['category_name', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nâœ“ ITEM-LEVEL DATA PREPARED WITH DATE GAPS FILLED\")\n",
    "print(f\"   â”œâ”€ Items: {item_sales['category_name'].nunique()}\")\n",
    "print(f\"   â”œâ”€ Total records: {len(item_sales)} (including {len(item_sales[item_sales['quantity_sold']==0])} zero-sale days)\")\n",
    "print(f\"   â”œâ”€ Date range: {item_sales['date'].min().date()} to {item_sales['date'].max().date()}\")\n",
    "print(f\"   â”œâ”€ Unique days: {len(date_range)}\")\n",
    "print(f\"   â””â”€ Avg daily sales per item: {item_sales[item_sales['quantity_sold']>0]['quantity_sold'].mean():.2f} units\")\n",
    "\n",
    "print(f\"\\nSample Item Sales Data (with gaps filled):\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "892a02b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: PREPARE DATA FOR PER-ITEM XGBOOST MODELS\n",
      "======================================================================\n",
      "Axes(0.125,0.125;0.62x0.755)\n",
      "[[ 4.          0.          0.          2.         -0.34684872]\n",
      " [ 5.          1.          0.          2.         -0.3572497 ]\n",
      " [ 6.          1.          0.          2.         -0.35670228]\n",
      " ...\n",
      " [ 4.          0.          0.          5.          5.18373258]\n",
      " [ 5.          1.          0.          5.          4.98556664]\n",
      " [ 6.          1.          0.          5.          2.01690957]]\n",
      "Axes(0.125,0.125;0.496x0.755)\n",
      "[[ 4.          0.          0.          2.         -0.36513575]\n",
      " [ 5.          1.          0.          2.         -0.36546145]\n",
      " [ 6.          1.          0.          2.         -0.36546145]\n",
      " ...\n",
      " [ 4.          0.          0.          5.          5.05808657]\n",
      " [ 5.          1.          0.          5.          4.82781703]\n",
      " [ 6.          1.          0.          5.          2.71858708]]\n",
      "Axes(0.125,0.125;0.3968x0.755)\n",
      "[[4.         0.         0.         2.         9.56194545]\n",
      " [5.         1.         0.         2.         9.56194545]\n",
      " [6.         1.         0.         2.         9.56194545]\n",
      " ...\n",
      " [4.         0.         0.         5.         3.3417696 ]\n",
      " [5.         1.         0.         5.         2.9631502 ]\n",
      " [6.         1.         0.         5.         2.36817686]]\n",
      "Axes(0.125,0.125;0.31744x0.755)\n",
      "[[ 4.          0.          0.          2.         -0.3848971 ]\n",
      " [ 5.          1.          0.          2.         -0.3848971 ]\n",
      " [ 6.          1.          0.          2.         -0.3848971 ]\n",
      " ...\n",
      " [ 4.          0.          0.          5.          5.92468792]\n",
      " [ 5.          1.          0.          5.          5.45647215]\n",
      " [ 6.          1.          0.          5.          2.97190786]]\n",
      "Axes(0.125,0.125;0.253952x0.755)\n",
      "[[ 4.          0.          0.          2.         -0.03005542]\n",
      " [ 5.          1.          0.          2.         -0.03005542]\n",
      " [ 6.          1.          0.          2.         -0.03005542]\n",
      " ...\n",
      " [ 4.          0.          0.          5.          3.94499264]\n",
      " [ 5.          1.          0.          5.          3.33365395]\n",
      " [ 6.          1.          0.          5.          2.53379671]]\n",
      "Axes(0.125,0.125;0.203162x0.755)\n",
      "[[4.         0.         0.         2.         0.09973123]\n",
      " [5.         1.         0.         2.         0.09973123]\n",
      " [6.         1.         0.         2.         0.09973123]\n",
      " ...\n",
      " [4.         0.         0.         5.         3.99002289]\n",
      " [5.         1.         0.         5.         3.36185775]\n",
      " [6.         1.         0.         5.         3.51781599]]\n",
      "Axes(0.125,0.125;0.162529x0.755)\n",
      "[[4.         0.         0.         2.         0.46955349]\n",
      " [5.         1.         0.         2.         0.46955349]\n",
      " [6.         1.         0.         2.         0.46955349]\n",
      " ...\n",
      " [4.         0.         0.         5.         3.35304183]\n",
      " [5.         1.         0.         5.         3.26150252]\n",
      " [6.         1.         0.         5.         2.7580363 ]]\n",
      "Axes(0.125,0.125;0.130023x0.755)\n",
      "[[4.         0.         0.         2.         0.52045794]\n",
      " [5.         1.         0.         2.         0.52045794]\n",
      " [6.         1.         0.         2.         0.52045794]\n",
      " ...\n",
      " [4.         0.         0.         5.         4.24552785]\n",
      " [5.         1.         0.         5.         5.35876713]\n",
      " [6.         1.         0.         5.         1.99764083]]\n",
      "Axes(0.125,0.125;0.104019x0.755)\n",
      "[[4.         0.         0.         2.         1.77200498]\n",
      " [5.         1.         0.         2.         1.77200498]\n",
      " [6.         1.         0.         2.         1.77200498]\n",
      " ...\n",
      " [4.         0.         0.         5.         1.94779266]\n",
      " [5.         1.         0.         5.         1.80716252]\n",
      " [6.         1.         0.         5.         2.3696831 ]]\n",
      "Axes(0.125,0.125;0.083215x0.755)\n",
      "[[4.         0.         0.         2.         0.56924188]\n",
      " [5.         1.         0.         2.         0.56924188]\n",
      " [6.         1.         0.         2.         0.56924188]\n",
      " ...\n",
      " [4.         0.         0.         5.         3.19965731]\n",
      " [5.         1.         0.         5.         2.39162119]\n",
      " [6.         1.         0.         5.         1.44604702]]\n",
      "\n",
      "âœ“ PER-ITEM DATA PREPARED\n",
      "   â”œâ”€ Items in models: 10\n",
      "   â”œâ”€ Feature Names: day_of_week, is_weekend, is_holiday, month, quantity_sold\n",
      "   â””â”€ Train/Test Split: 80/20\n",
      "\n",
      "ðŸ“Š DATA SUMMARY BY ITEM:\n",
      "   Beverages:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 3085.94 units\n",
      "      â””â”€ Std dev: 3038.65 units\n",
      "   Other/Uncategorized:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 5343.75 units\n",
      "      â””â”€ Std dev: 4970.71 units\n",
      "   Sides & Snacks:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 218.61 units\n",
      "      â””â”€ Std dev: 196.09 units\n",
      "   Desserts & Sweets:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 480.70 units\n",
      "      â””â”€ Std dev: 416.69 units\n",
      "   Handhelds:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 660.94 units\n",
      "      â””â”€ Std dev: 587.51 units\n",
      "   Main Courses:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 428.09 units\n",
      "      â””â”€ Std dev: 355.99 units\n",
      "   Salads & Greens:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 76.28 units\n",
      "      â””â”€ Std dev: 70.87 units\n",
      "   Breakfast & Brunch:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 70.69 units\n",
      "      â””â”€ Std dev: 85.78 units\n",
      "   Sushi & Asian:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 47.26 units\n",
      "      â””â”€ Std dev: 43.25 units\n",
      "   Misc/Services:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 100.61 units\n",
      "      â””â”€ Std dev: 87.36 units\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 11 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 3: DATA PREPARATION FOR PER-ITEM MODELING\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: PREPARE DATA FOR PER-ITEM XGBOOST MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "item_sales_filled = item_sales_filled\n",
    "\n",
    "# Dictionary to store data for each item\n",
    "item_data_dict = {}\n",
    "scaler_price_dict = {}\n",
    "feature_names = ['day_of_week', 'is_weekend', 'is_holiday', 'month', 'quantity_sold']\n",
    "\n",
    "for category in categories:\n",
    "    # Get data for this item only\n",
    "    item_df = item_sales_filled[item_sales_filled['category_name'] == category].copy()\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X_features = item_df[feature_names].copy()\n",
    "    \n",
    "    # Normalize price for this item\n",
    "    scaler = StandardScaler()\n",
    "    X_features['quantity'] = scaler.fit_transform(X_features[['quantity_sold']])\n",
    "    scaler_price_dict[category] = scaler\n",
    "    X_features.drop('quantity_sold',axis=1,inplace=True)\n",
    "    # Target: quantity sold (shift by 1 to predict next day)\n",
    "    y = item_df['quantity_sold'].values[1:]\n",
    "    X_features = X_features[:-1].values\n",
    "    df = pd.DataFrame(X_features,columns=['day_of_week', 'is_weekend', 'is_holiday', 'month', 'quantity_sold'])\n",
    "    df['val']=pd.Series(y)\n",
    "    #show most correlated items\n",
    "    print(sns.heatmap(df.corr(),annot=True))\n",
    "    print(X_features)\n",
    "    # Train/test split (80/20)\n",
    "    X_features = X_features[:,3:5]\n",
    "    train_size = int(len(X_features) * 0.8)\n",
    "    X_train = X_features[:train_size]\n",
    "    X_test = X_features[train_size:]\n",
    "    y_train = y[:train_size]\n",
    "    y_test = y[train_size:]\n",
    "    \n",
    "    # Store item data\n",
    "    item_data_dict[category] = {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'total_samples': len(X_features)\n",
    "    }\n",
    "\n",
    "print(f\"\\nâœ“ PER-ITEM DATA PREPARED\")\n",
    "print(f\"   â”œâ”€ Items in models: {len(item_data_dict)}\")\n",
    "print(f\"   â”œâ”€ Feature Names: {', '.join(feature_names)}\")\n",
    "print(f\"   â””â”€ Train/Test Split: 80/20\")\n",
    "\n",
    "print(f\"\\nðŸ“Š DATA SUMMARY BY ITEM:\")\n",
    "for item, data in item_data_dict.items():\n",
    "    print(f\"   {item}:\")\n",
    "    print(f\"      â”œâ”€ Train samples: {len(data['y_train'])}\")\n",
    "    print(f\"      â”œâ”€ Test samples: {len(data['y_test'])}\")\n",
    "    print(f\"      â”œâ”€ Mean sales: {data['y_test'].mean():.2f} units\")\n",
    "    print(f\"      â””â”€ Std dev: {data['y_test'].std():.2f} units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b25117ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 4: TRAIN PER-ITEM XGBOOST MODELS\n",
      "======================================================================\n",
      "\n",
      "ðŸ”„ Training 10 models...\n",
      "âœ“ All models trained successfully!\n",
      "\n",
      "======================================================================\n",
      "PER-ITEM MODEL PERFORMANCE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.8870\n",
      "   â”œâ”€ Test RÂ²: -0.9425\n",
      "   â”œâ”€ RMSE: 4235.05 units\n",
      "   â”œâ”€ MAE: 2952.37 units\n",
      "   â””â”€ MAPE: 73.71%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.8181\n",
      "   â”œâ”€ Test RÂ²: -0.0606\n",
      "   â”œâ”€ RMSE: 88.34 units\n",
      "   â”œâ”€ MAE: 48.09 units\n",
      "   â””â”€ MAPE: 58.96%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.7289\n",
      "   â”œâ”€ Test RÂ²: -1.1350\n",
      "   â”œâ”€ RMSE: 608.86 units\n",
      "   â”œâ”€ MAE: 444.91 units\n",
      "   â””â”€ MAPE: 72.64%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.7936\n",
      "   â”œâ”€ Test RÂ²: -0.8729\n",
      "   â”œâ”€ RMSE: 804.02 units\n",
      "   â”œâ”€ MAE: 586.93 units\n",
      "   â””â”€ MAPE: 80.92%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.9084\n",
      "   â”œâ”€ Test RÂ²: -0.7545\n",
      "   â”œâ”€ RMSE: 471.54 units\n",
      "   â”œâ”€ MAE: 344.94 units\n",
      "   â””â”€ MAPE: 68.98%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.9918\n",
      "   â”œâ”€ Test RÂ²: 0.1811\n",
      "   â”œâ”€ RMSE: 79.06 units\n",
      "   â”œâ”€ MAE: 54.79 units\n",
      "   â””â”€ MAPE: 52.56%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.9162\n",
      "   â”œâ”€ Test RÂ²: -0.9848\n",
      "   â”œâ”€ RMSE: 7002.88 units\n",
      "   â”œâ”€ MAE: 4965.27 units\n",
      "   â””â”€ MAPE: 69.41%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.9535\n",
      "   â”œâ”€ Test RÂ²: -0.2394\n",
      "   â”œâ”€ RMSE: 78.90 units\n",
      "   â”œâ”€ MAE: 54.89 units\n",
      "   â””â”€ MAPE: 58.30%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.7473\n",
      "   â”œâ”€ Test RÂ²: -1.2211\n",
      "   â”œâ”€ RMSE: 292.24 units\n",
      "   â”œâ”€ MAE: 216.46 units\n",
      "   â””â”€ MAPE: 92.16%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.9764\n",
      "   â”œâ”€ Test RÂ²: 0.3076\n",
      "   â”œâ”€ RMSE: 35.99 units\n",
      "   â”œâ”€ MAE: 23.32 units\n",
      "   â””â”€ MAPE: 58.15%\n",
      "\n",
      "======================================================================\n",
      "FEATURE IMPORTANCE BY ITEM\n",
      "======================================================================\n",
      "\n",
      "Beverages:\n",
      "   is_weekend: 0.7676\n",
      "   day_of_week: 0.2324\n",
      "\n",
      "Breakfast & Brunch:\n",
      "   is_weekend: 0.8226\n",
      "   day_of_week: 0.1774\n",
      "\n",
      "Desserts & Sweets:\n",
      "   is_weekend: 0.8243\n",
      "   day_of_week: 0.1757\n",
      "\n",
      "Handhelds:\n",
      "   is_weekend: 0.8831\n",
      "   day_of_week: 0.1169\n",
      "\n",
      "Main Courses:\n",
      "   is_weekend: 0.8707\n",
      "   day_of_week: 0.1293\n",
      "\n",
      "Misc/Services:\n",
      "   is_weekend: 0.9660\n",
      "   day_of_week: 0.0340\n",
      "\n",
      "Other/Uncategorized:\n",
      "   is_weekend: 0.7045\n",
      "   day_of_week: 0.2955\n",
      "\n",
      "Salads & Greens:\n",
      "   is_weekend: 0.9320\n",
      "   day_of_week: 0.0680\n",
      "\n",
      "Sides & Snacks:\n",
      "   is_weekend: 0.9522\n",
      "   day_of_week: 0.0478\n",
      "\n",
      "Sushi & Asian:\n",
      "   is_weekend: 0.8493\n",
      "   day_of_week: 0.1507\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: TRAIN PER-ITEM XGBOOST MODELS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: TRAIN PER-ITEM XGBOOST MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dictionary to store trained models and metrics\n",
    "xgb_models_dict = {}\n",
    "model_metrics = {}\n",
    "\n",
    "print(f\"\\nðŸ”„ Training {len(item_data_dict)} models...\")\n",
    "\n",
    "for item, data in item_data_dict.items():\n",
    "    # Initialize model for this item\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=10000,\n",
    "        learning_rate=0.005,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(data['X_train'], data['y_train'], verbose=False)\n",
    "    xgb_models_dict[item] = model\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(data['X_train'])\n",
    "    y_pred_test = model.predict(data['X_test'])\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    train_r2 = r2_score(data['y_train'], y_pred_train)\n",
    "    test_r2 = r2_score(data['y_test'], y_pred_test)\n",
    "    rmse = np.sqrt(mean_squared_error(data['y_test'], y_pred_test))\n",
    "    mae = mean_absolute_error(data['y_test'], y_pred_test)\n",
    "    mape = np.mean(np.abs((data['y_test'] - y_pred_test) / (data['y_test'] + 1))) * 100\n",
    "    \n",
    "    model_metrics[item] = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mape': mape,\n",
    "        'importances': model.feature_importances_\n",
    "    }\n",
    "\n",
    "print(\"âœ“ All models trained successfully!\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"PER-ITEM MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for category in sorted(model_metrics.keys()):\n",
    "    metrics = model_metrics[category]\n",
    "    print(f\"\\n{item}:\")\n",
    "    print(f\"   â”œâ”€ Train RÂ²: {metrics['train_r2']:.4f}\")\n",
    "    print(f\"   â”œâ”€ Test RÂ²: {metrics['test_r2']:.4f}\")\n",
    "    print(f\"   â”œâ”€ RMSE: {metrics['rmse']:.2f} units\")\n",
    "    print(f\"   â”œâ”€ MAE: {metrics['mae']:.2f} units\")\n",
    "    print(f\"   â””â”€ MAPE: {metrics['mape']:.2f}%\")\n",
    "\n",
    "# Feature importance across all models\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE BY ITEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for item in sorted(model_metrics.keys()):\n",
    "    importances = model_metrics[item]['importances']\n",
    "    print(f\"\\n{item}:\")\n",
    "    for fname, imp in sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   {fname}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1b59a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def predict_next_day(current_date, category_name, verbose=True):\n",
    "    \"\"\"\n",
    "    Predict sales volume for the next day for a specific item\n",
    "    Uses the item-specific trained XGBoost model.\n",
    "    \"\"\"\n",
    "    # 1. Validate item exists in trained models\n",
    "    if category_name not in xgb_models_dict:\n",
    "        return {'error': f'Item {category_name} not in trained models.'}\n",
    "\n",
    "    # 2. Convert to datetime if string\n",
    "    current_dt = pd.to_datetime(current_date)\n",
    "    next_dt = current_dt + pd.Timedelta(days=1)\n",
    "\n",
    "    # 3. Extract time-based features for the prediction date (Next Day)\n",
    "    day_of_week = next_dt.dayofweek\n",
    "    is_weekend = 1 if next_dt.dayofweek in [5, 6] else 0\n",
    "    # Assumes holidays_dates is a list of datetime objects available in scope\n",
    "    is_holiday = 1 if next_dt.date() in [d.date() for d in holidays_dates] else 0\n",
    "    month = next_dt.month\n",
    "\n",
    "    # 4. Get the feature: \"Quantity Sold\" from the current_date (Today)\n",
    "    # We filter for the specific item\n",
    "    item_data = item_sales_filled[item_sales_filled['category_name'] == category_name]\n",
    "    \n",
    "    if item_data.empty:\n",
    "        return {'error': f'No historical data for item {category_name}'}\n",
    "\n",
    "    # Find the row corresponding to \"today\" to use its quantity as a feature for \"tomorrow\"\n",
    "    selected_row = item_data[item_data['date'] == current_dt]\n",
    "\n",
    "    if not selected_row.empty:\n",
    "        last_qty = float(selected_row['quantity_sold'].iloc[0])\n",
    "    else:\n",
    "        # Fallback: Find the most recent date available BEFORE or ON the current_dt\n",
    "        recent_data = item_data[item_data['date'] <= current_dt].sort_values('date')\n",
    "        if not recent_data.empty:\n",
    "            last_qty = float(recent_data['quantity_sold'].iloc[-1])\n",
    "        else:\n",
    "            last_qty = 0.0\n",
    "\n",
    "    # 5. Scale the quantity feature\n",
    "    scaler = scaler_price_dict.get(category_name)\n",
    "    if scaler is not None:\n",
    "        # Reshape for scaler: [[value]]\n",
    "        qty_scaled = float(scaler.transform([[last_qty]])[0][0])\n",
    "    else:\n",
    "        qty_scaled = last_qty\n",
    "\n",
    "    # 6. Create feature vector (Must match training order exactly)\n",
    "    features = np.array([[\n",
    "        month,\n",
    "        qty_scaled\n",
    "    ]])\n",
    "\n",
    "    # 7. Get model and performance metrics\n",
    "    model = xgb_models_dict[category_name]\n",
    "    metrics = model_metrics.get(category_name, {})\n",
    "    mae = metrics.get('mae', 0)\n",
    "    r2 = metrics.get('test_r2', 0)\n",
    "\n",
    "    # 8. Make prediction\n",
    "    # XGBoost returns an array, we take the first element\n",
    "    prediction = float(model.predict(features)[0])\n",
    "    \n",
    "    # Ensure we don't predict negative sales\n",
    "    prediction = max(0, prediction)\n",
    "\n",
    "    # 9. Calculate Bounds and Safety Stock\n",
    "    lower_bound = max(0, prediction - mae)\n",
    "    upper_bound = prediction + mae\n",
    "    safety_stock = prediction + mae # Simple recommendation: Predict + Error Margin\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nðŸ“… WALKFORWARD PREDICTION ({category_name})\")\n",
    "        print(f\" Â  Current Date: {current_dt.date()} (Qty: {last_qty})\")\n",
    "        print(f\" Â  Predict For: {next_dt.date()}\")\n",
    "        print(f\"\\nðŸ“Š FEATURES:\")\n",
    "        print(f\" Â  â”œâ”€ Day: {next_dt.day_name()} | Month: {next_dt.month}\")\n",
    "        print(f\" Â  â””â”€ Holiday: {'Yes' if is_holiday else 'No'}\")\n",
    "        print(f\"\\nðŸŽ¯ PREDICTION:\")\n",
    "        print(f\" Â  â”œâ”€ Expected Sales: {prediction:.2f} units\")\n",
    "        print(f\" Â  â”œâ”€ Confidence Range: [{lower_bound:.2f} - {upper_bound:.2f}]\")\n",
    "        print(f\" Â  â””â”€ Model Accuracy (RÂ²): {r2:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'date': next_dt.date(),\n",
    "        'category_name': category_name,\n",
    "        'predicted_quantity': prediction,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound,\n",
    "        'safety_stock': safety_stock,\n",
    "        'model_r2': r2,\n",
    "        'confidence_margin': (upper_bound - lower_bound),\n",
    "        'feature_used': last_qty\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f789d350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 9: WALKFORWARD PREDICTIONS FOR TOP 5 ITEMS (PER-ITEM MODELS)\n",
      "======================================================================\n",
      "\n",
      "ðŸ“… WALKFORWARD PREDICTION (Beverages)\n",
      " Â  Current Date: 2024-05-13 (Qty: 764.0)\n",
      " Â  Predict For: 2024-05-14\n",
      "\n",
      "ðŸ“Š FEATURES:\n",
      " Â  â”œâ”€ Day: Tuesday | Month: 5\n",
      " Â  â””â”€ Holiday: No\n",
      "\n",
      "ðŸŽ¯ PREDICTION:\n",
      " Â  â”œâ”€ Expected Sales: 135.87 units\n",
      " Â  â”œâ”€ Confidence Range: [0.00 - 3088.24]\n",
      " Â  â””â”€ Model Accuracy (RÂ²): -0.9425\n",
      "\n",
      "ðŸ“… WALKFORWARD PREDICTION (Other/Uncategorized)\n",
      " Â  Current Date: 2024-05-13 (Qty: 1466.0)\n",
      " Â  Predict For: 2024-05-14\n",
      "\n",
      "ðŸ“Š FEATURES:\n",
      " Â  â”œâ”€ Day: Tuesday | Month: 5\n",
      " Â  â””â”€ Holiday: No\n",
      "\n",
      "ðŸŽ¯ PREDICTION:\n",
      " Â  â”œâ”€ Expected Sales: 411.71 units\n",
      " Â  â”œâ”€ Confidence Range: [0.00 - 5376.98]\n",
      " Â  â””â”€ Model Accuracy (RÂ²): -0.9848\n",
      "\n",
      "ðŸ“… WALKFORWARD PREDICTION (Sides & Snacks)\n",
      " Â  Current Date: 2024-05-13 (Qty: 4.0)\n",
      " Â  Predict For: 2024-05-14\n",
      "\n",
      "ðŸ“Š FEATURES:\n",
      " Â  â”œâ”€ Day: Tuesday | Month: 5\n",
      " Â  â””â”€ Holiday: No\n",
      "\n",
      "ðŸŽ¯ PREDICTION:\n",
      " Â  â”œâ”€ Expected Sales: 4.44 units\n",
      " Â  â”œâ”€ Confidence Range: [0.00 - 220.90]\n",
      " Â  â””â”€ Model Accuracy (RÂ²): -1.2211\n",
      "\n",
      "ðŸ“… WALKFORWARD PREDICTION (Desserts & Sweets)\n",
      " Â  Current Date: 2024-05-13 (Qty: 127.0)\n",
      " Â  Predict For: 2024-05-14\n",
      "\n",
      "ðŸ“Š FEATURES:\n",
      " Â  â”œâ”€ Day: Tuesday | Month: 5\n",
      " Â  â””â”€ Holiday: No\n",
      "\n",
      "ðŸŽ¯ PREDICTION:\n",
      " Â  â”œâ”€ Expected Sales: 35.87 units\n",
      " Â  â”œâ”€ Confidence Range: [0.00 - 480.78]\n",
      " Â  â””â”€ Model Accuracy (RÂ²): -1.1350\n",
      "\n",
      "ðŸ“… WALKFORWARD PREDICTION (Handhelds)\n",
      " Â  Current Date: 2024-05-13 (Qty: 121.0)\n",
      " Â  Predict For: 2024-05-14\n",
      "\n",
      "ðŸ“Š FEATURES:\n",
      " Â  â”œâ”€ Day: Tuesday | Month: 5\n",
      " Â  â””â”€ Holiday: No\n",
      "\n",
      "ðŸŽ¯ PREDICTION:\n",
      " Â  â”œâ”€ Expected Sales: 15.23 units\n",
      " Â  â”œâ”€ Confidence Range: [0.00 - 602.17]\n",
      " Â  â””â”€ Model Accuracy (RÂ²): -0.8729\n",
      "[{'date': datetime.date(2024, 5, 14), 'category_name': 'Beverages', 'predicted_quantity': 135.87437438964844, 'lower_bound': 0, 'upper_bound': 3088.243343321215, 'safety_stock': 3088.243343321215, 'model_r2': -0.942479951834786, 'confidence_margin': 3088.243343321215, 'feature_used': 764.0}, {'date': datetime.date(2024, 5, 14), 'category_name': 'Other/Uncategorized', 'predicted_quantity': 411.7144470214844, 'lower_bound': 0, 'upper_bound': 5376.984943806624, 'safety_stock': 5376.984943806624, 'model_r2': -0.9848001126805883, 'confidence_margin': 5376.984943806624, 'feature_used': 1466.0}, {'date': datetime.date(2024, 5, 14), 'category_name': 'Sides & Snacks', 'predicted_quantity': 4.43956184387207, 'lower_bound': 0, 'upper_bound': 220.8964933958374, 'safety_stock': 220.8964933958374, 'model_r2': -1.2211181423796784, 'confidence_margin': 220.8964933958374, 'feature_used': 4.0}, {'date': datetime.date(2024, 5, 14), 'category_name': 'Desserts & Sweets', 'predicted_quantity': 35.87357711791992, 'lower_bound': 0, 'upper_bound': 480.78413709271854, 'safety_stock': 480.78413709271854, 'model_r2': -1.1350073378524805, 'confidence_margin': 480.78413709271854, 'feature_used': 127.0}, {'date': datetime.date(2024, 5, 14), 'category_name': 'Handhelds', 'predicted_quantity': 15.231729507446289, 'lower_bound': 0, 'upper_bound': 602.1663748737143, 'safety_stock': 602.1663748737143, 'model_r2': -0.8728896330581666, 'confidence_margin': 602.1663748737143, 'feature_used': 121.0}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Predicted Date</th>\n",
       "      <th>Predicted Quantity</th>\n",
       "      <th>Lower Bound</th>\n",
       "      <th>Upper Bound</th>\n",
       "      <th>Safety Stock</th>\n",
       "      <th>Confidence (Â±)</th>\n",
       "      <th>Model RÂ²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beverages</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3088.0</td>\n",
       "      <td>3088.0</td>\n",
       "      <td>3088.0</td>\n",
       "      <td>-0.9425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Other/Uncategorized</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>412.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5377.0</td>\n",
       "      <td>5377.0</td>\n",
       "      <td>5377.0</td>\n",
       "      <td>-0.9848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sides &amp; Snacks</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>-1.2211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Desserts &amp; Sweets</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>-1.1350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Handhelds</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>-0.8729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Category Predicted Date  Predicted Quantity  Lower Bound  \\\n",
       "0            Beverages     2024-05-14               136.0            0   \n",
       "1  Other/Uncategorized     2024-05-14               412.0            0   \n",
       "2       Sides & Snacks     2024-05-14                 4.0            0   \n",
       "3    Desserts & Sweets     2024-05-14                36.0            0   \n",
       "4            Handhelds     2024-05-14                15.0            0   \n",
       "\n",
       "   Upper Bound  Safety Stock  Confidence (Â±)  Model RÂ²  \n",
       "0       3088.0        3088.0          3088.0   -0.9425  \n",
       "1       5377.0        5377.0          5377.0   -0.9848  \n",
       "2        221.0         221.0           221.0   -1.2211  \n",
       "3        481.0         481.0           481.0   -1.1350  \n",
       "4        602.0         602.0           602.0   -0.8729  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 9: DEMONSTRATION - WALKFORWARD PREDICTIONS WITH PER-ITEM MODELS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 9: WALKFORWARD PREDICTIONS FOR TOP 5 ITEMS (PER-ITEM MODELS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get the latest date in data\n",
    "latest_date = item_sales_filled['date'].max()\n",
    "\n",
    "# Make predictions for top 5 items using their specific models\n",
    "results_list = []\n",
    "for category in categories[:5]:\n",
    "    result = predict_next_day(latest_date,category)\n",
    "    results_list.append(result)\n",
    "print(results_list)\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Category': r['category_name'],\n",
    "        'Predicted Date': r['date'],\n",
    "        'Predicted Quantity': round(r['predicted_quantity'], 0),\n",
    "        'Lower Bound': round(r['lower_bound'], 0),\n",
    "        'Upper Bound': round(r['upper_bound'], 0),\n",
    "        'Safety Stock': round(r['safety_stock'], 0),\n",
    "        'Confidence (Â±)': round(r['confidence_margin'], 0),\n",
    "        'Model RÂ²': round(r['model_r2'], 4)\n",
    "    }\n",
    "    for r in results_list\n",
    "])\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e6bca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 5: PREPARE SEQUENCES FOR LSTM MODELS\n",
      "======================================================================\n",
      "\n",
      "ðŸ”„ Creating sequences (lookback window: 10 days)...\n",
      "{'Beverages': {'X_train_seq': array([[[ 2.        ,  0.11960078],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.51323487],\n",
      "        ...,\n",
      "        [ 2.        , -0.1616595 ],\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.51323487]],\n",
      "\n",
      "       [[ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.51323487],\n",
      "        ...,\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.5483924 ]],\n",
      "\n",
      "       [[ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.51323487],\n",
      "        ...,\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.5483924 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        ,  2.29936802],\n",
      "        [ 9.        ,  2.61578585],\n",
      "        [ 9.        ,  1.94779266],\n",
      "        ...,\n",
      "        [ 9.        ,  3.77598454],\n",
      "        [ 9.        ,  2.05326527],\n",
      "        [ 9.        ,  5.42838874]],\n",
      "\n",
      "       [[ 9.        ,  2.61578585],\n",
      "        [ 9.        ,  1.94779266],\n",
      "        [ 9.        ,  2.01810774],\n",
      "        ...,\n",
      "        [ 9.        ,  2.05326527],\n",
      "        [ 9.        ,  5.42838874],\n",
      "        [ 9.        ,  1.66653237]],\n",
      "\n",
      "       [[ 9.        ,  1.94779266],\n",
      "        [ 9.        ,  2.01810774],\n",
      "        [ 9.        ,  3.14314889],\n",
      "        ...,\n",
      "        [ 9.        ,  5.42838874],\n",
      "        [ 9.        ,  1.66653237],\n",
      "        [ 9.        ,  3.38925164]]]), 'X_test_seq': array([[[  9.        ,   2.01810774],\n",
      "        [  9.        ,   3.14314889],\n",
      "        [  9.        ,   2.65094339],\n",
      "        ...,\n",
      "        [  9.        ,   1.66653237],\n",
      "        [  9.        ,   3.38925164],\n",
      "        [  9.        ,   5.00649831]],\n",
      "\n",
      "       [[  9.        ,   3.14314889],\n",
      "        [  9.        ,   2.65094339],\n",
      "        [  9.        ,   3.45956672],\n",
      "        ...,\n",
      "        [  9.        ,   3.38925164],\n",
      "        [  9.        ,   5.00649831],\n",
      "        [  9.        ,   2.58062831]],\n",
      "\n",
      "       [[  9.        ,   2.65094339],\n",
      "        [  9.        ,   3.45956672],\n",
      "        [  9.        ,   3.77598454],\n",
      "        ...,\n",
      "        [  9.        ,   5.00649831],\n",
      "        [  9.        ,   2.58062831],\n",
      "        [  9.        ,   3.45956672]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[  5.        , 279.9735884 ],\n",
      "        [  5.        , 221.08471538],\n",
      "        [  5.        , 398.34901255],\n",
      "        ...,\n",
      "        [  5.        , 296.63826052],\n",
      "        [  5.        , 241.61671648],\n",
      "        [  5.        , 355.31618833]],\n",
      "\n",
      "       [[  5.        , 221.08471538],\n",
      "        [  5.        , 398.34901255],\n",
      "        [  5.        , 352.22232515],\n",
      "        ...,\n",
      "        [  5.        , 241.61671648],\n",
      "        [  5.        , 355.31618833],\n",
      "        [  5.        , 342.58916025]],\n",
      "\n",
      "       [[  5.        , 398.34901255],\n",
      "        [  5.        , 352.22232515],\n",
      "        [  5.        , 146.30463602],\n",
      "        ...,\n",
      "        [  5.        , 355.31618833],\n",
      "        [  5.        , 342.58916025],\n",
      "        [  5.        , 151.9298418 ]]]), 'y_train_seq': array([  1.,   1.,   1.,   1.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "         4.,   4.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   1.,\n",
      "         7.,   2.,   1.,   2.,   2.,   2.,   2.,   2.,   2.,   5.,   1.,\n",
      "         1.,   1.,   1.,   4.,   4.,   4.,   4.,   4.,   3.,   3.,   2.,\n",
      "         2.,   1.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   1.,   2.,\n",
      "         2.,   2.,   2.,   2.,   2.,   1.,   3.,   3.,   3.,   1.,   3.,\n",
      "         8.,   3.,   2.,   2.,   2.,   2.,   6.,   6.,   6.,   6.,   6.,\n",
      "         6.,   6.,   6.,   6.,   6.,   2.,   5.,   5.,   5.,   5.,   5.,\n",
      "         5.,   5.,   1.,   4.,   4.,   1.,   1.,   2.,   6.,   6.,   6.,\n",
      "         7.,   2.,   8.,   1.,   6.,   6.,   3.,   3.,   3.,   2.,   2.,\n",
      "         2.,   1.,   1.,   1.,   1.,   1.,   4.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   6.,   6.,   1.,   2.,   2.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   2.,   3.,   3.,   3.,   3.,\n",
      "         3.,   6.,   7.,   6.,   1.,   1.,   9.,   6.,   2.,   4.,   4.,\n",
      "         3.,   3.,   3.,   7.,   7.,   8.,   5.,   5.,   2.,   2.,   1.,\n",
      "         4.,   7.,   7.,   2.,   2.,   2.,   2.,   6.,   2.,   7.,   5.,\n",
      "         3.,  12.,  12.,  13.,   4.,   1.,  11.,   6.,   3.,   7.,   3.,\n",
      "         7.,   7.,   7.,   5.,   6.,   1.,   1.,   1.,   2.,   3.,   3.,\n",
      "         3.,   3.,   3.,   3.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   2.,   2.,\n",
      "         2.,   2.,   4.,   4.,   2.,   3.,   3.,   6.,   9.,   7.,   3.,\n",
      "         2.,   1.,   1.,   1.,   1.,   6.,   3.,   3.,   3.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   7.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   2.,   3.,   3.,   1.,   2.,   2.,   2.,   2.,   1.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   1.,   1.,   1.,   1.,   1.,\n",
      "         2.,   2.,   2.,   4.,   4.,   1.,   1.,   2.,   1.,   2.,   1.,\n",
      "         1.,   1.,   1.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   3.,\n",
      "         3.,   2.,   2.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   3.,   2.,\n",
      "         2.,   1.,   6.,   6.,   1.,   1.,   1.,   1.,   1.,   3.,  14.,\n",
      "        14.,  14.,   5.,   5.,   1.,   3.,   1.,   1.,   1.,   1.,   1.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   4.,   4.,   4.,   2.,   2.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   3.,   3.,  28.,\n",
      "        41.,  70.,  52.,  34.,  29.,  40.,  54.,  49.,  56.,  83.,   9.,\n",
      "        35.,  34.,  33.,  21.,  10.,  10.,  22.,  23.,  32.,  29.,  26.,\n",
      "        68.,  57.,  23.,  33.,  30.,  21.,  32.,  56.,  50.,  29.,  30.,\n",
      "        39.,  51.,  26.,  43.,  67.,  52.,  50.,  20.,  70.,  29.,  25.,\n",
      "        47.,  15.,   6.,  55.,  16.,  36.,  47.,  37.,  23.,  21.,  37.,\n",
      "        90.,  83., 104.,  57.,  14.,  26.,  18.,   7.,  16.,  28.,  31.,\n",
      "         5.,   2.,  33.,  20.,  32.,  52.,  32.,  23.,   9.,  47.,  42.,\n",
      "        56.,  74.,  91.,  34.,  32.,  51.,  49.,  87.,  80.,  76.,  45.,\n",
      "        41.,  51.,  56.,  79.,  74.,  56.,  50.,  49.,  36.,  58.,  34.,\n",
      "        48.,  66.,  62.,  76.,  43.,  58.,  63.,  56.,  63.,  57.,  55.,\n",
      "        35.,  49.,  49.,  72.,  57.,  36.,  53.,  31.,  30.,  49., 126.,\n",
      "       142., 114., 121., 165., 122., 112., 144., 156., 115., 144., 191.,\n",
      "       153., 144., 190., 138., 112., 124., 135., 225., 144., 192., 112.,\n",
      "       113., 161., 156., 154., 168., 197., 196., 145., 130., 143., 189.,\n",
      "       180., 205., 197., 101., 154., 208., 168., 236., 253., 236., 126.,\n",
      "       152., 168., 189., 180., 168., 263., 145., 167., 175., 200., 141.,\n",
      "       237., 184., 157., 108., 142., 137., 173., 213., 183.,  94., 125.,\n",
      "       117., 107., 104., 162., 172., 102., 108., 120., 125.,  95., 123.,\n",
      "       203., 127.,  99., 126.,  80., 159., 164., 182.,  89.,  74.,  91.,\n",
      "        82.,  81., 150., 124.,  86.,  92.,  82.,  97.,  68., 144., 175.,\n",
      "       117.,  93.,  58.,  61.,  68., 202., 132.,  95.,  67., 100., 104.,\n",
      "        96., 111., 156.,  56.,  58.,  58.,  67.,  98., 139., 151.,  63.,\n",
      "        41.,  41.,  71.,  49., 102.,  72.,  57.,  54.,  69.,  73.,  65.,\n",
      "        85.,  94.,  41.,  40.,  66.,  43.,  36.,  85., 109.,  54.,  37.,\n",
      "        50.,  59.,  40.,  80.,  82.,  43.,  55.,  42.,  22.,  50.,  64.,\n",
      "       109.,  43.,  38.,  50.,  56.,  30.,  55., 106.,  41.,  42.,  54.,\n",
      "        37.,  48., 101.,  71.,  33.,  50.,  61.,  39.,  53.,  22.,   5.,\n",
      "        13.,  30.,  33.,  61.,  70.,  91.,  43.,  50.,  33.,  37.,  51.,\n",
      "        41.,  89., 113.,  44.,  51.,  33.,  67.,  27., 153.,  92.,  51.,\n",
      "        49.,  56.,  52.,  54.,  75., 116.,  31.,  43.,  35.,  46.,  39.,\n",
      "        85., 120.,  64.,  55.,  58.,  72.,  52., 103., 112.,  34.,  40.,\n",
      "        57.,  68.,  40.,  80.,  69.,  38.,  42.,  96.,  55.,  57.,  87.,\n",
      "        96.,  51.,  24.,  31.,  47.,  44.,  55.,  76.,  47.,  43.,  54.,\n",
      "        40.,  46., 119.,  91.,  51.,  45.,  36.,  37.,  42.,  85., 133.,\n",
      "        44.,  33.,  41.,  32.,  31.,  65.,  58.,  47.,  40.,  40.,  54.,\n",
      "        46.,  41.,  61.,  36.,  36.,  45.,  53.,  32.,  69.,  66.,  53.,\n",
      "        35.,  54.,  34.,  40.,  50.,  62.,  37.,  38.,  35.,  31.,  35.,\n",
      "        57.,  34.,  41.,  43.,  27.,  26.,  28.,  30.,  45.,  46.,  37.,\n",
      "        45.,  64.,  40.,  55.,  60.,  42.,  46.,  67.,  47.,  31.,  51.,\n",
      "        54.,  29.,  34.,  22.,  23.,  28.,  41.,  54.,  39.,  32.,  37.,\n",
      "        44.,  39.,  34.,  37.,  20.,  60.,  23.,  52.,  56.,  52.,  67.,\n",
      "        26.,  60.,  74.,  57.,  56.,  81., 102.,  37.,  85.,  60.,  59.,\n",
      "        70.,  70.,  59.,  29.,  49.,  43.,  50.,  44.,  69.,  96.,  30.,\n",
      "        63.,  67.,  38.,  57.,  45.,  74.,  24.,  46.,  70.,  72.,  92.,\n",
      "        77.,  39.,  24.,  56.,  60.,  45.,  48.,  70.,  61.,  27.,  39.,\n",
      "        31.,  27.,  45.,  64.,  53.,  26.,  32.,  67.,  33.,  44.,  56.,\n",
      "        26.,  14.,  34.,  55.,  50.,  47.,  57.,  43.,  32.,  73.,  55.,\n",
      "        57.,  59.,  48.,  48.,  14.,  43.,  50.,  65.,  55.,  54.,  71.,\n",
      "        16.,  65.,  41.,  55.,  39.,  54., 159.,  74.,  52., 105., 118.,\n",
      "       134.,  87.,  69.,  82.,  62.,  72.,  68.,  52., 130., 106.,  95.,\n",
      "        81.,  57.,  64.,  76.,  82.,  91.,  72.,  74., 106.,  92., 115.,\n",
      "       124.,  75., 171.,  64., 113., 159.]), 'y_test_seq': array([   90.,   115.,   165.,   167.,    98.,    80.,    81.,    87.,\n",
      "         205.,   144.,   197.,   138.,   134.,   161.,   132.,   170.,\n",
      "         199.,   185.,   156.,   144.,   154.,   129.,   159.,   222.,\n",
      "         200.,   146.,   156.,   132.,   168.,   166.,   176.,   181.,\n",
      "         191.,   170.,   145.,   227.,   216.,   218.,   294.,   174.,\n",
      "         172.,   196.,   243.,   233.,   198.,   259.,   155.,   156.,\n",
      "         183.,   201.,   337.,   224.,   149.,   204.,   185.,   222.,\n",
      "         197.,   241.,   391.,   162.,   145.,   283.,   233.,   267.,\n",
      "         262.,   307.,   295.,   207.,   284.,   368.,   372.,   475.,\n",
      "         686.,   681.,   317.,   416.,   415.,   643.,   792.,   859.,\n",
      "         620.,   355.,   648.,   610.,   784.,   679.,   832.,   559.,\n",
      "         543.,   427.,   595.,   545.,   756.,   472.,   116.,   908.,\n",
      "         565.,   936.,   994.,  1776.,   795.,   419.,   469.,   934.,\n",
      "        1063.,  1817.,  3390.,  3874.,  1924.,  1766.,  2374.,  2812.,\n",
      "        3055.,  5259.,  5043.,  2184.,  1955.,  2561.,  2397.,  3414.,\n",
      "        5404.,  5474.,  1951.,  2588.,  3084.,  3512.,  4371.,  6158.,\n",
      "        7162.,  3086.,  2693.,  3460.,  5119.,  4517.,  8091.,  7872.,\n",
      "        3359.,  3166.,  3515.,  4322.,  4774.,  6529.,  6411.,  3117.,\n",
      "        3176.,  4141.,  4673.,  4546.,  6955.,  7323.,  2644.,  3288.,\n",
      "        4257.,  4441.,  4458.,  7483.,  6934.,  2716.,  3116.,  3945.,\n",
      "        4409.,  5695.,  7443.,  8222.,  3793.,  3236.,  4144.,  5135.,\n",
      "        5073.,  8014.,  7880.,  3596.,  3939.,  3937.,  4530.,  4849.,\n",
      "        8402.,  9482.,  4882.,  4048.,  4421.,  4777.,  5288.,  8044.,\n",
      "        9122.,  4183.,  4566.,  5736.,  8340.,  6419.,  7035.,  8247.,\n",
      "        4450.,  3733.,  4097.,  5436.,  4996.,  9797., 11067.,  4273.,\n",
      "        4755.,  4309.,  5292.,  5527., 10199., 10954.,  3479.,  4451.,\n",
      "        4905.,  5493.,  6043.,  9847., 10983.,  4041.,  3903.,  4251.,\n",
      "        5130.,  5708.,  9731., 10198.,  4022.,  4613.,  7180.,  7980.,\n",
      "        6305., 11347., 10035.,  4178.,  4212.,  5903.,  8454.,  6889.,\n",
      "       10123.,  9761.,  4338.,   764.]), 'n_features': 2, 'n_sequences': 1177}, 'Other/Uncategorized': {'X_train_seq': array([[[ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        ...,\n",
      "        [ 2.        , -0.23197458],\n",
      "        [ 2.        , -0.40776226],\n",
      "        [ 2.        , -0.5483924 ]],\n",
      "\n",
      "       [[ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.51323487],\n",
      "        ...,\n",
      "        [ 2.        , -0.40776226],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.5483924 ]],\n",
      "\n",
      "       [[ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        ...,\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.5483924 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        ,  9.92855337],\n",
      "        [ 9.        , 16.71395784],\n",
      "        [ 9.        , 14.32324538],\n",
      "        ...,\n",
      "        [ 9.        , 13.02241654],\n",
      "        [ 9.        , 12.84662886],\n",
      "        [ 9.        , 14.85060842]],\n",
      "\n",
      "       [[ 9.        , 16.71395784],\n",
      "        [ 9.        , 14.32324538],\n",
      "        [ 9.        ,  9.43634786],\n",
      "        ...,\n",
      "        [ 9.        , 12.84662886],\n",
      "        [ 9.        , 14.85060842],\n",
      "        [ 9.        ,  6.97532033]],\n",
      "\n",
      "       [[ 9.        , 14.32324538],\n",
      "        [ 9.        ,  9.43634786],\n",
      "        [ 9.        ,  9.75276568],\n",
      "        ...,\n",
      "        [ 9.        , 14.85060842],\n",
      "        [ 9.        ,  6.97532033],\n",
      "        [ 9.        ,  8.06520395]]]), 'X_test_seq': array([[[  9.        ,   9.43634786],\n",
      "        [  9.        ,   9.75276568],\n",
      "        [  9.        ,   8.87382728],\n",
      "        ...,\n",
      "        [  9.        ,   6.97532033],\n",
      "        [  9.        ,   8.06520395],\n",
      "        [  9.        ,   9.19024511]],\n",
      "\n",
      "       [[  9.        ,   9.75276568],\n",
      "        [  9.        ,   8.87382728],\n",
      "        [  9.        ,   9.40119032],\n",
      "        ...,\n",
      "        [  9.        ,   8.06520395],\n",
      "        [  9.        ,   9.19024511],\n",
      "        [  9.        ,   7.85425873]],\n",
      "\n",
      "       [[  9.        ,   8.87382728],\n",
      "        [  9.        ,   9.40119032],\n",
      "        [  9.        ,  13.02241654],\n",
      "        ...,\n",
      "        [  9.        ,   9.19024511],\n",
      "        [  9.        ,   7.85425873],\n",
      "        [  9.        ,  11.79190278]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[  5.        , 509.27103905],\n",
      "        [  5.        , 449.85480299],\n",
      "        [  5.        , 655.03418386],\n",
      "        ...,\n",
      "        [  5.        , 502.90752501],\n",
      "        [  5.        , 435.68631593],\n",
      "        [  5.        , 584.89489927]],\n",
      "\n",
      "       [[  5.        , 449.85480299],\n",
      "        [  5.        , 655.03418386],\n",
      "        [  5.        , 549.31547271],\n",
      "        ...,\n",
      "        [  5.        , 435.68631593],\n",
      "        [  5.        , 584.89489927],\n",
      "        [  5.        , 560.03852123]],\n",
      "\n",
      "       [[  5.        , 655.03418386],\n",
      "        [  5.        , 549.31547271],\n",
      "        [  5.        , 277.72350608],\n",
      "        ...,\n",
      "        [  5.        , 584.89489927],\n",
      "        [  5.        , 560.03852123],\n",
      "        [  5.        , 332.35831723]]]), 'y_train_seq': array([  1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   4.,   4.,   3.,\n",
      "         4.,   6.,   6.,   1.,   1.,   7.,   7.,   2.,   2.,   2.,   1.,\n",
      "         4.,   7.,   3.,   1.,   9.,   2.,   6.,   2.,  19.,   7.,   7.,\n",
      "         2.,   3.,   3.,   7.,   4.,  11.,  11.,   5.,   5.,   8.,   1.,\n",
      "         2.,  17.,   6.,   1.,   4.,   3.,   2.,  11.,   3.,  10.,   8.,\n",
      "         3.,   3.,   2.,   1.,   5.,   5.,   7.,   7.,   1.,   2.,   4.,\n",
      "         3.,   3.,   3.,   3.,   2.,   1.,   1.,   1.,   2.,   2.,   2.,\n",
      "         2.,   2.,   4.,   2.,   2.,   2.,   1.,   1.,   1.,   1.,  11.,\n",
      "        11.,   6.,  13.,   5.,   9.,   8.,   8.,   8.,   2.,   1.,   1.,\n",
      "        10.,  14.,   1.,  17.,   8.,   6.,   4.,   7.,   2.,  10.,   7.,\n",
      "         6.,   2.,   4.,   4.,   4.,   7.,   7.,   4.,   2.,   2.,   2.,\n",
      "         1.,   3.,   5.,   5.,   5.,   5.,   2.,   2.,   3.,   8.,   8.,\n",
      "         1.,   1.,   1.,   2.,   2.,   3.,   3.,   4.,   6.,   6.,   6.,\n",
      "         4.,  16.,  15.,   9.,   2.,   2.,  16.,  13.,  12.,  10.,   3.,\n",
      "         5.,   5.,   3.,  13.,  18.,   4.,   4.,   4.,   1.,   7.,   2.,\n",
      "         5.,   2.,   3.,   2.,   2.,   1.,   4.,   8.,   3.,   4.,   6.,\n",
      "         2.,  11.,  13.,  15.,   1.,   2.,  12.,   5.,   4.,   6.,   5.,\n",
      "         4.,   4.,   4.,   8.,   5.,   4.,  13.,   2.,  11.,   9.,   9.,\n",
      "         9.,   9.,   9.,   9.,   4.,   4.,   4.,   4.,   1.,   1.,   1.,\n",
      "         1.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   3.,   1.,   1.,\n",
      "         1.,   3.,   2.,   1.,   3.,   2.,   2.,   4.,  10.,  16.,   4.,\n",
      "         2.,   5.,  17.,  17.,   2.,  13.,  10.,   2.,   2.,   1.,   1.,\n",
      "         1.,   5.,   1.,   1.,   3.,   1.,   1.,   6.,   6.,   6.,   6.,\n",
      "         1.,   1.,  10.,  10.,  10.,  10.,   3.,   3.,   3.,   2.,   2.,\n",
      "        10.,  10.,   1.,   1.,   1.,   8.,   8.,   8.,   8.,   5.,   5.,\n",
      "         3.,   3.,   3.,   9.,   4.,   1.,   3.,   3.,   3.,   9.,   4.,\n",
      "         1.,   1.,   1.,   4.,   4.,   2.,   2.,   2.,   2.,   2.,   7.,\n",
      "         3.,   3.,   3.,   3.,  10.,  10.,  10.,  10.,  10.,  10.,  10.,\n",
      "        10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.,   2.,   2.,   2.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   3.,   3.,   3.,\n",
      "         3.,   3.,   2.,   2.,   1.,   1.,   2.,   2.,   2.,   4.,   4.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   3.,   3.,\n",
      "         3.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   4.,   3.,   3.,\n",
      "         3.,   1.,   1.,   1.,   1.,   1.,   4.,   4.,   9.,   9.,  24.,\n",
      "        22.,  43.,  34.,  17.,  36.,  29.,  46.,  43.,  46.,  65.,   8.,\n",
      "        22.,  22.,  20.,  16.,  16.,  16.,   6.,  19.,  29.,  18.,  22.,\n",
      "        49.,  51.,  20.,  29.,  26.,   9.,  28.,  42.,  25.,  11.,  15.,\n",
      "        24.,  74.,  26.,  30.,  50.,  36.,  23.,  16., 123.,  19.,  12.,\n",
      "        26.,  11.,   7.,  34.,  11.,  24.,  33.,  15., 100.,  24.,  18.,\n",
      "       154., 112., 126.,  50.,  13.,  14.,   9.,  12.,  12.,  16.,  22.,\n",
      "         5.,   3.,  21.,  12.,  32.,  32.,  26.,  20.,   8.,  41.,  42.,\n",
      "        63.,  67.,  96.,  38.,  48.,  64.,  54.,  71.,  87.,  79.,  54.,\n",
      "        44.,  61.,  42.,  65.,  64.,  59.,  53.,  53.,  45.,  44.,  43.,\n",
      "        40.,  54.,  62.,  62.,  49.,  67.,  62.,  66.,  70.,  69.,  46.,\n",
      "        31.,  54.,  37.,  82.,  43.,  44.,  63.,  33.,  23.,  43., 171.,\n",
      "       180., 115., 128., 145., 135., 139., 191., 138., 108., 184., 174.,\n",
      "       202., 150., 171., 201., 121., 144., 156., 176., 197., 262., 165.,\n",
      "       135., 190., 196., 209., 258., 287., 232., 174., 190., 147., 209.,\n",
      "       229., 214., 238., 144., 173., 263., 216., 233., 292., 254., 156.,\n",
      "       150., 181., 181., 176., 261., 288., 155., 227., 212., 196., 187.,\n",
      "       309., 278., 180., 141., 148., 232., 226., 252., 256., 161., 137.,\n",
      "       141., 177., 144., 233., 268., 164., 144., 124., 142., 136., 216.,\n",
      "       264., 180., 110., 123., 149., 144., 281., 228., 110.,  88., 112.,\n",
      "       150., 114., 193., 151., 118., 154., 139., 131., 111., 202., 195.,\n",
      "       176.,  87.,  67.,  91.,  73., 261., 213., 131., 126., 113., 135.,\n",
      "       127., 130., 164.,  88.,  96.,  96., 128., 142., 219., 241.,  92.,\n",
      "        89.,  89.,  93.,  85., 117., 113.,  66.,  69.,  95.,  61., 101.,\n",
      "        78., 116.,  67., 116.,  79.,  50.,  63.,  94.,  79.,  65.,  50.,\n",
      "        55.,  53.,  60.,  82., 102.,  68.,  61.,  52.,  45.,  50.,  77.,\n",
      "       125.,  58.,  44.,  75.,  64.,  59., 145., 114.,  49.,  62.,  79.,\n",
      "        61.,  69.,  78.,  70.,  43.,  91.,  66.,  62.,  67.,  47.,  13.,\n",
      "        19.,  19.,  36.,  62.,  84., 100.,  65.,  55.,  75.,  53.,  91.,\n",
      "        35., 117., 105.,  60.,  75.,  57.,  67.,  46., 144.,  99.,  54.,\n",
      "        59.,  67.,  84.,  52.,  66., 153.,  80.,  75.,  68.,  65.,  64.,\n",
      "       102., 115.,  76.,  52.,  68.,  73.,  43.,  96., 120.,  69.,  61.,\n",
      "        55.,  71.,  44.,  63.,  77.,  61.,  59.,  79.,  56.,  41.,  72.,\n",
      "        92.,  70.,  52.,  54.,  52.,  56.,  81., 106.,  60.,  66.,  56.,\n",
      "        69.,  75.,  90.,  92.,  66.,  58.,  61.,  51.,  35.,  85., 105.,\n",
      "        59.,  51.,  51.,  63.,  58.,  68.,  76.,  49.,  59.,  37.,  70.,\n",
      "        32.,  71.,  63.,  66.,  43.,  57.,  67.,  56.,  95.,  85.,  49.,\n",
      "        49.,  53.,  40.,  46.,  58.,  57.,  58.,  51.,  50.,  55.,  38.,\n",
      "        55.,  52.,  66.,  70.,  36.,  51.,  35.,  37.,  59.,  65.,  55.,\n",
      "        74.,  95.,  63.,  72.,  64.,  84.,  51.,  60.,  31.,  54.,  66.,\n",
      "        76.,  37.,  66.,  53.,  41.,  35.,  56.,  43.,  51.,  47.,  56.,\n",
      "        63.,  46.,  44.,  81.,  45.,  89.,  58., 121., 107., 102., 137.,\n",
      "        48., 187., 138., 228., 179., 194., 222.,  33., 152., 115., 143.,\n",
      "       112., 166., 103.,  58.,  76.,  86.,  85.,  75., 121., 155.,  40.,\n",
      "       105., 101.,  70., 122.,  76., 188.,  32.,  66.,  95., 117., 135.,\n",
      "       156.,  92.,  83., 117., 116.,  80.,  90., 107.,  60.,  39.,  81.,\n",
      "        90.,  58.,  85., 134.,  90.,  39.,  85.,  98.,  82., 187.,  98.,\n",
      "        51.,  72., 143., 106.,  75.,  84., 145.,  75.,  83., 113., 157.,\n",
      "       118., 131., 117.,  95.,  67., 126., 125., 127.,  98., 257., 169.,\n",
      "        48., 142., 146., 113., 193., 323., 355., 273., 205., 259., 286.,\n",
      "       288., 439., 387., 286., 269., 240., 255., 330., 468., 478., 336.,\n",
      "       249., 263., 196., 276., 299., 492., 424., 285., 294., 269., 284.,\n",
      "       387., 382., 439., 215., 246., 278.]), 'y_test_seq': array([  240.,   352.,   338.,   339.,   319.,   308.,   290.,   302.,\n",
      "         459.,   398.,   416.,   268.,   305.,   353.,   402.,   523.,\n",
      "         453.,   511.,   317.,   296.,   338.,   300.,   397.,   464.,\n",
      "         403.,   274.,   376.,   366.,   330.,   436.,   345.,   548.,\n",
      "         361.,   397.,   389.,   430.,   508.,   444.,   778.,   462.,\n",
      "         455.,   489.,   474.,   533.,   429.,   580.,   334.,   353.,\n",
      "         414.,   474.,   502.,   436.,   434.,   322.,   388.,   562.,\n",
      "         509.,   494.,   786.,   490.,   378.,   513.,   585.,   636.,\n",
      "         720.,   677.,   681.,   542.,   616.,   859.,   726.,  1038.,\n",
      "        1264.,  1077.,   732.,   620.,   786.,   980.,  1318.,  1194.,\n",
      "         995.,   598.,   867.,   958.,  1271.,  1410.,  1157.,   800.,\n",
      "         842.,   793.,  1000.,   925.,  1388.,   860.,   241.,   616.,\n",
      "         678.,  1210.,  1352.,  2315.,  1679.,   810.,  1891.,  2767.,\n",
      "        2781.,  3690.,  6281.,  6111.,  3219.,  3649.,  4286.,  4965.,\n",
      "        5244.,  8660.,  7248.,  4752.,  4356.,  4957.,  4983.,  6133.,\n",
      "        9519.,  8921.,  4275.,  4702.,  5746.,  5851.,  7049.,  9701.,\n",
      "        9147.,  5863.,  5396.,  6056.,  8286.,  7478., 12598., 10985.,\n",
      "        6098.,  5857.,  6435.,  7572.,  8130., 10834.,  9975.,  6009.,\n",
      "        5606.,  7020.,  8061.,  8046., 11735., 11496.,  4781.,  6078.,\n",
      "        7506.,  8091.,  8188., 12426., 11988.,  6081.,  6132.,  6918.,\n",
      "        7674.,  9581., 14394., 13128.,  7379.,  7490.,  7924.,  8603.,\n",
      "        9460., 15818., 13123.,  7064.,  7063.,  7107.,  8384.,  8609.,\n",
      "       14708., 13861.,  8324.,  7273.,  7793.,  9393.,  9498., 14852.,\n",
      "       14597.,  7969.,  8924.,  9479., 14461., 10576., 10575., 11910.,\n",
      "        7731.,  6031.,  7260.,  9129.,  8961., 15168., 15579.,  7663.,\n",
      "        8271.,  7950.,  9701.,  9793., 16446., 16403.,  7271.,  7488.,\n",
      "        8287., 10016., 10078., 15786., 16102.,  7857.,  7093.,  7719.,\n",
      "        8963., 10221., 15675., 15386.,  8749.,  8393., 12579., 14502.,\n",
      "       12812., 18648., 15641.,  7916.,  8398., 10054., 14321., 12409.,\n",
      "       16653., 15946.,  9470.,  1466.]), 'n_features': 2, 'n_sequences': 1177}, 'Sides & Snacks': {'X_train_seq': array([[[ 2.        , 50.95739803],\n",
      "        [ 2.        , 50.95739803],\n",
      "        [ 2.        , 50.95739803],\n",
      "        ...,\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979]],\n",
      "\n",
      "       [[ 2.        , 50.95739803],\n",
      "        [ 2.        , 50.95739803],\n",
      "        [ 2.        , 50.95739803],\n",
      "        ...,\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979]],\n",
      "\n",
      "       [[ 2.        , 50.95739803],\n",
      "        [ 2.        , 50.95739803],\n",
      "        [ 2.        , -0.47807733],\n",
      "        ...,\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        , -0.51323487],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.40776226],\n",
      "        ...,\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.1616595 ],\n",
      "        [ 9.        , -0.33744719]],\n",
      "\n",
      "       [[ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.44291979],\n",
      "        ...,\n",
      "        [ 9.        , -0.1616595 ],\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.51323487]],\n",
      "\n",
      "       [[ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.51323487],\n",
      "        ...,\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.51323487],\n",
      "        [ 9.        , -0.51323487]]]), 'X_test_seq': array([[[ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.51323487],\n",
      "        [ 9.        , -0.5483924 ],\n",
      "        ...,\n",
      "        [ 9.        , -0.51323487],\n",
      "        [ 9.        , -0.51323487],\n",
      "        [ 9.        , -0.26713211]],\n",
      "\n",
      "       [[ 9.        , -0.51323487],\n",
      "        [ 9.        , -0.5483924 ],\n",
      "        [ 9.        , -0.51323487],\n",
      "        ...,\n",
      "        [ 9.        , -0.51323487],\n",
      "        [ 9.        , -0.26713211],\n",
      "        [ 9.        , -0.51323487]],\n",
      "\n",
      "       [[ 9.        , -0.5483924 ],\n",
      "        [ 9.        , -0.51323487],\n",
      "        [ 9.        , -0.30228965],\n",
      "        ...,\n",
      "        [ 9.        , -0.26713211],\n",
      "        [ 9.        , -0.51323487],\n",
      "        [ 9.        ,  0.15475832]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 5.        , 15.7998619 ],\n",
      "        [ 5.        , 12.53021104],\n",
      "        [ 5.        , 22.26884855],\n",
      "        ...,\n",
      "        [ 5.        , 15.06155364],\n",
      "        [ 5.        , 19.70234841],\n",
      "        [ 5.        , 18.61246479]],\n",
      "\n",
      "       [[ 5.        , 12.53021104],\n",
      "        [ 5.        , 22.26884855],\n",
      "        [ 5.        , 21.53054029],\n",
      "        ...,\n",
      "        [ 5.        , 19.70234841],\n",
      "        [ 5.        , 18.61246479],\n",
      "        [ 5.        , 16.64364277]],\n",
      "\n",
      "       [[ 5.        , 22.26884855],\n",
      "        [ 5.        , 21.53054029],\n",
      "        [ 5.        , 13.40914944],\n",
      "        ...,\n",
      "        [ 5.        , 18.61246479],\n",
      "        [ 5.        , 16.64364277],\n",
      "        [ 5.        , 13.54977959]]]), 'y_train_seq': array([ 4.,  4.,  4.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        3.,  3.,  3.,  3.,  3.,  3.,  3.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  2.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  2.,  2.,  2.,  2.,  3.,  3.,  3.,  2.,  3.,  3.,  3.,\n",
      "        5.,  5.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  2.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  4.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
      "        3.,  3.,  3.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  1.,  1.,  3.,  2.,  4.,  6.,  6.,  6.,  6.,\n",
      "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
      "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
      "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
      "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  1.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  1.,  1.,  3.,  3.,  6.,  3.,  1.,  5.,  1.,\n",
      "        1.,  9.,  6.,  3.,  9.,  2.,  2., 10.,  7.,  6.,  2.,  3.,  5.,\n",
      "        4., 10.,  6., 13., 11.,  8.,  4.,  4.,  4.,  8.,  7., 14.,  8.,\n",
      "        8.,  5.,  6.,  6., 10.,  3., 13.,  8.,  6., 11., 11., 11.,  6.,\n",
      "       10.,  4.,  6.,  6.,  8., 14.,  5.,  2.,  5.,  5.,  5.,  8., 12.,\n",
      "        5.,  8.,  8.,  6.,  1.,  8., 13.,  1.,  2.,  7.,  4.,  5.,  4.,\n",
      "        4.,  2.,  4.,  3.,  2.,  3.,  8.,  5.,  8.,  6.,  6.,  6.,  6.,\n",
      "        1., 14.,  8.,  4.,  3.,  1.,  6.,  2.,  2.,  1.,  1.,  4.,  4.,\n",
      "        4.,  1.,  4.,  7., 10., 10.,  9.,  1.,  6.,  1.,  4.,  4.,  4.,\n",
      "        4.,  4.,  2.,  3.,  7.,  3.,  3.,  3.,  3.,  4.,  3.,  1.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  3.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  1.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  1.,  3.,  5.,  1.,  9., 13.,  8., 10.,  6.,  7.,\n",
      "        3.,  8.,  7.,  9.,  9.,  9., 10.,  2.,  2.,  7.,  3.,  3.,  4.,\n",
      "        7.,  2.,  3.,  4.,  5.,  2.,  5.,  2.,  3.,  1.,  4., 11.,  8.,\n",
      "        5., 13., 11., 11.,  1.,  3., 10., 12.,  3.,  7.,  4.,  2.,  9.,\n",
      "        3.,  3.,  6.,  1.,  3.,  2.,  4.,  7.,  7.,  4., 10.,  3.,  1.,\n",
      "        3.,  4.,  7.,  5.,  7.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
      "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
      "        6.,  6.,  6.,  3., 15., 15., 17.,  2.,  1.,  1.,  6.,  5.,  7.,\n",
      "        2.,  4.,  1.,  1.,  1.,  2.,  4.,  5.,  4.,  2.,  1.,  2.,  8.,\n",
      "       12.,  7.,  2.,  2.,  9.]), 'y_test_seq': array([  2.,  21.,  17.,  17.,   7.,   4.,   4.,   5.,  14.,  18.,  26.,\n",
      "        12.,   9.,  21.,  15.,  26.,  22.,  32.,   9.,  17.,  18.,  22.,\n",
      "        27.,  20.,  26.,  19.,  16.,  18.,  18.,  19.,  32.,  17.,  21.,\n",
      "        30.,  26.,  25.,  28.,  25.,  95.,  17.,  14.,  28.,  42.,  34.,\n",
      "        41.,  45.,  20.,  19.,  31.,  37.,  27.,  23.,  28.,  18.,  36.,\n",
      "        45.,  28.,  17.,  46.,  21.,  19.,  50.,  39.,  29.,  33.,  43.,\n",
      "        38.,  16.,  25.,  33.,  18.,  38.,  47.,  54.,  24.,  34.,  37.,\n",
      "        36.,  51.,  39.,  64.,  20.,  74.,  48., 114.,  43.,  75., 120.,\n",
      "        28.,  37.,  28.,  49.,  39.,  15.,   4.,   2.,   6., 202., 105.,\n",
      "       177.,  74.,   7., 191.,  67.,  77., 100., 235., 391., 200., 120.,\n",
      "       145., 209., 220., 326., 519., 301., 138., 227., 218., 232., 457.,\n",
      "       380., 348., 159., 203., 219., 251., 384., 385., 460., 171., 237.,\n",
      "       280., 257., 446., 455., 358., 174., 276., 261., 227., 445., 360.,\n",
      "       380., 224., 299., 324., 332., 408., 427., 298., 216., 473., 301.,\n",
      "       288., 438., 492., 381., 233., 267., 273., 378., 513., 551., 467.,\n",
      "       237., 313., 295., 326., 513., 599., 390., 238., 225., 303., 308.,\n",
      "       610., 755., 630., 220., 262., 337., 439., 497., 742., 584., 302.,\n",
      "       418., 471., 413., 401., 429., 384., 348., 219., 353., 368., 519.,\n",
      "       645., 442., 248., 269., 356., 363., 563., 592., 435., 233., 316.,\n",
      "       351., 341., 520., 655., 399., 301., 325., 338., 483., 571., 547.,\n",
      "       502., 282., 376., 466., 373., 650., 629., 398., 267., 347., 445.,\n",
      "       577., 546., 490., 402.,   4.]), 'n_features': 2, 'n_sequences': 1177}, 'Desserts & Sweets': {'X_train_seq': array([[[ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979],\n",
      "        ...,\n",
      "        [ 2.        , -0.40776226],\n",
      "        [ 2.        , -0.47807733],\n",
      "        [ 2.        , -0.47807733]],\n",
      "\n",
      "       [[ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979],\n",
      "        ...,\n",
      "        [ 2.        , -0.47807733],\n",
      "        [ 2.        , -0.47807733],\n",
      "        [ 2.        , -0.47807733]],\n",
      "\n",
      "       [[ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        ...,\n",
      "        [ 2.        , -0.47807733],\n",
      "        [ 2.        , -0.47807733],\n",
      "        [ 2.        , -0.47807733]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        ,  0.75243643],\n",
      "        [ 9.        ,  1.24464194],\n",
      "        [ 9.        ,  0.47117615],\n",
      "        ...,\n",
      "        [ 9.        ,  1.1040118 ],\n",
      "        [ 9.        ,  1.03369672],\n",
      "        [ 9.        ,  1.70168991]],\n",
      "\n",
      "       [[ 9.        ,  1.24464194],\n",
      "        [ 9.        ,  0.47117615],\n",
      "        [ 9.        ,  0.40086107],\n",
      "        ...,\n",
      "        [ 9.        ,  1.03369672],\n",
      "        [ 9.        ,  1.70168991],\n",
      "        [ 9.        ,  0.82275151]],\n",
      "\n",
      "       [[ 9.        ,  0.47117615],\n",
      "        [ 9.        ,  0.40086107],\n",
      "        [ 9.        ,  0.92822412],\n",
      "        ...,\n",
      "        [ 9.        ,  1.70168991],\n",
      "        [ 9.        ,  0.82275151],\n",
      "        [ 9.        ,  0.78759397]]]), 'X_test_seq': array([[[ 9.        ,  0.40086107],\n",
      "        [ 9.        ,  0.92822412],\n",
      "        [ 9.        ,  0.15475832],\n",
      "        ...,\n",
      "        [ 9.        ,  0.82275151],\n",
      "        [ 9.        ,  0.78759397],\n",
      "        [ 9.        ,  0.7172789 ]],\n",
      "\n",
      "       [[ 9.        ,  0.92822412],\n",
      "        [ 9.        ,  0.15475832],\n",
      "        [ 9.        ,  0.7172789 ],\n",
      "        ...,\n",
      "        [ 9.        ,  0.78759397],\n",
      "        [ 9.        ,  0.7172789 ],\n",
      "        [ 9.        ,  0.29538846]],\n",
      "\n",
      "       [[ 9.        ,  0.15475832],\n",
      "        [ 9.        ,  0.7172789 ],\n",
      "        [ 9.        ,  1.1040118 ],\n",
      "        ...,\n",
      "        [ 9.        ,  0.7172789 ],\n",
      "        [ 9.        ,  0.29538846],\n",
      "        [ 9.        ,  1.17432687]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 5.        , 36.50765068],\n",
      "        [ 5.        , 35.62871228],\n",
      "        [ 5.        , 44.13683602],\n",
      "        ...,\n",
      "        [ 5.        , 36.78891097],\n",
      "        [ 5.        , 36.47249314],\n",
      "        [ 5.        , 58.30532308]],\n",
      "\n",
      "       [[ 5.        , 35.62871228],\n",
      "        [ 5.        , 44.13683602],\n",
      "        [ 5.        , 39.07415082],\n",
      "        ...,\n",
      "        [ 5.        , 36.47249314],\n",
      "        [ 5.        , 58.30532308],\n",
      "        [ 5.        , 53.9457886 ]],\n",
      "\n",
      "       [[ 5.        , 44.13683602],\n",
      "        [ 5.        , 39.07415082],\n",
      "        [ 5.        , 23.04231434],\n",
      "        ...,\n",
      "        [ 5.        , 58.30532308],\n",
      "        [ 5.        , 53.9457886 ],\n",
      "        [ 5.        , 30.81212983]]]), 'y_train_seq': array([ 3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  2.,  2.,  2.,  2., 12.,\n",
      "       12., 12., 12.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  1.,  1.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  2.,  3.,  1.,  1.,  1.,  1.,  4.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  5.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  3.,  1.,  1.,  3.,  3.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  2.,  1.,  5.,  1.,  1.,  1.,  1.,  1.,  1.,  7.,  7.,  7.,\n",
      "        1.,  2.,  1.,  5.,  3.,  1.,  2.,  2.,  2.,  8.,  2.,  2.,  1.,\n",
      "        3.,  1.,  1.,  5.,  5.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  4.,  4.,  4.,\n",
      "        4.,  4.,  1.,  1.,  1.,  1.,  2.,  1.,  1.,  1.,  3.,  1.,  1.,\n",
      "        1.,  1.,  2.,  4.,  4.,  4.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,\n",
      "        2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  4.,  4.,  4.,  4.,  4.,  4.,  3.,  3.,\n",
      "        2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  7., 20., 10., 14.,\n",
      "        8., 11.,  3.,  6.,  8., 15.,  8.,  3.,  7.,  7.,  8.,  5.,  5.,\n",
      "        5.,  1.,  4.,  2., 13.,  4., 13.,  9.,  6., 10.,  6.,  4.,  4.,\n",
      "        5., 10., 10.,  5., 11., 14.,  5.,  9., 14.,  7.,  9.,  8., 25.,\n",
      "        8.,  9.,  7.,  6.,  4.,  4.,  1.,  6., 21.,  9., 12.,  8., 10.,\n",
      "       16.,  5., 14., 10.,  4., 11.,  5.,  2.,  3.,  5.,  7.,  7.,  7.,\n",
      "        5.,  8.,  7., 13.,  8.,  5.,  5.,  9., 15., 14., 24., 22., 11.,\n",
      "        7., 17., 13., 25., 25., 26., 25., 15., 16., 15., 29., 16., 14.,\n",
      "       12., 13., 17., 17., 10., 14.,  9., 18., 19., 11., 17., 20., 15.,\n",
      "       15., 23., 15., 14., 17., 16., 14., 19., 14.,  9., 12., 19., 12.,\n",
      "       28., 14., 15., 29., 21., 14., 18., 11., 23., 17., 17., 22., 19.,\n",
      "       23., 20., 23., 30.,  8.,  6., 18., 18., 31., 39., 23., 13., 24.,\n",
      "       30., 28., 14., 19., 24., 24., 23.,  6., 12., 19., 21., 13., 14.,\n",
      "       32., 17., 16., 16., 32., 11., 11., 27., 22., 15., 16., 40., 29.,\n",
      "       22., 16., 21., 26., 28., 32., 24., 14., 18., 14., 19., 28., 23.,\n",
      "       21., 18., 14., 21., 15., 26., 30., 21., 20., 19., 19., 18., 22.,\n",
      "       30., 27., 16., 23., 16., 19., 36., 22., 17., 17., 21., 24., 27.,\n",
      "       31., 17., 17., 27., 21., 17., 22., 32., 31., 19., 17., 18., 12.,\n",
      "       13., 29., 22., 10., 10., 23., 23., 34., 27., 23., 21., 13., 26.,\n",
      "       20., 25., 20., 23., 38., 23., 15., 38., 26., 39., 26., 16., 27.,\n",
      "       14., 21., 21., 26., 38., 17., 12., 39., 20., 14., 21., 37., 20.,\n",
      "       17., 18., 25., 17., 27., 29., 32., 20., 18., 17., 13., 21., 25.,\n",
      "       12., 10., 19., 17.,  9., 24., 27., 17., 18., 25., 12., 21., 31.,\n",
      "       29., 12., 19., 24., 11., 17., 11.,  2., 14., 11., 15., 18., 26.,\n",
      "       30., 19., 24., 15., 19., 26., 17., 30., 44., 17., 25., 12., 15.,\n",
      "       19., 41., 33., 24., 21., 23., 22., 18., 18., 46., 20., 23., 23.,\n",
      "       23., 19., 23., 41., 41., 12., 21., 21., 22., 26., 41., 21., 14.,\n",
      "       17., 25., 24., 13., 23.,  9., 13., 19., 26., 18., 23., 37., 18.,\n",
      "       14., 15., 28., 18., 22., 34., 14., 19., 15.,  9., 20., 31., 36.,\n",
      "       18., 14., 17., 18., 20., 34., 36., 26., 13., 13., 14., 18., 21.,\n",
      "       19., 16., 12.,  9., 14., 17., 11., 21., 15., 15., 10., 21., 19.,\n",
      "       19., 33., 19., 19., 19., 12., 11., 20., 29., 15., 16., 12., 11.,\n",
      "       14., 28., 12., 11., 10., 13., 10., 16.,  9.,  8., 19., 15., 17.,\n",
      "       27., 22., 34., 26., 16., 21., 22., 19., 14.,  9., 20., 15., 13.,\n",
      "       12., 13., 13., 15., 19., 16., 13., 11., 10., 11.,  9., 16., 11.,\n",
      "       13.,  6., 11.,  9., 14., 14., 13., 28., 12., 14.,  7., 19., 11.,\n",
      "       14., 24., 13.,  8.,  7., 13., 32., 15., 10., 14., 22., 12.,  7.,\n",
      "       15.,  9., 10., 18.,  5., 15., 11., 25.,  4., 22., 17.,  5., 25.,\n",
      "       14., 11., 13., 16., 33., 27., 20., 25., 21.,  8., 12.,  9.,  9.,\n",
      "       14., 16., 16., 15., 17., 20., 14.,  8.,  9., 15., 13., 16., 16.,\n",
      "       21., 33., 19., 23., 15., 32., 21., 19., 21., 23., 24., 12., 15.,\n",
      "       28., 16., 24., 20., 31.,  6., 15., 27., 20., 29., 35., 42., 38.,\n",
      "       42., 22., 37., 23., 33., 37., 54., 28., 44., 42., 51., 45., 53.,\n",
      "       43., 24., 43., 33., 27., 38., 52., 30., 28., 43., 21., 37., 48.,\n",
      "       46., 65., 40., 39., 37.]), 'y_test_seq': array([  25.,   50.,   86.,   55.,   41.,   33.,   32.,   24.,   53.,\n",
      "         62.,   70.,   40.,   63.,   48.,   69.,   87.,   73.,   74.,\n",
      "         41.,   64.,   50.,   57.,   52.,  113.,   70.,   45.,   50.,\n",
      "         43.,   47.,   46.,   58.,   52.,   55.,   40.,   46.,   63.,\n",
      "         71.,   66.,   66.,   62.,   56.,   69.,   60.,   65.,   69.,\n",
      "         86.,   74.,   40.,   76.,   57.,   37.,   53.,   48.,   48.,\n",
      "         47.,   88.,   53.,   58.,  112.,   52.,   63.,   64.,   59.,\n",
      "         93.,   80.,  143.,   93.,   74.,   68.,  101.,   72.,   97.,\n",
      "        124.,   96.,   95.,   90.,   88.,   97.,  122.,  154.,   88.,\n",
      "         71.,   95.,   97.,   85.,  114.,  107.,   49.,  170.,   84.,\n",
      "        103.,  117.,  115.,   38.,   14.,   28.,   28.,   72.,   97.,\n",
      "        138.,   86.,   35.,  303.,  614.,  430.,  584.,  646.,  602.,\n",
      "        335.,  418.,  489.,  421.,  509.,  651.,  655.,  456.,  404.,\n",
      "        428.,  355.,  536.,  726.,  895.,  500.,  460.,  554.,  570.,\n",
      "        635.,  870.,  951.,  675.,  491.,  560.,  670.,  625.,  871.,\n",
      "        952.,  650.,  578.,  698.,  810.,  763.,  668.,  841.,  440.,\n",
      "        519.,  780.,  867.,  758.,  957.,  937.,  406.,  663.,  721.,\n",
      "        821.,  751., 1067.,  962.,  506.,  596.,  661.,  753.,  801.,\n",
      "       1109., 1173.,  664.,  744.,  787.,  873.,  861., 1250., 1352.,\n",
      "        649.,  650.,  679.,  654.,  762., 1118., 1319.,  830.,  721.,\n",
      "        780.,  732.,  745., 1212., 1280.,  775.,  812.,  969., 1048.,\n",
      "        710.,  769., 1110.,  557.,  433.,  668.,  824.,  831., 1124.,\n",
      "       1355.,  767.,  735.,  773., 1009.,  848., 1366., 1425.,  770.,\n",
      "        603.,  860.,  870.,  886., 1187., 1467.,  754.,  728.,  767.,\n",
      "        853.,  822., 1277., 1176.,  695.,  740.,  948., 1055., 1030.,\n",
      "       1272., 1128.,  672.,  682.,  864., 1063., 1054., 1675., 1551.,\n",
      "        893.,  127.]), 'n_features': 2, 'n_sequences': 1177}, 'Handhelds': {'X_train_seq': array([[[ 2.        ,  3.88145715],\n",
      "        [ 2.        ,  3.88145715],\n",
      "        [ 2.        ,  3.88145715],\n",
      "        ...,\n",
      "        [ 2.        ,  3.88145715],\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.51323487]],\n",
      "\n",
      "       [[ 2.        ,  3.88145715],\n",
      "        [ 2.        ,  3.88145715],\n",
      "        [ 2.        ,  3.88145715],\n",
      "        ...,\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.51323487]],\n",
      "\n",
      "       [[ 2.        ,  3.88145715],\n",
      "        [ 2.        ,  3.88145715],\n",
      "        [ 2.        ,  3.88145715],\n",
      "        ...,\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.51323487]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.1616595 ],\n",
      "        ...,\n",
      "        [ 9.        ,  0.22507339],\n",
      "        [ 9.        ,  0.40086107],\n",
      "        [ 9.        ,  0.64696383]],\n",
      "\n",
      "       [[ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.1616595 ],\n",
      "        [ 9.        , -0.23197458],\n",
      "        ...,\n",
      "        [ 9.        ,  0.40086107],\n",
      "        [ 9.        ,  0.64696383],\n",
      "        [ 9.        , -0.1616595 ]],\n",
      "\n",
      "       [[ 9.        , -0.1616595 ],\n",
      "        [ 9.        , -0.23197458],\n",
      "        [ 9.        ,  0.29538846],\n",
      "        ...,\n",
      "        [ 9.        ,  0.64696383],\n",
      "        [ 9.        , -0.1616595 ],\n",
      "        [ 9.        ,  0.50633368]]]), 'X_test_seq': array([[[ 9.        , -0.23197458],\n",
      "        [ 9.        ,  0.29538846],\n",
      "        [ 9.        , -0.33744719],\n",
      "        ...,\n",
      "        [ 9.        , -0.1616595 ],\n",
      "        [ 9.        ,  0.50633368],\n",
      "        [ 9.        ,  0.57664875]],\n",
      "\n",
      "       [[ 9.        ,  0.29538846],\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        ,  0.36570354],\n",
      "        ...,\n",
      "        [ 9.        ,  0.50633368],\n",
      "        [ 9.        ,  0.57664875],\n",
      "        [ 9.        ,  0.40086107]],\n",
      "\n",
      "       [[ 9.        , -0.33744719],\n",
      "        [ 9.        ,  0.36570354],\n",
      "        [ 9.        ,  0.22507339],\n",
      "        ...,\n",
      "        [ 9.        ,  0.57664875],\n",
      "        [ 9.        ,  0.40086107],\n",
      "        [ 9.        ,  0.92822412]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 5.        , 67.62207016],\n",
      "        [ 5.        , 57.70764497],\n",
      "        [ 5.        , 77.57165288],\n",
      "        ...,\n",
      "        [ 5.        , 68.32522088],\n",
      "        [ 5.        , 47.47680195],\n",
      "        [ 5.        , 55.77398048]],\n",
      "\n",
      "       [[ 5.        , 57.70764497],\n",
      "        [ 5.        , 77.57165288],\n",
      "        [ 5.        , 48.95341847],\n",
      "        ...,\n",
      "        [ 5.        , 47.47680195],\n",
      "        [ 5.        , 55.77398048],\n",
      "        [ 5.        , 47.79321978]],\n",
      "\n",
      "       [[ 5.        , 77.57165288],\n",
      "        [ 5.        , 48.95341847],\n",
      "        [ 5.        , 35.62871228],\n",
      "        ...,\n",
      "        [ 5.        , 55.77398048],\n",
      "        [ 5.        , 47.79321978],\n",
      "        [ 5.        , 37.35143155]]]), 'y_train_seq': array([ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        3.,  2.,  2.,  2.,  2.,  2.,  3.,  2.,  3.,  3.,  3.,  2.,  2.,\n",
      "        4.,  2.,  2.,  2.,  2.,  2.,  2.,  3.,  3.,  2.,  1.,  1.,  3.,\n",
      "        2.,  2.,  2.,  3.,  3.,  3.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,\n",
      "        1.,  3.,  3.,  3.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  2.,  1.,  1., 13.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  4.,  2.,  2.,  2.,  2.,  3.,  3.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  2., 11., 11., 11.,\n",
      "       11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
      "       11., 11., 11.,  5.,  5.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  3.,  3.,  3.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  6.,\n",
      "        6.,  2.,  2.,  2.,  1.,  1.,  3.,  4., 10.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,  3.,\n",
      "        3.,  3.,  3.,  3.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  4.,  7., 25.,  5.,\n",
      "        5., 14.,  5.,  4.,  3.,  7.,  8.,  5.,  6.,  6.,  5., 12., 13.,\n",
      "        6.,  7., 10., 10.,  6.,  7.,  8.,  7.,  8.,  5.,  2.,  4.,  1.,\n",
      "        5., 12., 17., 17.,  4., 11., 13.,  6.,  8.,  4.,  3.,  1.,  7.,\n",
      "       11.,  4.,  5.,  8., 10.,  3.,  8.,  6., 11.,  6.,  6., 11., 10.,\n",
      "        7.,  6.,  6.,  9.,  3.,  8.,  4.,  7.,  7., 12.,  9.,  1.,  8.,\n",
      "        6., 11.,  7.,  1.,  6.,  9.,  3.,  1.,  4.,  7.,  4., 13.,  8.,\n",
      "        3.,  5.,  8.,  1.,  4.,  2.,  2.,  9.,  7.,  9.,  9., 28.,  2.,\n",
      "        8., 10.,  3.,  5.,  5.,  9., 17., 13.,  3.,  1.,  8.,  8.,  2.,\n",
      "        4.,  5.,  8.,  3.,  7.,  5.,  5., 16.,  5.,  5.,  9.,  2.,  4.,\n",
      "       11., 14.,  2.,  6.,  7.,  4.,  5., 11.,  7.,  4.,  4.,  3., 14.,\n",
      "        5., 13.,  4.,  2.,  8.,  3., 10.,  6.,  8.,  6.,  1.,  4.,  2.,\n",
      "        2.,  5.,  7.,  6.,  7.,  1.,  1.,  2.,  8., 11.,  6.,  6., 10.,\n",
      "        3.,  7.,  4.,  7., 15.,  6.,  5.,  9.,  9.,  8., 10.,  8., 12.,\n",
      "        3.,  7., 10.,  2., 13.,  9.,  7.,  5.,  6.,  8.,  8.,  9.,  4.,\n",
      "        2.,  2.,  1.,  8.,  6., 14.,  3.,  7.,  5.,  2.,  1., 11., 12.,\n",
      "        7.,  4.,  4.,  5.,  5.,  4.,  8.,  4.,  2.,  4.,  4.,  5.,  5.,\n",
      "        9.,  4., 12.,  4.,  4., 10.,  3.,  3.,  2.,  5.,  2.,  8.,  8.,\n",
      "       10.,  2.,  8., 18., 10.,  9., 12.,  2., 13.,  1.,  3.,  1.,  1.,\n",
      "        1.,  5.,  8.,  7.,  9.,  8.,  5., 10.,  6., 24.,  5.,  8.,  4.,\n",
      "        6.,  4., 16.,  9., 15., 10., 10.,  4.,  7., 12.,  8.,  5.,  3.,\n",
      "        6.,  3.,  6.,  3., 14.,  6.,  6.,  3., 14., 12., 12., 14.,  4.,\n",
      "        3.,  5., 21., 11.,  4.,  6., 15.,  3.,  8.,  9.,  1., 10., 13.,\n",
      "        6.,  8.,  8.,  2.,  1.,  8., 11.,  7., 12.,  4.,  6., 14.,  2.,\n",
      "        5., 14., 12.,  5., 10., 13.,  3., 16.,  5.,  6., 12.,  7.,  7.,\n",
      "       16., 17., 10.,  8., 10.,  4.,  3., 14., 12.,  3.,  1.,  5.,  8.,\n",
      "        6., 16., 11.,  6.,  4.,  6.,  7.,  1.,  4.,  8.,  4.,  6., 10.,\n",
      "       13., 11.,  5.,  5., 12.,  3.,  6., 15.,  4.,  5.,  9.,  5., 14.,\n",
      "        1.,  9.,  8.,  5., 11.,  1., 10., 13.,  8.,  3., 12.,  5.,  7.,\n",
      "        8.,  4., 11., 18., 19., 23., 10., 21., 22., 19., 18., 25., 22.,\n",
      "       26., 35., 17., 19., 16., 15., 22., 19., 13.,  9., 20., 23., 19.,\n",
      "       15., 26., 19., 14., 20., 12., 17., 18., 17., 13., 21., 22., 21.,\n",
      "       27., 52., 16., 29., 27., 17., 24., 29., 23., 35., 16., 27., 22.,\n",
      "       22., 32., 25., 10., 20., 21., 24., 11., 20., 12., 27., 17., 15.,\n",
      "       19., 16., 24., 19., 21., 28.,  1.,  7.,  8.,  4., 17.,  6.,  6.,\n",
      "        7., 10., 12.,  6.,  9.,  1.,  9.,  5.,  6., 11., 10., 13.,  9.,\n",
      "        3.,  9., 13.,  2., 11., 12.,  7.,  7., 15., 10., 19., 11., 19.,\n",
      "       15.,  9.,  9., 13.,  3.,  7.,  4., 12., 10., 25.,  7., 27., 23.,\n",
      "       28., 35., 12., 31., 33.]), 'y_test_seq': array([  28.,   43.,   42.,   46.,   23.,   30.,   47.,   31.,   37.,\n",
      "         51.,   95.,   71.,   71.,   86.,   56.,  117.,   88.,   65.,\n",
      "         52.,   73.,   66.,   51.,   72.,   47.,   77.,   38.,   78.,\n",
      "         63.,   73.,   75.,   76.,   75.,   70.,   53.,   66.,   61.,\n",
      "         99.,   84.,   76.,   66.,   75.,   97.,   78.,   88.,   77.,\n",
      "         74.,   43.,   74.,   91.,   91.,   76.,   84.,   71.,   76.,\n",
      "         65.,   86.,   78.,   66.,   88.,   69.,   54.,   89.,  103.,\n",
      "        137.,  142.,   67.,   70.,  109.,  134.,  129.,   94.,  165.,\n",
      "        105.,   52.,  102.,  116.,  126.,  118.,  156.,   97.,   73.,\n",
      "        101.,  126.,  130.,  139.,  182.,   83.,   98.,  111.,  170.,\n",
      "        150.,  124.,   95.,   30.,    3.,   15.,   14.,  122.,  129.,\n",
      "        147.,  113.,   24.,  362.,  315.,  312.,  462.,  668.,  653.,\n",
      "        426.,  393.,  562.,  579.,  756.,  887.,  827.,  630.,  471.,\n",
      "        644.,  651.,  809., 1056.,  940.,  604.,  531.,  716.,  750.,\n",
      "        798., 1002.,  871.,  769.,  669.,  721.,  957.,  951., 1325.,\n",
      "        969.,  702.,  920., 1097., 1146., 1262., 1391.,  943.,  785.,\n",
      "        822., 1011., 1028., 1065., 1371., 1148.,  764.,  809., 1143.,\n",
      "       1246., 1249., 1538., 1196.,  867.,  833., 1097., 1263., 1388.,\n",
      "       1625., 1471., 1006., 1083., 1239., 1288., 1309., 1815., 1217.,\n",
      "        838.,  819., 1040., 1284., 1369., 1765., 1460., 1198.,  932.,\n",
      "       1048., 1330., 1432., 1618., 1389.,  984., 1079., 1295., 1569.,\n",
      "        904., 1033., 1015.,  868.,  913., 1053., 1331., 1261., 1660.,\n",
      "       1552., 1012., 1016., 1112., 1425., 1358., 1791., 1499., 1047.,\n",
      "        966., 1266., 1287., 1399., 1742., 1465., 1193.,  960.,  969.,\n",
      "       1347., 1457., 1851., 2145., 1355., 1078., 1513., 1940., 1658.,\n",
      "       2223., 1409., 1030., 1134., 1465., 1960., 1367., 1603., 1376.,\n",
      "       1079.,  121.]), 'n_features': 2, 'n_sequences': 1177}, 'Main Courses': {'X_train_seq': array([[[ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        ...,\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193]],\n",
      "\n",
      "       [[ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        ...,\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193]],\n",
      "\n",
      "       [[ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        ...,\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        ,  0.11960078],\n",
      "        [ 9.        ,  0.78759397],\n",
      "        [ 9.        ,  0.54149122],\n",
      "        ...,\n",
      "        [ 9.        ,  0.40086107],\n",
      "        [ 9.        ,  0.47117615],\n",
      "        [ 9.        ,  0.04928571]],\n",
      "\n",
      "       [[ 9.        ,  0.78759397],\n",
      "        [ 9.        ,  0.54149122],\n",
      "        [ 9.        ,  0.04928571],\n",
      "        ...,\n",
      "        [ 9.        ,  0.47117615],\n",
      "        [ 9.        ,  0.04928571],\n",
      "        [ 9.        ,  0.08444325]],\n",
      "\n",
      "       [[ 9.        ,  0.54149122],\n",
      "        [ 9.        ,  0.04928571],\n",
      "        [ 9.        , -0.09134443],\n",
      "        ...,\n",
      "        [ 9.        ,  0.04928571],\n",
      "        [ 9.        ,  0.08444325],\n",
      "        [ 9.        ,  0.18991586]]]), 'X_test_seq': array([[[ 9.        ,  0.04928571],\n",
      "        [ 9.        , -0.09134443],\n",
      "        [ 9.        ,  0.08444325],\n",
      "        ...,\n",
      "        [ 9.        ,  0.08444325],\n",
      "        [ 9.        ,  0.18991586],\n",
      "        [ 9.        ,  0.54149122]],\n",
      "\n",
      "       [[ 9.        , -0.09134443],\n",
      "        [ 9.        ,  0.08444325],\n",
      "        [ 9.        ,  0.11960078],\n",
      "        ...,\n",
      "        [ 9.        ,  0.18991586],\n",
      "        [ 9.        ,  0.54149122],\n",
      "        [ 9.        ,  0.08444325]],\n",
      "\n",
      "       [[ 9.        ,  0.08444325],\n",
      "        [ 9.        ,  0.11960078],\n",
      "        [ 9.        ,  0.40086107],\n",
      "        ...,\n",
      "        [ 9.        ,  0.54149122],\n",
      "        [ 9.        ,  0.08444325],\n",
      "        [ 9.        ,  0.26023093]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 5.        , 30.39023939],\n",
      "        [ 5.        , 30.53086954],\n",
      "        [ 5.        , 46.70333616],\n",
      "        ...,\n",
      "        [ 5.        , 32.04264359],\n",
      "        [ 5.        , 36.26154793],\n",
      "        [ 5.        , 35.24197938]],\n",
      "\n",
      "       [[ 5.        , 30.53086954],\n",
      "        [ 5.        , 46.70333616],\n",
      "        [ 5.        , 33.69504779],\n",
      "        ...,\n",
      "        [ 5.        , 36.26154793],\n",
      "        [ 5.        , 35.24197938],\n",
      "        [ 5.        , 30.14413664]],\n",
      "\n",
      "       [[ 5.        , 46.70333616],\n",
      "        [ 5.        , 33.69504779],\n",
      "        [ 5.        , 28.52688998],\n",
      "        ...,\n",
      "        [ 5.        , 35.24197938],\n",
      "        [ 5.        , 30.14413664],\n",
      "        [ 5.        , 31.40980794]]]), 'y_train_seq': array([121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n",
      "       121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n",
      "        13.,   6.,   6.,   6.,   6.,   6.,   6.,   1.,   3.,   3.,   3.,\n",
      "         3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,\n",
      "         3.,   3.,   3.,   3.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   2.,   2.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   3.,   3.,   3.,\n",
      "         3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   2.,   2.,   2.,\n",
      "         7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,\n",
      "         7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   2.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   3.,\n",
      "         3.,   1.,   3.,   2.,   2.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   2.,   2.,   2.,   2.,   2.,\n",
      "         2.,   2.,   1.,   2.,   2.,   2.,   1.,   1.,   2.,   2.,   2.,\n",
      "         2.,   1.,   3.,   3.,   3.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   2.,   2.,   8.,   5.,\n",
      "         4.,   5.,   3.,  11.,  11.,   4.,  11.,  10.,   6.,   5.,   7.,\n",
      "         7.,   3.,   6.,   4.,   8.,  14.,  15.,  10.,  12.,  12.,  10.,\n",
      "         6.,  16.,   5.,   6.,  18.,  14.,   9.,  19.,   8.,   6.,   8.,\n",
      "         9.,   7.,  11.,   6.,   4.,   6.,   9.,   7.,   8.,  12.,   9.,\n",
      "         8.,   3.,  10.,   8.,   4.,  19.,   6.,   5.,   5.,   6.,  15.,\n",
      "         3.,   9.,   7.,   4.,   4.,   8.,  26.,   3.,  24.,  13.,  15.,\n",
      "        20.,  22.,  40.,  15.,  28.,  34.,  32.,  26.,  31.,  30.,  16.,\n",
      "        21.,  20.,  23.,  13.,  35.,  50.,  16.,   9.,  36.,  29.,  36.,\n",
      "        26.,  27.,   6.,  27.,  28.,  27.,  33.,  33.,  24.,  32.,  30.,\n",
      "        33.,  32.,  24.,  27.,  42.,  22.,  20.,  21.,  25.,  14.,  31.,\n",
      "        16.,  31.,  15.,  45.,  22.,  45.,  17.,  32.,  20.,  21.,  33.,\n",
      "        19.,  28.,  22.,  36.,  11.,  20.,  17.,  14.,  20.,  17.,  10.,\n",
      "        17.,  10.,  20.,  22.,  13.,  15.,  16.,  21.,   6.,  10.,  22.,\n",
      "        16.,  32.,  16.,  16.,   7.,  15.,  26.,  29.,  33.,  22.,   3.,\n",
      "         3.,   2.,  13.,  21.,  23.,  24.,   3.,  26.,  41.,  11.,   9.,\n",
      "        24.,  41.,  14.,  17.,  28.,  21.,   6.,  16.,  21.,  10.,  22.,\n",
      "        16.,  10.,  12.,  10.,  21.,  24.,  19.,  17.,   3.,  30.,  22.,\n",
      "        18.,  28.,  13.,  16.,  28.,  10.,   2.,  30.,  13.,  14.,  12.,\n",
      "         7.,  15.,   9.,  21.,  17.,  15.,  18.,  10.,  17.,  11.,  10.,\n",
      "         6.,  15.,   6.,   7.,   8.,  20.,   7.,  14.,  12.,   9.,  19.,\n",
      "        19.,  12.,  19.,   8.,   7.,  12.,  12.,  13.,   4.,  16.,   9.,\n",
      "         7.,  15.,  11.,   8.,   8.,  12.,   6.,  16.,   6.,   5.,  12.,\n",
      "        10.,  24.,   8.,  16.,   6.,  23.,   8.,  21.,  23.,   7.,  10.,\n",
      "        13.,   8.,   6.,   6.,  10.,   9.,  20.,  21.,   7.,  13.,   9.,\n",
      "        17.,   2.,  17.,  11.,  10.,  12.,  13.,  11.,   5.,  17.,  13.,\n",
      "        14.,  19.,  10.,  16.,   5.,  13.,   3.,   5.,   9.,  19.,  21.,\n",
      "        10.,  18.,  16.,   4.,  11.,  18.,  10.,   5.,  21.,   7.,  21.,\n",
      "        12.,  13.,   5.,   6.,  24.,  20.,  12.,  26.,  43.,  56.,  85.,\n",
      "        44.,  96.,  24.,  56.,  51.,  53.,  65.,  49.,  73.,  44.,  65.,\n",
      "        31.,  55.,  43.,  31.,  40.,  37.,  33.,  39.,  41.,  72.,  62.,\n",
      "        42.,  35.,  32.,  30.,  45.,  54.,  44.,  42.,  51.,  86.,  84.,\n",
      "       101., 109., 114.,  43.,  52.,  45.,  85.,  94.,  67.,  83.,  38.,\n",
      "        49.,  42.,  54.,  87., 114.,  58.,  61.,  38.,  43.,  66.,  94.,\n",
      "        48.,  60.,  38.,  41.,  37.,  43.,  57.,  56.,  69.,  85.,  16.,\n",
      "         7.,  20.,  12.,   9.,  10.,  10.,  32.,  16.,   7.,  17.,  21.,\n",
      "         8.,  20.,  13.,  14.,  16.,  11.,  11.,  11.,   7.,  10.,  16.,\n",
      "        22.,  78.,  23.,  74.,  25.,  19.,  14.,  35.,  76.,  39.,  41.,\n",
      "        26.,  16.,  10.,  36.,  20.,  39.,  32.,  18.,  14.,  19.,  20.,\n",
      "        28.,  30.,  18.,  19.,  22.,  32.]), 'y_test_seq': array([  19.,   24.,   39.,   28.,   21.,   26.,   21.,   16.,   41.,\n",
      "         45.,   45.,   26.,   30.,   42.,   37.,  192.,   81.,   76.,\n",
      "         37.,   71.,   66.,   46.,   74.,   58.,   60.,   34.,   37.,\n",
      "         71.,   49.,   38.,   66.,   58.,   54.,   55.,   63.,   64.,\n",
      "         80.,   65.,  106.,   60.,   77.,   93.,   68.,   84.,   57.,\n",
      "         77.,   37.,   66.,   52.,   70.,   53.,   72.,   66.,   51.,\n",
      "         54.,  103.,   77.,   81.,   72.,   76.,   51.,   69.,   54.,\n",
      "        115.,  135.,   84.,   57.,   45.,   76.,   81.,  112.,   92.,\n",
      "        123.,   85.,   57.,   76.,   86.,  105.,  115.,  127.,  129.,\n",
      "         67.,   75.,   85.,   72.,  145.,  129.,  116.,   75.,   75.,\n",
      "        137.,  101.,  118.,   73.,    4.,   41.,   43.,  100.,  155.,\n",
      "        211.,  162.,   15.,  293.,  244.,  232.,  312.,  610.,  528.,\n",
      "        394.,  312.,  432.,  481.,  586.,  730.,  643.,  600.,  375.,\n",
      "        479.,  440.,  532.,  926.,  806.,  541.,  353.,  508.,  488.,\n",
      "        629.,  842.,  951.,  810.,  425.,  538.,  711.,  601., 1091.,\n",
      "        930.,  642.,  435.,  584.,  637.,  687.,  845.,  751.,  630.,\n",
      "        440.,  583.,  654.,  687.,  955.,  929.,  661.,  427.,  487.,\n",
      "        625.,  658.,  893.,  961.,  710.,  497.,  582.,  693.,  845.,\n",
      "       1084., 1015.,  777.,  497.,  613.,  641.,  692., 1085., 1036.,\n",
      "        730.,  443.,  533.,  683.,  700., 1043.,  946.,  806.,  452.,\n",
      "        651.,  615.,  783.,  980.,  888.,  704.,  511.,  725.,  955.,\n",
      "        723.,  772.,  724.,  693.,  774.,  469.,  603.,  662., 1032.,\n",
      "        941.,  739.,  567.,  641.,  760.,  747., 1198.,  965.,  829.,\n",
      "        492.,  677.,  659.,  753., 1126., 1010.,  814.,  484.,  620.,\n",
      "        631.,  717., 1081.,  924.,  773.,  526.,  820.,  881.,  885.,\n",
      "       1345.,  975.,  828.,  591.,  724.,  928., 1048., 1019.,  874.,\n",
      "        910.,   38.]), 'n_features': 2, 'n_sequences': 1177}, 'Salads & Greens': {'X_train_seq': array([[[ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        ...,\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643]],\n",
      "\n",
      "       [[ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        ...,\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643]],\n",
      "\n",
      "       [[ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        ...,\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.44291979],\n",
      "        ...,\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.47807733],\n",
      "        [ 9.        , -0.40776226]],\n",
      "\n",
      "       [[ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.44291979],\n",
      "        ...,\n",
      "        [ 9.        , -0.47807733],\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.47807733]],\n",
      "\n",
      "       [[ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.47807733],\n",
      "        ...,\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.47807733],\n",
      "        [ 9.        , -0.5483924 ]]]), 'X_test_seq': array([[[ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.47807733],\n",
      "        [ 9.        , -0.5483924 ],\n",
      "        ...,\n",
      "        [ 9.        , -0.47807733],\n",
      "        [ 9.        , -0.5483924 ],\n",
      "        [ 9.        , -0.40776226]],\n",
      "\n",
      "       [[ 9.        , -0.47807733],\n",
      "        [ 9.        , -0.5483924 ],\n",
      "        [ 9.        , -0.40776226],\n",
      "        ...,\n",
      "        [ 9.        , -0.5483924 ],\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.44291979]],\n",
      "\n",
      "       [[ 9.        , -0.5483924 ],\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.44291979],\n",
      "        ...,\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.44291979]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 5.        ,  7.08079294],\n",
      "        [ 5.        ,  6.69406004],\n",
      "        [ 5.        ,  9.11993003],\n",
      "        ...,\n",
      "        [ 5.        ,  6.69406004],\n",
      "        [ 5.        ,  3.38925164],\n",
      "        [ 5.        ,  5.18228599]],\n",
      "\n",
      "       [[ 5.        ,  6.69406004],\n",
      "        [ 5.        ,  9.11993003],\n",
      "        [ 5.        ,  4.44397773],\n",
      "        ...,\n",
      "        [ 5.        ,  3.38925164],\n",
      "        [ 5.        ,  5.18228599],\n",
      "        [ 5.        ,  5.04165584]],\n",
      "\n",
      "       [[ 5.        ,  9.11993003],\n",
      "        [ 5.        ,  4.44397773],\n",
      "        [ 5.        ,  2.8618886 ],\n",
      "        ...,\n",
      "        [ 5.        ,  5.18228599],\n",
      "        [ 5.        ,  5.04165584],\n",
      "        [ 5.        ,  4.26819005]]]), 'y_train_seq': array([38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "       38., 38., 38., 38., 38., 38., 38., 38., 38.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  4.,  4.,\n",
      "        1.,  2.,  2.,  2.,  2.,  5.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
      "        3.,  3.,  3.,  3.,  3.,  3.,  3.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  4.,  1.,\n",
      "        1.,  1.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  1.,  2.,\n",
      "        2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  4.,  4.,  4.,\n",
      "        2.,  2.,  3.,  3.,  3.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,\n",
      "        1.,  1.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  3.,\n",
      "        2.,  2.,  2.,  6.,  6.,  1.,  2.,  2.,  2.,  2.,  1.,  2.,  2.,\n",
      "        3.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  3.,  1.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  3.,  2.,  2.,  2.,  1.,  3.,  3.,\n",
      "        3.,  3.,  3.,  3.,  1.,  1.,  5.,  5.,  3.,  3.,  2.,  2.,  2.,\n",
      "        2.,  2.,  1.,  1.,  1.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  2.,  2.,  2.,  3.,  3.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  3.,  3.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  1.,\n",
      "        2.,  1.,  1.,  2.,  4.,  1.,  1.,  2.,  2.,  4.,  2.,  1.,  1.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  3.,  3.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        3.,  3.,  3.,  1.,  3.,  4.,  4.,  1.,  2.,  2.,  1.,  4.,  2.,\n",
      "        2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  2.,  2.,  1.,  3.,  1.,\n",
      "        3.,  3.,  1.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  2.,  1.,  1.,  2.,  2.,  1.,  2.,  2.,  2.,  2.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  2.,  1.,  1.,  4.,  4.,  4.,  1.,\n",
      "        1.,  2.,  1.,  1.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  3.,  4.,  1.,  1.,  1.,  1.,  2.,  1.,  1.,  1.,\n",
      "        1.,  1.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,\n",
      "        1.,  2.,  2.,  6.,  6.,  6.,  6.,  6.,  3.,  7.,  3.,  1.,  1.,\n",
      "        1.,  4.,  8.,  2.,  2.,  2.,  1.,  1.,  1.,  2.,  3.,  1.,  4.,\n",
      "        5.,  2.,  1.,  4.,  4.,  2.,  2.,  3.,  1.,  1.,  2.,  5.,  2.,\n",
      "        3.,  3.,  3.,  2.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  2.,  2.,  2.,  1.,  1.,  3.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        7.,  3.,  1.,  4.,  1.,  1.,  2.,  4.,  4.,  3.,  3.,  3.,  3.,\n",
      "        3.,  3.,  1.,  4.,  3.,  3.,  1.,  5.,  2.,  2.,  2.,  9.,  6.,\n",
      "        1.,  1.,  3.,  5.,  5.,  5.,  5.,  4.,  4.,  3.,  1.,  5.,  4.,\n",
      "        3.,  5.,  3.,  1.,  5.]), 'y_test_seq': array([  4.,   4.,   7.,   7.,   2.,   2.,   1.,   6.,   2.,   1.,   2.,\n",
      "         1.,   1.,   2.,   2.,   6.,   4.,   3.,   3.,  13.,  16.,   2.,\n",
      "         4.,   3.,   3.,   6.,   4.,   4.,   1.,   2.,   5.,   4.,   1.,\n",
      "         3.,   5.,   2.,   5.,   1.,   8.,   4.,   1.,   4.,   2.,   2.,\n",
      "         6.,   4.,   3.,   2.,   5.,   3.,   5.,   5.,   1.,   1.,   1.,\n",
      "         1.,   1.,   3.,   2.,   3.,  23.,  31.,  30.,  28.,   5.,   3.,\n",
      "         2.,  29.,  21.,  18.,  15.,   6.,  13.,   3.,  15.,  16.,  22.,\n",
      "        18.,   4.,   4.,   9.,  23.,   9.,  15.,  20.,   9.,  13.,   4.,\n",
      "         8.,  15.,  12.,  10.,  11.,   9.,  16.,   1.,   1.,  17.,  15.,\n",
      "         9.,   7.,  22.,  26.,  40.,  41.,  53., 101.,  83.,  26.,  71.,\n",
      "        57.,  97.,  91., 113.,  73.,  58.,  76., 106.,  85.,  84., 103.,\n",
      "        78.,  62.,  81., 129.,  98., 120., 137.,  92.,  87., 122., 135.,\n",
      "       165., 133., 144., 111.,  79., 155., 133., 153., 152., 148., 106.,\n",
      "        71.,  99., 103., 110.,  99., 148., 107.,  68., 132., 148., 170.,\n",
      "       158., 166., 120.,  91., 121., 183., 172., 161., 192., 111.,  88.,\n",
      "       143., 188., 165., 135., 237., 141.,  71., 142., 140., 181., 136.,\n",
      "       190., 138.,  58., 123., 166., 150., 164., 157., 139.,  47., 114.,\n",
      "       122., 145.,  89.,  81.,  77.,  56.,  81., 130., 146., 186., 167.,\n",
      "       105.,  96., 156., 172., 183., 172., 220., 137., 112., 150., 182.,\n",
      "       144., 184., 169., 109.,  53., 149., 158., 163., 195., 222., 205.,\n",
      "       118., 205., 228., 218., 207., 276., 143.,  98., 177., 215., 207.,\n",
      "       113., 164., 160., 138.,  46.]), 'n_features': 2, 'n_sequences': 1177}, 'Breakfast & Brunch': {'X_train_seq': array([[[ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        ...,\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672]],\n",
      "\n",
      "       [[ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        ...,\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672]],\n",
      "\n",
      "       [[ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        ...,\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        , -0.23197458],\n",
      "        [ 9.        , -0.0561869 ],\n",
      "        [ 9.        , -0.30228965],\n",
      "        ...,\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.19681704],\n",
      "        [ 9.        ,  0.18991586]],\n",
      "\n",
      "       [[ 9.        , -0.0561869 ],\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.19681704],\n",
      "        ...,\n",
      "        [ 9.        , -0.19681704],\n",
      "        [ 9.        ,  0.18991586],\n",
      "        [ 9.        , -0.30228965]],\n",
      "\n",
      "       [[ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.19681704],\n",
      "        [ 9.        , -0.12650197],\n",
      "        ...,\n",
      "        [ 9.        ,  0.18991586],\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.33744719]]]), 'X_test_seq': array([[[ 9.        , -0.19681704],\n",
      "        [ 9.        , -0.12650197],\n",
      "        [ 9.        , -0.44291979],\n",
      "        ...,\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.33744719]],\n",
      "\n",
      "       [[ 9.        , -0.12650197],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.19681704],\n",
      "        ...,\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.23197458]],\n",
      "\n",
      "       [[ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.19681704],\n",
      "        [ 9.        , -0.30228965],\n",
      "        ...,\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.23197458],\n",
      "        [ 9.        , -0.26713211]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 5.        ,  4.44397773],\n",
      "        [ 5.        ,  3.10799136],\n",
      "        [ 5.        ,  4.97134077],\n",
      "        ...,\n",
      "        [ 5.        ,  3.21346396],\n",
      "        [ 5.        ,  9.54182047],\n",
      "        [ 5.        ,  7.15110801]],\n",
      "\n",
      "       [[ 5.        ,  3.10799136],\n",
      "        [ 5.        ,  4.97134077],\n",
      "        [ 5.        ,  8.3113067 ],\n",
      "        ...,\n",
      "        [ 5.        ,  9.54182047],\n",
      "        [ 5.        ,  7.15110801],\n",
      "        [ 5.        ,  8.97929989]],\n",
      "\n",
      "       [[ 5.        ,  4.97134077],\n",
      "        [ 5.        ,  8.3113067 ],\n",
      "        [ 5.        ,  0.61180629],\n",
      "        ...,\n",
      "        [ 5.        ,  7.15110801],\n",
      "        [ 5.        ,  8.97929989],\n",
      "        [ 5.        ,  3.45956672]]]), 'y_train_seq': array([46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46.,\n",
      "       46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46.,\n",
      "       46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46.,\n",
      "       46., 46., 46., 46., 46.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
      "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
      "        5.,  5.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,\n",
      "        1.,  1.,  1.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
      "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  2.,  2.,  2.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  2.,  4.,  3., 11., 17., 21., 14.,\n",
      "        7., 10.,  7., 21., 30., 24., 14., 13., 10., 12., 10.,  9., 18.,\n",
      "       10., 14.,  3.,  8.,  8., 19., 14., 17.,  9.,  2.,  7., 13., 12.,\n",
      "       23., 12.,  9.,  4.,  5.,  8., 11., 13., 13.,  7.,  5.,  6., 10.,\n",
      "        9., 16., 17., 15., 23., 19., 16., 19., 23., 17., 11., 11., 12.,\n",
      "       18., 15., 20., 24.,  4.,  5., 15., 16., 17.,  8., 21., 17., 14.,\n",
      "       24., 11., 14., 27.,  9.,  8.,  8.,  8., 20., 13., 26., 14., 17.,\n",
      "       17., 14., 17., 14., 22., 15.,  9., 10., 16., 16., 16., 30., 27.,\n",
      "       15.,  9., 13., 10., 13., 31., 20.,  6.,  5., 16.,  8., 15., 11.,\n",
      "       11.,  4.,  8., 11., 22., 19., 21., 14., 11.,  6.,  6., 11., 14.,\n",
      "       26., 16.,  6.,  4.,  5.,  6., 19., 14., 27., 13.,  7., 13.,  4.,\n",
      "        8., 10., 16.,  9.,  3.,  7.,  7., 15., 13.,  8.,  5., 10.,  2.,\n",
      "       10.,  6., 20., 19.,  6.,  4., 11.,  4., 11., 26., 16.,  4.,  7.,\n",
      "        8.,  7.,  9., 13., 17., 11., 17., 18., 13., 19., 10., 19., 14.,\n",
      "        6.,  5.,  6.,  9., 24., 22.,  9.,  8.,  2.,  4.,  1., 19., 16.,\n",
      "        3.,  5.,  6.,  9., 16., 14., 29., 12.,  4.,  4.,  6.,  4., 29.,\n",
      "       32.,  4.,  3., 12.,  7.,  4., 19., 22., 11.,  1.,  5., 11., 11.,\n",
      "       21.,  8., 12.,  9.,  5.,  8.,  6.,  6., 14.,  7., 12., 15., 12.,\n",
      "       26., 15., 11.,  2.,  3., 10.,  1., 10., 14., 27.,  9.,  1.,  5.,\n",
      "        3.,  8.,  7., 27.,  9.,  1., 12.,  8.,  3., 30., 23.,  3.,  4.,\n",
      "       10.,  9.,  4., 12., 36.,  7.,  7.,  4.,  5.,  5., 17.,  9.,  5.,\n",
      "        9.,  5.,  6.,  6., 11., 11.,  5., 14.,  9.,  6.,  6., 23., 20.,\n",
      "        4.,  6., 11., 14., 11., 20., 15.,  6.,  9.,  7.,  9., 17., 19.,\n",
      "       24.,  2., 11.,  8.,  5., 10., 15., 19.,  7.,  5.,  9.,  5., 12.,\n",
      "       18., 18., 15.,  5.,  1.,  2., 11., 21., 18.,  9.,  9., 10.,  7.,\n",
      "        4., 20., 12.,  5., 12.,  7., 15., 23., 22., 23.,  6., 12., 11.,\n",
      "       10., 15., 18., 29., 12., 15.,  9.,  3.,  3., 27.,  9.,  9., 18.,\n",
      "       22., 14., 21., 14., 39., 18., 21.,  7., 10., 22., 22., 14., 12.,\n",
      "        5.,  5.,  4.,  8., 12., 17., 12.,  4.,  7., 15., 15., 22., 11.,\n",
      "        9.,  1.,  7.,  4., 14., 18., 17., 17.,  2., 15.,  3., 14.,  7.,\n",
      "        7., 13.,  4.,  8.,  6.,  6., 18., 29.,  8.,  3.,  5.,  5., 12.,\n",
      "       13., 16.,  5., 17., 16.,  5.,  2., 21., 11.,  8., 13., 10., 11.,\n",
      "        6., 17., 11., 13., 15., 10., 15., 17., 14.,  2., 14., 15., 10.,\n",
      "        7., 17., 33.,  7.,  9., 17.,  4., 23., 20., 11.,  9., 15., 10.,\n",
      "       14.,  5., 16., 21., 13., 25., 10., 25., 10.,  8., 18.,  6.,  9.,\n",
      "       19., 10.,  9., 16., 15.,  6.,  6.,  6., 10.,  7.,  9.,  6., 13.,\n",
      "        7.,  4., 22., 10.,  7., 12., 18.,  7.,  5.,  4., 15., 10., 20.,\n",
      "        5.,  4.,  2.,  7.,  7., 10., 15.,  8., 11., 13.,  4., 11.,  8.,\n",
      "       11., 22.,  8.,  7.,  7.]), 'y_test_seq': array([ 10.,   9.,  16.,  14.,   4.,   5.,   7.,  12.,   9.,  10.,  25.,\n",
      "        11.,  12.,  12.,  22.,  24.,  13.,  12.,   5.,   5.,   9.,   7.,\n",
      "        10.,  15.,  10.,   9.,  14.,  22.,  13.,  22.,  24.,  14.,   9.,\n",
      "        14.,   9.,   6.,   7.,  17.,  20.,  18.,   8.,   8.,  13.,  15.,\n",
      "        21.,  26.,   6.,   4.,   6.,   8.,  12.,  15.,  18.,   5.,   5.,\n",
      "         6.,   2.,  12.,  21.,  25.,  15.,   4.,   4.,  11.,   5.,  23.,\n",
      "        33.,   8.,  19.,  19.,   6.,   8.,  24.,  25.,   6.,  13.,   5.,\n",
      "        14.,   8.,  17.,  21.,   4.,   6.,  10.,   9.,   8.,  23.,  11.,\n",
      "         3.,   1.,   6.,   6.,   7.,   3.,  12.,  14.,   7.,  14.,   9.,\n",
      "        10.,  14.,  22.,  14.,  19.,  12.,  58.,  66.,  24.,  17.,  25.,\n",
      "        35.,  54.,  52.,  66.,  76.,  15.,  76.,  54.,  50.,  42.,  96.,\n",
      "        91.,  12.,  75.,  61.,  79.,  73., 128., 115.,  25., 110.,  45.,\n",
      "        92.,  99., 116., 100.,  19.,  81.,  49.,  84.,  93., 100., 100.,\n",
      "        37.,  58.,  75.,  75.,  97., 213., 291.,  32.,  91.,  80., 121.,\n",
      "        73., 149., 226.,  26.,  72.,  46., 108., 116., 181., 286.,  45.,\n",
      "       175., 240., 114., 103., 227., 290.,  59.,  98.,  66.,  83.,  73.,\n",
      "       236., 264.,  47., 100.,  88.,  76.,  85., 171., 295.,  20.,  75.,\n",
      "        98., 128., 230., 187., 215., 218.,  19.,  79.,  88.,  97., 209.,\n",
      "       232.,  30.,  93.,  72., 157., 126., 478., 614.,  28.,  93.,  81.,\n",
      "       132.,  84., 220., 262.,  28., 104., 127.,  92., 104., 227., 298.,\n",
      "        71.,  87., 117., 143., 105., 158., 253.,  34., 102., 110., 108.,\n",
      "       288., 220., 272., 115.,  67.]), 'n_features': 2, 'n_sequences': 1177}, 'Sushi & Asian': {'X_train_seq': array([[[ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        ...,\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498]],\n",
      "\n",
      "       [[ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        ...,\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498]],\n",
      "\n",
      "       [[ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        ...,\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        ,  0.22507339],\n",
      "        [ 9.        , -0.23197458],\n",
      "        [ 9.        , -0.26713211],\n",
      "        ...,\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.12650197],\n",
      "        [ 9.        , -0.30228965]],\n",
      "\n",
      "       [[ 9.        , -0.23197458],\n",
      "        [ 9.        , -0.26713211],\n",
      "        [ 9.        , -0.30228965],\n",
      "        ...,\n",
      "        [ 9.        , -0.12650197],\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.44291979]],\n",
      "\n",
      "       [[ 9.        , -0.26713211],\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.23197458],\n",
      "        ...,\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.44291979]]]), 'X_test_seq': array([[[ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.23197458],\n",
      "        [ 9.        , -0.33744719],\n",
      "        ...,\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.33744719]],\n",
      "\n",
      "       [[ 9.        , -0.23197458],\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.12650197],\n",
      "        ...,\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.47807733]],\n",
      "\n",
      "       [[ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.12650197],\n",
      "        [ 9.        , -0.44291979],\n",
      "        ...,\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.47807733],\n",
      "        [ 9.        , -0.47807733]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 5.        ,  1.52590223],\n",
      "        [ 5.        ,  1.17432687],\n",
      "        [ 5.        ,  2.29936802],\n",
      "        ...,\n",
      "        [ 5.        ,  1.17432687],\n",
      "        [ 5.        ,  1.27979948],\n",
      "        [ 5.        ,  1.94779266]],\n",
      "\n",
      "       [[ 5.        ,  1.17432687],\n",
      "        [ 5.        ,  2.29936802],\n",
      "        [ 5.        ,  2.08842281],\n",
      "        ...,\n",
      "        [ 5.        ,  1.27979948],\n",
      "        [ 5.        ,  1.94779266],\n",
      "        [ 5.        ,  1.80716252]],\n",
      "\n",
      "       [[ 5.        ,  2.29936802],\n",
      "        [ 5.        ,  2.08842281],\n",
      "        [ 5.        ,  1.45558716],\n",
      "        ...,\n",
      "        [ 5.        ,  1.94779266],\n",
      "        [ 5.        ,  1.80716252],\n",
      "        [ 5.        ,  2.3696831 ]]]), 'y_train_seq': array([67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67.,\n",
      "       67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67.,\n",
      "       67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67.,\n",
      "       67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67.,\n",
      "       67.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4., 20.,\n",
      "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 14., 14., 14.,\n",
      "        8.,  8.,  8.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  6.,  6.,  6.,\n",
      "        6.,  6.,  7.,  7.,  7.,  8.,  8.,  8.,  8.,  8.,  6.,  6.,  6.,\n",
      "        6.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  1.,  1.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
      "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
      "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
      "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
      "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
      "        5.,  5.,  5.,  5.,  5.,  5.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  1.,  1.,\n",
      "        1.,  1.,  1.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  2.,  2.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  4.,  4.,  2.,  4.,  5.,  4.,  4.,\n",
      "        4.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  3.,\n",
      "        3.,  1.,  5.,  2.,  2.,  1.,  2.,  2.,  1.,  2.,  2., 25., 23.,\n",
      "        8.,  6.,  6., 10., 10., 23., 10.,  9.,  8., 10.,  7., 13.,  4.,\n",
      "       13.,  8.,  4.,  4.,  7.]), 'y_test_seq': array([  3.,   3.,   2.,  10.,   1.,  11.,  11.,  11.,   8.,   4.,   4.,\n",
      "         2.,   4.,   5.,   3.,  22.,  10.,  10.,   1.,  16.,  12.,   1.,\n",
      "         7.,  12.,   9.,   2.,   4.,   2.,   2.,   6.,   5.,   2.,   9.,\n",
      "         4.,   6.,   9.,  15.,   5.,   4.,   5.,  10.,   6.,   7.,  19.,\n",
      "        11.,   8.,   1.,   2.,   5.,   8.,   4.,  18.,  11.,  17.,   1.,\n",
      "         3.,  31.,   8.,   9.,   4.,   2.,   5.,   4.,  10.,  13.,   6.,\n",
      "         7.,   7.,   2.,   5.,   6.,   7.,  10.,   2.,   4.,   6.,   7.,\n",
      "         9.,   7.,   7.,   7.,   7.,   3.,  10.,   7.,   6.,  10.,   6.,\n",
      "         7.,   4.,   7.,   8.,  16.,   5.,   5.,  16.,  12.,  10.,  22.,\n",
      "        13.,   6.,   8.,  40.,  34.,  46.,  90., 116., 112.,  80.,  41.,\n",
      "        63.,  75.,  65., 129., 110.,  88.,  43.,  65.,  83.,  41., 178.,\n",
      "       171.,  79.,  32.,  46.,  81.,  83., 127., 112., 103.,  21.,  58.,\n",
      "        96.,  61., 144., 146.,  79.,  26.,  50.,  68., 107., 100., 103.,\n",
      "        49.,  59.,  55., 148.,  70.,  98., 101.,  83.,  20.,  43.,  64.,\n",
      "        71.,  98., 104., 105.,  24.,  69.,  88., 102., 137., 103.,  84.,\n",
      "        39.,  36., 103.,  78., 147., 131.,  87.,  46.,  38.,  78.,  83.,\n",
      "       136., 112.,  67.,  29.,  49.,  68.,  83., 128., 107.,  69.,  89.,\n",
      "        64., 126.,  62.,  85.,  69.,  85.,  70.,  41.,  55.,  58., 138.,\n",
      "       138.,  97.,  46.,  53.,  95.,  64., 152., 137.,  88.,  47.,  35.,\n",
      "        65.,  79.,  95.,  97.,  48.,  73.,  36.,  34.,  62.,  76.,  75.,\n",
      "        51.,  52.,  33.,  60.,  50.,  82.,  76.,  58.,  53.,  30.,  50.,\n",
      "        53.,  72.,  68.,  84.,  84.]), 'n_features': 2, 'n_sequences': 1177}, 'Misc/Services': {'X_train_seq': array([[[ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        ...,\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ]],\n",
      "\n",
      "       [[ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        ...,\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ]],\n",
      "\n",
      "       [[ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        ...,\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        , -0.12650197],\n",
      "        [ 9.        , -0.23197458],\n",
      "        [ 9.        , -0.12650197],\n",
      "        ...,\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.37260472]],\n",
      "\n",
      "       [[ 9.        , -0.23197458],\n",
      "        [ 9.        , -0.12650197],\n",
      "        [ 9.        , -0.19681704],\n",
      "        ...,\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.37260472],\n",
      "        [ 9.        , -0.26713211]],\n",
      "\n",
      "       [[ 9.        , -0.12650197],\n",
      "        [ 9.        , -0.19681704],\n",
      "        [ 9.        , -0.37260472],\n",
      "        ...,\n",
      "        [ 9.        , -0.37260472],\n",
      "        [ 9.        , -0.26713211],\n",
      "        [ 9.        , -0.40776226]]]), 'X_test_seq': array([[[ 9.        , -0.19681704],\n",
      "        [ 9.        , -0.37260472],\n",
      "        [ 9.        , -0.30228965],\n",
      "        ...,\n",
      "        [ 9.        , -0.26713211],\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.40776226]],\n",
      "\n",
      "       [[ 9.        , -0.37260472],\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.26713211],\n",
      "        ...,\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.26713211]],\n",
      "\n",
      "       [[ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.26713211],\n",
      "        [ 9.        , -0.33744719],\n",
      "        ...,\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.26713211],\n",
      "        [ 9.        , -0.19681704]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 5.        ,  7.29173816],\n",
      "        [ 5.        ,  5.56901888],\n",
      "        [ 5.        , 10.28012873],\n",
      "        ...,\n",
      "        [ 5.        ,  5.99090932],\n",
      "        [ 5.        ,  6.06122439],\n",
      "        [ 5.        ,  7.74878612]],\n",
      "\n",
      "       [[ 5.        ,  5.56901888],\n",
      "        [ 5.        , 10.28012873],\n",
      "        [ 5.        ,  7.74878612],\n",
      "        ...,\n",
      "        [ 5.        ,  6.06122439],\n",
      "        [ 5.        ,  7.74878612],\n",
      "        [ 5.        ,  6.09638193]],\n",
      "\n",
      "       [[ 5.        , 10.28012873],\n",
      "        [ 5.        ,  7.74878612],\n",
      "        [ 5.        ,  5.21744352],\n",
      "        ...,\n",
      "        [ 5.        ,  7.74878612],\n",
      "        [ 5.        ,  6.09638193],\n",
      "        [ 5.        ,  4.16271744]]]), 'y_train_seq': array([84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  9.,  9., 11.,  4.,  1.,  4., 10.,\n",
      "       14.,  7.,  8., 10., 10.,  9.,  7.,  5.,  4.,  7.,  7.,  7.,  5.,\n",
      "        7., 12.,  4.,  7., 12., 11., 10., 11., 11.,  1., 14.,  8., 10.,\n",
      "        6., 10.,  5.,  8.,  9., 16.,  5., 11., 10., 17., 12.,  7.,  8.,\n",
      "       10.,  1.,  7.,  7.,  8.,  7., 20., 12., 10.,  5.,  6.,  8.,  8.,\n",
      "        8.,  2., 10.,  5.,  6.,  5.,  9.,  3.,  5.,  7.,  5.,  4.,  5.,\n",
      "        2.,  8., 11.,  5.,  8.,  6.,  8.,  4.,  3., 12.,  4., 10.,  5.,\n",
      "        5.,  5.,  8., 13., 13.,  5.,  5.,  1.,  2.,  1.,  3., 10.,  9.,\n",
      "       11.,  3., 12., 11.,  4.,  3.,  6.,  7.,  5.,  6.,  9., 10.,  1.,\n",
      "        8.,  9.,  2.,  4.,  9.,  3.,  6.,  6.,  2., 12.,  4.,  8.,  3.,\n",
      "        8.,  8.,  5.,  5.,  6.,  8.,  7.,  3.,  3.,  7.,  2.,  4.,  5.,\n",
      "        4., 11.,  3.,  5.,  5.,  6.,  5.,  1.,  2.,  3.,  7.,  3.,  3.,\n",
      "        2.,  6.,  2.,  4.,  5.,  5.,  5.,  6.,  5.,  4.,  7.,  5.,  3.,\n",
      "        4.,  4.,  3.,  9.,  3.,  3.,  5.,  5.,  6.,  2.,  8.,  6.,  6.,\n",
      "        2.,  8.,  5.,  1.,  7.,  2.,  9.,  4.,  8.,  1., 10.,  4.,  6.,\n",
      "        8.,  5.,  4.,  5.,  3.,  1.,  3.,  4.,  1.,  4.,  5.,  3.,  2.,\n",
      "        6.,  2.,  2.,  7.,  3.,  6.,  6.,  4.,  8.,  1.,  3.,  8.,  3.,\n",
      "        7.,  3.,  3.,  3.,  5.,  5.,  2.,  4.,  7.,  2.,  4.,  5.,  3.,\n",
      "        2.,  3.,  3.,  1.,  1.,  6.,  4.,  5.,  6.,  6.,  2.,  2.,  5.,\n",
      "        8.,  1.,  5.,  5.,  8., 11.,  5., 23.,  2., 13., 13., 10.,  4.,\n",
      "       13., 15.,  9.,  2.,  7.,  1.,  5.,  5.,  2., 12., 11.,  1.,  6.,\n",
      "        4., 10., 13.,  8.,  9., 10.,  8., 12., 12., 11.,  8.,  3.,  4.,\n",
      "        3., 11.,  5.,  6.,  5.,  5.,  3.,  8.,  4., 12., 10.,  6.,  5.,\n",
      "       13.,  2., 12.,  6.,  3.,  3.,  6.,  5.,  1.,  2., 12.,  8., 15.,\n",
      "        6.,  2.,  8.,  2., 15.,  7., 10.,  3., 13.,  4., 22.,  8.,  6.,\n",
      "       13.,  6.,  2.,  6., 11.,  6.,  8.,  7.,  8.,  6., 11.,  7.,  7.,\n",
      "        4.,  7., 10.,  8.,  7.,  8., 11.,  4.,  6.,  4., 12.,  9., 19.,\n",
      "       14.,  9., 18.,  5., 12., 13., 10., 13., 11.,  6.,  8.,  9.,  7.,\n",
      "        7.,  6.,  9.,  5.,  5.]), 'y_test_seq': array([  9.,  11.,  15.,  17.,   2.,   2.,  10.,   5.,   8.,  14.,   8.,\n",
      "        13.,   7.,  12.,  12.,  13.,  10.,  10.,   5.,   9.,  12.,  11.,\n",
      "        12.,   7.,   6.,   8.,   4.,   9.,   9.,  11.,  13.,   7.,   1.,\n",
      "         4.,   6.,   4.,  14.,   7.,   6.,   8.,   8.,   7.,  14.,   4.,\n",
      "        19.,  10.,  12.,  11.,  12.,  16.,  11.,  12.,   9.,   9.,  18.,\n",
      "        14.,  19.,   4.,  10.,  10.,  13.,  18.,  20.,  19.,  16.,   9.,\n",
      "         7.,  19.,  18.,  17.,  30.,  28.,  16.,  11.,   8.,  19.,  18.,\n",
      "        21.,  20.,   9.,  18.,  15.,  33.,  27.,  39.,  58.,  29.,  36.,\n",
      "        16.,  17.,  27.,  21.,  31.,   7.,   3.,  13.,   3.,  20.,  16.,\n",
      "        38.,  22.,  29.,  48.,  70., 101.,  96., 129., 167.,  84.,  73.,\n",
      "       120., 132., 110., 230., 182., 166.,  69., 141.,  98., 112., 215.,\n",
      "       183., 167.,  71., 129., 114., 164., 223., 200., 158.,  87., 134.,\n",
      "       151., 134., 282., 184., 180.,  83., 153., 147., 178., 190., 196.,\n",
      "       127.,  97., 128., 176., 149., 231., 197., 124.,  92., 157., 139.,\n",
      "       133., 233., 219., 136.,  83., 153., 155., 197., 316., 263., 187.,\n",
      "        99., 149., 169., 186., 246., 230., 121., 132., 160., 150., 160.,\n",
      "       272., 227., 232., 116., 177., 148., 165., 264., 257., 204., 115.,\n",
      "       175., 222., 175., 178., 220., 134., 114., 151., 155., 164., 236.,\n",
      "       315., 186., 141., 160., 160., 174., 277., 248., 161., 149., 161.,\n",
      "       127., 189., 281., 256., 157., 119., 147., 146., 153., 212., 243.,\n",
      "       158., 137., 203., 224., 175., 309., 237., 165., 151., 182., 187.,\n",
      "       189., 237., 190., 135.,  13.]), 'n_features': 2, 'n_sequences': 1177}}\n",
      "\n",
      "ðŸ“Š SEQUENCE SUMMARY BY ITEM:\n",
      "   Beverages:\n",
      "      â”œâ”€ Train sequences: 941\n",
      "      â”œâ”€ Test sequences: 236\n",
      "      â”œâ”€ Sequence shape: (10, 2)\n",
      "      â””â”€ Total sequences: 1177\n",
      "   Other/Uncategorized:\n",
      "      â”œâ”€ Train sequences: 941\n",
      "      â”œâ”€ Test sequences: 236\n",
      "      â”œâ”€ Sequence shape: (10, 2)\n",
      "      â””â”€ Total sequences: 1177\n",
      "   Sides & Snacks:\n",
      "      â”œâ”€ Train sequences: 941\n",
      "      â”œâ”€ Test sequences: 236\n",
      "      â”œâ”€ Sequence shape: (10, 2)\n",
      "      â””â”€ Total sequences: 1177\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: PREPARE SEQUENCES FOR LSTM MODELS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: PREPARE SEQUENCES FOR LSTM MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def create_sequences(data, lookback=10):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM training.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array\n",
    "        Input data (features only)\n",
    "    lookback : int\n",
    "        Number of previous timesteps to use as input\n",
    "    Returns:\n",
    "    --------\n",
    "    X : array of shape (n_sequences, lookback, n_features)\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    for i in range(len(data) - lookback):\n",
    "        X.append(data[i:i + lookback])\n",
    "    return np.array(X)\n",
    "\n",
    "# Dictionary to store LSTM sequence data\n",
    "lstm_sequence_dict = {}\n",
    "lookback_window = 10  # Use 10 days of history to predict next day\n",
    "\n",
    "print(f\"\\nðŸ”„ Creating sequences (lookback window: {lookback_window} days)...\")\n",
    "\n",
    "for category in categories:\n",
    "    # Get data for this item\n",
    "    item_df = item_sales_filled[item_sales_filled['category_name'] == category].copy()\n",
    "    \n",
    "    # Create feature matrix (use the same 5 features as XGBoost after scaling)\n",
    "    features_for_lstm = ['month']\n",
    "    X_lstm = item_df[features_for_lstm].copy().values\n",
    "    \n",
    "    # Add the scaled quantity as 5th feature\n",
    "    qty_scaled_list = []\n",
    "    scaler_temp = scaler_price_dict[item]\n",
    "    for qty in item_df['quantity_sold'].values:\n",
    "        qty_scaled = float(scaler_temp.transform([[qty]])[0][0])\n",
    "        qty_scaled_list.append(qty_scaled)\n",
    "    \n",
    "    X_lstm = np.column_stack([X_lstm, qty_scaled_list])\n",
    "    y = item_df['quantity_sold'].values\n",
    "    \n",
    "    # Create sequences\n",
    "    X_seq = create_sequences(X_lstm, lookback=lookback_window)\n",
    "    y_seq = y[lookback_window:]  # Skip first lookback_window values\n",
    "    \n",
    "    # Train/test split (80/20)\n",
    "    split_idx = int(len(X_seq) * 0.8)\n",
    "    X_train_seq = X_seq[:split_idx]\n",
    "    X_test_seq = X_seq[split_idx:]\n",
    "    y_train_seq = y_seq[:split_idx]\n",
    "    y_test_seq = y_seq[split_idx:]\n",
    "    \n",
    "    # Store sequence data\n",
    "    lstm_sequence_dict[category] = {\n",
    "        'X_train_seq': X_train_seq,\n",
    "        'X_test_seq': X_test_seq,\n",
    "        'y_train_seq': y_train_seq,\n",
    "        'y_test_seq': y_test_seq,\n",
    "        'n_features': X_lstm.shape[1],\n",
    "        'n_sequences': len(X_seq)\n",
    "    }\n",
    "\n",
    "print(lstm_sequence_dict)\n",
    "\n",
    "print(f\"\\nðŸ“Š SEQUENCE SUMMARY BY ITEM:\")\n",
    "for category in categories[:3]:\n",
    "    data = lstm_sequence_dict[category]\n",
    "    print(f\"   {category}:\")\n",
    "    print(f\"      â”œâ”€ Train sequences: {len(data['X_train_seq'])}\")\n",
    "    print(f\"      â”œâ”€ Test sequences: {len(data['X_test_seq'])}\")\n",
    "    print(f\"      â”œâ”€ Sequence shape: ({lookback_window}, {data['n_features']})\")\n",
    "    print(f\"      â””â”€ Total sequences: {data['n_sequences']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bbbdb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 6: BUILD AND TRAIN PER-ITEM LSTM MODELS (ENHANCED)\n",
      "======================================================================\n",
      "\n",
      "ðŸ”„ Building and training 10 LSTM models...\n",
      "   Model Architecture:\n",
      "   â”œâ”€ LSTM 256 + BatchNorm + Dropout(0.15)\n",
      "   â”œâ”€ LSTM 128 + BatchNorm + Dropout(0.15)\n",
      "   â”œâ”€ LSTM 64 + BatchNorm + Dropout(0.2)\n",
      "   â”œâ”€ Dense 64 + Dropout(0.2)\n",
      "   â”œâ”€ Dense 32\n",
      "   â””â”€ Output: Dense 1\n",
      "   Regularization: L1=1e-5, L2=1e-4\n",
      "   Optimizer: Adam(lr=0.0002, clipnorm=1.0)\n",
      "\n",
      "   Training Beverages... âœ“\n",
      "\n",
      "   Training Other/Uncategorized... âœ“\n",
      "\n",
      "   Training Sides & Snacks... âœ“\n",
      "\n",
      "   Training Desserts & Sweets... âœ“\n",
      "\n",
      "   Training Handhelds... âœ“\n",
      "\n",
      "   Training Main Courses... âœ“\n",
      "\n",
      "   Training Salads & Greens... âœ“\n",
      "\n",
      "   Training Breakfast & Brunch... âœ“\n",
      "\n",
      "   Training Sushi & Asian... âœ“\n",
      "\n",
      "   Training Misc/Services... âœ“\n",
      "\n",
      "âœ“ All LSTM models trained successfully!\n",
      "\n",
      "======================================================================\n",
      "PER-ITEM LSTM MODEL PERFORMANCE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Beverages:\n",
      "   â”œâ”€ Train RÂ²: 0.8379\n",
      "   â”œâ”€ Test RÂ²: 0.0325\n",
      "   â”œâ”€ RMSE: 2989.43 units\n",
      "   â”œâ”€ MAE: 1973.67 units\n",
      "   â””â”€ MAPE: 52.15%\n",
      "\n",
      "Breakfast & Brunch:\n",
      "   â”œâ”€ Train RÂ²: 0.7472\n",
      "   â”œâ”€ Test RÂ²: 0.0480\n",
      "   â”œâ”€ RMSE: 83.86 units\n",
      "   â”œâ”€ MAE: 44.27 units\n",
      "   â””â”€ MAPE: 64.21%\n",
      "\n",
      "Desserts & Sweets:\n",
      "   â”œâ”€ Train RÂ²: 0.6425\n",
      "   â”œâ”€ Test RÂ²: -3.3332\n",
      "   â”œâ”€ RMSE: 866.90 units\n",
      "   â”œâ”€ MAE: 507.91 units\n",
      "   â””â”€ MAPE: 103.18%\n",
      "\n",
      "Handhelds:\n",
      "   â”œâ”€ Train RÂ²: 0.4249\n",
      "   â”œâ”€ Test RÂ²: 0.4107\n",
      "   â”œâ”€ RMSE: 450.70 units\n",
      "   â”œâ”€ MAE: 301.00 units\n",
      "   â””â”€ MAPE: 57.65%\n",
      "\n",
      "Main Courses:\n",
      "   â”œâ”€ Train RÂ²: 0.4784\n",
      "   â”œâ”€ Test RÂ²: -0.5280\n",
      "   â”œâ”€ RMSE: 439.52 units\n",
      "   â”œâ”€ MAE: 314.97 units\n",
      "   â””â”€ MAPE: 68.03%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.9807\n",
      "   â”œâ”€ Test RÂ²: 0.1446\n",
      "   â”œâ”€ RMSE: 80.73 units\n",
      "   â”œâ”€ MAE: 56.83 units\n",
      "   â””â”€ MAPE: 55.52%\n",
      "\n",
      "Other/Uncategorized:\n",
      "   â”œâ”€ Train RÂ²: 0.7927\n",
      "   â”œâ”€ Test RÂ²: 0.2750\n",
      "   â”œâ”€ RMSE: 4231.42 units\n",
      "   â”œâ”€ MAE: 2784.50 units\n",
      "   â””â”€ MAPE: 50.85%\n",
      "\n",
      "Salads & Greens:\n",
      "   â”œâ”€ Train RÂ²: 0.1380\n",
      "   â”œâ”€ Test RÂ²: -0.3323\n",
      "   â”œâ”€ RMSE: 81.77 units\n",
      "   â”œâ”€ MAE: 58.98 units\n",
      "   â””â”€ MAPE: 67.10%\n",
      "\n",
      "Sides & Snacks:\n",
      "   â”œâ”€ Train RÂ²: 0.3414\n",
      "   â”œâ”€ Test RÂ²: -0.8387\n",
      "   â”œâ”€ RMSE: 265.68 units\n",
      "   â”œâ”€ MAE: 189.77 units\n",
      "   â””â”€ MAPE: 76.67%\n",
      "\n",
      "Sushi & Asian:\n",
      "   â”œâ”€ Train RÂ²: 0.9635\n",
      "   â”œâ”€ Test RÂ²: 0.4002\n",
      "   â”œâ”€ RMSE: 33.51 units\n",
      "   â”œâ”€ MAE: 21.52 units\n",
      "   â””â”€ MAPE: 48.06%\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: BUILD AND TRAIN PER-ITEM LSTM MODELS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 6: BUILD AND TRAIN PER-ITEM LSTM MODELS (ENHANCED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dictionary to store trained LSTM models and metrics\n",
    "lstm_models_dict = {}\n",
    "lstm_metrics = {}\n",
    "\n",
    "print(f\"\\nðŸ”„ Building and training {len(lstm_sequence_dict)} LSTM models...\")\n",
    "print(f\"   Model Architecture:\")\n",
    "print(f\"   â”œâ”€ LSTM 256 + BatchNorm + Dropout(0.15)\")\n",
    "print(f\"   â”œâ”€ LSTM 128 + BatchNorm + Dropout(0.15)\")\n",
    "print(f\"   â”œâ”€ LSTM 64 + BatchNorm + Dropout(0.2)\")\n",
    "print(f\"   â”œâ”€ Dense 64 + Dropout(0.2)\")\n",
    "print(f\"   â”œâ”€ Dense 32\")\n",
    "print(f\"   â””â”€ Output: Dense 1\")\n",
    "print(f\"   Regularization: L1=1e-5, L2=1e-4\")\n",
    "print(f\"   Optimizer: Adam(lr=0.0002, clipnorm=1.0)\")\n",
    "\n",
    "for category in categories:\n",
    "    data = lstm_sequence_dict[category]\n",
    "    \n",
    "    # Build LSTM model with exact user-provided architecture\n",
    "    model = Sequential([\n",
    "        LSTM(256, activation='relu', return_sequences=True,\n",
    "             kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "             recurrent_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "             input_shape=(lookback_window, data['n_features'])),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.15),\n",
    "        \n",
    "        LSTM(128, activation='relu', return_sequences=True,\n",
    "             kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "             recurrent_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.15),\n",
    "        \n",
    "        LSTM(64, activation='relu', return_sequences=False,\n",
    "             kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "             recurrent_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # Compile with optimized settings\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0002, clipnorm=1.0),\n",
    "        loss='mse',  # MSE for better precision on predictions\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n   Training {category}...\", end='')\n",
    "    \n",
    "    # Enhanced callbacks\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,            # Increased patience to allow more learning\n",
    "        restore_best_weights=True,\n",
    "        min_delta=0.0001        # Minimum improvement threshold\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,             # Reduce LR by 50% when plateau detected\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        data['X_train_seq'], data['y_train_seq'],\n",
    "        validation_split=0.2,\n",
    "        epochs=150,             # More epochs with early stopping\n",
    "        batch_size=16,          # Smaller batch size for better gradient updates\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "    print(\" âœ“\")\n",
    "    \n",
    "    # Store model\n",
    "    lstm_models_dict[category] = model\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(data['X_train_seq'], verbose=0).flatten()\n",
    "    y_pred_test = model.predict(data['X_test_seq'], verbose=0).flatten()\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    train_r2 = r2_score(data['y_train_seq'], y_pred_train)\n",
    "    test_r2 = r2_score(data['y_test_seq'], y_pred_test)\n",
    "    rmse = np.sqrt(mean_squared_error(data['y_test_seq'], y_pred_test))\n",
    "    mae = mean_absolute_error(data['y_test_seq'], y_pred_test)\n",
    "    mape = np.mean(np.abs((data['y_test_seq'] - y_pred_test) / (data['y_test_seq'] + 1))) * 100\n",
    "    \n",
    "    lstm_metrics[category] = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mape': mape,\n",
    "        'history': history\n",
    "    }\n",
    "\n",
    "print(\"\\nâœ“ All LSTM models trained successfully!\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"PER-ITEM LSTM MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for item in sorted(lstm_metrics.keys()):\n",
    "    metrics = lstm_metrics[item]\n",
    "    print(f\"\\n{item}:\")\n",
    "    print(f\"   â”œâ”€ Train RÂ²: {metrics['train_r2']:.4f}\")\n",
    "    print(f\"   â”œâ”€ Test RÂ²: {metrics['test_r2']:.4f}\")\n",
    "    print(f\"   â”œâ”€ RMSE: {metrics['rmse']:.2f} units\")\n",
    "    print(f\"   â”œâ”€ MAE: {metrics['mae']:.2f} units\")\n",
    "    print(f\"   â””â”€ MAPE: {metrics['mape']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc03808c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 7: LSTM WALKFORWARD PREDICTION FUNCTION\n",
      "======================================================================\n",
      "âœ“ LSTM walkforward prediction function created!\n",
      "Available items: Beverages, Other/Uncategorized, Sides & Snacks, Desserts & Sweets, Handhelds, Main Courses, Salads & Greens, Breakfast & Brunch, Sushi & Asian, Misc/Services\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: LSTM WALKFORWARD PREDICTION FUNCTION\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 7: LSTM WALKFORWARD PREDICTION FUNCTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def predict_next_day_lstm(current_date, category_name, verbose=True):\n",
    "    \"\"\"\n",
    "    Predict sales volume for the next day using LSTM model for a specific item.\n",
    "    Uses the trained per-item LSTM model and historical data.\n",
    "    \"\"\"\n",
    "    # Validate item\n",
    "    if category_name not in lstm_models_dict:\n",
    "        return {'error': f'Item {category_name} not in trained LSTM models. Available: {list(lstm_models_dict.keys())}'}\n",
    "    \n",
    "    # Convert to datetime if string\n",
    "    current_dt = pd.to_datetime(current_date)\n",
    "    next_dt = current_dt + pd.Timedelta(days=1)\n",
    "    \n",
    "    # Extract temporal features for next day\n",
    "    day_of_week = next_dt.dayofweek\n",
    "    is_weekend = 1 if next_dt.dayofweek in [5, 6] else 0\n",
    "    is_holiday = 1 if next_dt.date() in [d.date() for d in holidays_dates] else 0\n",
    "    month = next_dt.month\n",
    "    \n",
    "    # Get item data and last quantity sold\n",
    "    item_data = item_sales_filled[item_sales_filled['category_name'] == category_name]\n",
    "    if item_data.empty:\n",
    "        return {'error': f'No historical data for item {category_name}'}\n",
    "    \n",
    "    # Get recent quantity (last non-null or average of last 7 days)\n",
    "    recent_qty_series = item_data['quantity_sold'].dropna()\n",
    "    if len(recent_qty_series) == 0:\n",
    "        last_qty = 0.0\n",
    "    else:\n",
    "        last_qty = float(recent_qty_series.iloc[-1]) if not np.isnan(recent_qty_series.iloc[-1]) else float(recent_qty_series.tail(7).mean())\n",
    "    \n",
    "    # Scale the quantity using item-specific scaler\n",
    "    scaler = scaler_price_dict.get(category_name)\n",
    "    if scaler is None:\n",
    "        qty_scaled = last_qty\n",
    "    else:\n",
    "        qty_scaled = float(scaler.transform([[last_qty]])[0][0])\n",
    "    \n",
    "    # Get last lookback_window days for LSTM sequence\n",
    "    features_for_lstm = ['month']\n",
    "    historical_data = item_data.tail(lookback_window + 1)[features_for_lstm].values\n",
    "    \n",
    "    # Add scaled quantities to features\n",
    "    hist_qty_values = item_data.tail(lookback_window + 1)['quantity_sold'].values\n",
    "    hist_qty_scaled = [float(scaler.transform([[q]])[0][0]) for q in hist_qty_values]\n",
    "    \n",
    "    sequence_data = np.column_stack([historical_data, hist_qty_scaled])\n",
    "    \n",
    "    # Ensure we have exactly lookback_window timesteps\n",
    "    if len(sequence_data) < lookback_window:\n",
    "        # Pad with first row if not enough data\n",
    "        padding = np.tile(sequence_data[0], (lookback_window - len(sequence_data), 1))\n",
    "        sequence_data = np.vstack([padding, sequence_data])\n",
    "    else:\n",
    "        sequence_data = sequence_data[-lookback_window:]\n",
    "    \n",
    "    # Reshape for LSTM: (1, lookback_window, n_features)\n",
    "    sequence_data = sequence_data.reshape(1, lookback_window, sequence_data.shape[1])\n",
    "    \n",
    "    # Get LSTM model and metrics\n",
    "    model = lstm_models_dict[category_name]\n",
    "    metrics = lstm_metrics[category_name]\n",
    "    mae = metrics['mae']\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = float(model.predict(sequence_data, verbose=0)[0][0])\n",
    "    prediction = max(0, prediction)  # Ensure non-negative\n",
    "    \n",
    "    # Confidence interval\n",
    "    lower_bound = max(0, prediction - mae)\n",
    "    upper_bound = prediction + mae\n",
    "    safety_stock = prediction + mae\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nðŸ“… LSTM WALKFORWARD PREDICTION\")\n",
    "        print(f\"   Current Date: {current_dt.date()}\")\n",
    "        print(f\"   Predict For: {next_dt.date()}\")\n",
    "        print(f\"   Item: {category_name}\")\n",
    "        print(f\"\\nðŸ“Š FEATURES (Next Day):\")\n",
    "        print(f\"   â”œâ”€ Day of Week: {next_dt.day_name()}\")\n",
    "        print(f\"   â”œâ”€ Is Weekend: {'Yes' if is_weekend else 'No'}\")\n",
    "        print(f\"   â”œâ”€ Is Holiday: {'Yes' if is_holiday else 'No'}\")\n",
    "        print(f\"   â””â”€ Month: {next_dt.strftime('%B')}\")\n",
    "        print(f\"\\nðŸŽ¯ LSTM PREDICTION:\")\n",
    "        print(f\"   â”œâ”€ Expected Sales: {prediction:.0f} units\")\n",
    "        print(f\"   â”œâ”€ Confidence Interval: [{lower_bound:.0f}, {upper_bound:.0f}]\")\n",
    "        print(f\"   â”œâ”€ Safety Stock: {safety_stock:.0f} units\")\n",
    "        print(f\"   â”œâ”€ Error Margin (MAE): Â±{mae:.0f} units\")\n",
    "        print(f\"   â””â”€ Model RÂ² Score: {metrics['test_r2']:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'date': next_dt.date(),\n",
    "        'category_name': category_name,\n",
    "        'predicted_quantity': prediction,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound,\n",
    "        'safety_stock': safety_stock,\n",
    "        'confidence_margin': mae,\n",
    "        'model_r2': metrics['test_r2'],\n",
    "        'model_type': 'LSTM',\n",
    "        'features': {\n",
    "            'day_of_week': day_of_week,\n",
    "            'is_weekend': is_weekend,\n",
    "            'is_holiday': is_holiday,\n",
    "            'month': month,\n",
    "            'quantity_used': last_qty\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"âœ“ LSTM walkforward prediction function created!\")\n",
    "print(f\"Available items: {', '.join(list(lstm_models_dict.keys()))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccf3fada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 8: LSTM WALKFORWARD PREDICTIONS FOR TOP 5 ITEMS\n",
      "======================================================================\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Beverages\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 91 units\n",
      "   â”œâ”€ Confidence Interval: [0, 2065]\n",
      "   â”œâ”€ Safety Stock: 2065 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±1974 units\n",
      "   â””â”€ Model RÂ² Score: 0.0325\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Other/Uncategorized\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 80 units\n",
      "   â”œâ”€ Confidence Interval: [0, 2864]\n",
      "   â”œâ”€ Safety Stock: 2864 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±2784 units\n",
      "   â””â”€ Model RÂ² Score: 0.2750\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Sides & Snacks\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 24 units\n",
      "   â”œâ”€ Confidence Interval: [0, 214]\n",
      "   â”œâ”€ Safety Stock: 214 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±190 units\n",
      "   â””â”€ Model RÂ² Score: -0.8387\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Desserts & Sweets\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 63 units\n",
      "   â”œâ”€ Confidence Interval: [0, 570]\n",
      "   â”œâ”€ Safety Stock: 570 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±508 units\n",
      "   â””â”€ Model RÂ² Score: -3.3332\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Handhelds\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 57 units\n",
      "   â”œâ”€ Confidence Interval: [0, 358]\n",
      "   â”œâ”€ Safety Stock: 358 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±301 units\n",
      "   â””â”€ Model RÂ² Score: 0.4107\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Main Courses\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 30 units\n",
      "   â”œâ”€ Confidence Interval: [0, 345]\n",
      "   â”œâ”€ Safety Stock: 345 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±315 units\n",
      "   â””â”€ Model RÂ² Score: -0.5280\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Salads & Greens\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 23 units\n",
      "   â”œâ”€ Confidence Interval: [0, 82]\n",
      "   â”œâ”€ Safety Stock: 82 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±59 units\n",
      "   â””â”€ Model RÂ² Score: -0.3323\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Breakfast & Brunch\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 59 units\n",
      "   â”œâ”€ Confidence Interval: [15, 103]\n",
      "   â”œâ”€ Safety Stock: 103 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±44 units\n",
      "   â””â”€ Model RÂ² Score: 0.0480\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Sushi & Asian\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 67 units\n",
      "   â”œâ”€ Confidence Interval: [45, 89]\n",
      "   â”œâ”€ Safety Stock: 89 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±22 units\n",
      "   â””â”€ Model RÂ² Score: 0.4002\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Misc/Services\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 28 units\n",
      "   â”œâ”€ Confidence Interval: [0, 84]\n",
      "   â”œâ”€ Safety Stock: 84 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±57 units\n",
      "   â””â”€ Model RÂ² Score: 0.1446\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Predicted Date</th>\n",
       "      <th>LSTM Prediction</th>\n",
       "      <th>Lower Bound</th>\n",
       "      <th>Upper Bound</th>\n",
       "      <th>Safety Stock</th>\n",
       "      <th>Confidence (Â±)</th>\n",
       "      <th>Model RÂ²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beverages</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2065.0</td>\n",
       "      <td>2065.0</td>\n",
       "      <td>1974.0</td>\n",
       "      <td>0.0325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Other/Uncategorized</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2864.0</td>\n",
       "      <td>2864.0</td>\n",
       "      <td>2784.0</td>\n",
       "      <td>0.2750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sides &amp; Snacks</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>-0.8387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Desserts &amp; Sweets</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>508.0</td>\n",
       "      <td>-3.3332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Handhelds</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>0.4107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Main Courses</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>-0.5280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Salads &amp; Greens</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>-0.3323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Breakfast &amp; Brunch</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>59.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sushi &amp; Asian</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>67.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.4002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Misc/Services</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.1446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Category Predicted Date  LSTM Prediction  Lower Bound  \\\n",
       "0            Beverages     2024-05-14             91.0          0.0   \n",
       "1  Other/Uncategorized     2024-05-14             80.0          0.0   \n",
       "2       Sides & Snacks     2024-05-14             24.0          0.0   \n",
       "3    Desserts & Sweets     2024-05-14             63.0          0.0   \n",
       "4            Handhelds     2024-05-14             57.0          0.0   \n",
       "5         Main Courses     2024-05-14             30.0          0.0   \n",
       "6      Salads & Greens     2024-05-14             23.0          0.0   \n",
       "7   Breakfast & Brunch     2024-05-14             59.0         15.0   \n",
       "8        Sushi & Asian     2024-05-14             67.0         45.0   \n",
       "9        Misc/Services     2024-05-14             28.0          0.0   \n",
       "\n",
       "   Upper Bound  Safety Stock  Confidence (Â±)  Model RÂ²  \n",
       "0       2065.0        2065.0          1974.0    0.0325  \n",
       "1       2864.0        2864.0          2784.0    0.2750  \n",
       "2        214.0         214.0           190.0   -0.8387  \n",
       "3        570.0         570.0           508.0   -3.3332  \n",
       "4        358.0         358.0           301.0    0.4107  \n",
       "5        345.0         345.0           315.0   -0.5280  \n",
       "6         82.0          82.0            59.0   -0.3323  \n",
       "7        103.0         103.0            44.0    0.0480  \n",
       "8         89.0          89.0            22.0    0.4002  \n",
       "9         84.0          84.0            57.0    0.1446  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 8: LSTM DEMONSTRATION - WALKFORWARD PREDICTIONS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 8: LSTM WALKFORWARD PREDICTIONS FOR TOP 5 ITEMS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get the latest date in data\n",
    "latest_date = item_sales_filled['date'].max()\n",
    "\n",
    "# Make LSTM predictions for top 5 items\n",
    "lstm_results_list = []\n",
    "for category in categories:\n",
    "    result = predict_next_day_lstm(latest_date, category, verbose=True)\n",
    "    lstm_results_list.append(result)\n",
    "# Create LSTM results dataframe\n",
    "lstm_results_df =  pd.DataFrame([\n",
    "    {\n",
    "        'Category': r['category_name'],\n",
    "        'Predicted Date': r['date'],\n",
    "        'LSTM Prediction': round(r['predicted_quantity'], 0),\n",
    "        'Lower Bound': round(r['lower_bound'], 0),\n",
    "        'Upper Bound': round(r['upper_bound'], 0),\n",
    "        'Safety Stock': round(r['safety_stock'], 0),\n",
    "        'Confidence (Â±)': round(r['confidence_margin'], 0),\n",
    "        'Model RÂ²': round(r['model_r2'], 4)\n",
    "    }\n",
    "    for r in lstm_results_list\n",
    "])\n",
    "    \n",
    "\n",
    "lstm_results_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f619b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 10: EXPORT CONSOLIDATED DICTIONARIES\n",
      "======================================================================\n",
      "\n",
      "ðŸ”„ Exporting all XGBoost models into one file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Exported 10 models to models/consolidated/xgb_models_dict.joblib\n",
      "\n",
      "ðŸ”„ Exporting all scalers into one file...\n",
      "âœ“ Exported 10 scalers to models/consolidated/scaler_price_dict.joblib\n",
      "\n",
      "ðŸ”„ Exporting LSTM models (Individual .h5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Exported 10 LSTM models\n"
     ]
    }
   ],
   "source": [
    "# STEP 10: EXPORT MODELS, PREDICTIONS, AND METRICS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 10: EXPORT XGBOOST AND LSTM MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import os\n",
    "\n",
    "os.makedirs('models/xgb_models', exist_ok=True)\n",
    "os.makedirs('models/lstm_models', exist_ok=True)\n",
    "os.makedirs('models/scalers', exist_ok=True)\n",
    "os.makedirs('output', exist_ok=True)\n",
    "os.makedirs('output/plots', exist_ok=True)\n",
    "\n",
    "# Export XGBoost models\n",
    "print(\"\\nðŸ”„ Exporting XGBoost models...\")\n",
    "for item, model in xgb_models_dict.items():\n",
    "    safe_name = item.replace(' ', '_').replace('/', '_')\n",
    "    model_path = os.path.join('models', 'xgb_models', f\"{safe_name}.joblib\")\n",
    "    joblib.dump(model, model_path)\n",
    "print(f\"âœ“ Exported {len(xgb_models_dict)} XGBoost models\")\n",
    "\n",
    "# Export LSTM models\n",
    "print(\"\\nðŸ”„ Exporting LSTM models...\")\n",
    "for item, model in lstm_models_dict.items():\n",
    "    safe_name = item.replace(' ', '_').replace('/', '_')\n",
    "    model_path = os.path.join('models', 'lstm_models', f\"{safe_name}_lstm.h5\")\n",
    "    model.save(model_path)\n",
    "print(f\"âœ“ Exported {len(lstm_models_dict)} LSTM models\")\n",
    "\n",
    "# Export scalers\n",
    "print(\"\\nðŸ”„ Exporting scalers...\")\n",
    "for item, scaler in scaler_price_dict.items():\n",
    "    safe_name = item.replace(' ', '_').replace('/', '_')\n",
    "    scaler_path = os.path.join('models', 'scalers', f\"{safe_name}_scaler.joblib\")\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "print(f\"âœ“ Exported {len(scaler_price_dict)} scalers\")\n",
    "\n",
    "# Export predictions from both models\n",
    "print(\"\\nðŸ”„ Exporting predictions...\")\n",
    "xgb_preds_list = []\n",
    "lstm_preds_list = []\n",
    "\n",
    "for category in categories:\n",
    "    # XGBoost predictions\n",
    "    if item in item_data_dict:\n",
    "        data = item_data_dict[item]\n",
    "        y_pred = xgb_models_dict[item].predict(data['X_test'])\n",
    "        xgb_preds_list.append(pd.DataFrame({\n",
    "            'item': item,\n",
    "            'actual': data['y_test'],\n",
    "            'predicted': y_pred,\n",
    "            'model': 'XGBoost'\n",
    "        }))\n",
    "    \n",
    "    # LSTM predictions\n",
    "    if item in lstm_sequence_dict:\n",
    "        data = lstm_sequence_dict[item]\n",
    "        y_pred = lstm_models_dict[item].predict(data['X_test_seq'], verbose=0).flatten()\n",
    "        lstm_preds_list.append(pd.DataFrame({\n",
    "            'item': item,\n",
    "            'actual': data['y_test_seq'],\n",
    "            'predicted': y_pred,\n",
    "            'model': 'LSTM'\n",
    "        }))\n",
    "\n",
    "xgb_preds_df = pd.concat(xgb_preds_list, ignore_index=True) if xgb_preds_list else pd.DataFrame()\n",
    "lstm_preds_df = pd.concat(lstm_preds_list, ignore_index=True) if lstm_preds_list else pd.DataFrame()\n",
    "all_preds_df = pd.concat([xgb_preds_df, lstm_preds_df], ignore_index=True)\n",
    "\n",
    "all_preds_df.to_csv(os.path.join('output', 'predictions.csv'), index=False)\n",
    "print(f\"âœ“ Saved {len(all_preds_df)} predictions to output/predictions.csv\")\n",
    "\n",
    "# Export metrics\n",
    "print(\"\\nðŸ”„ Exporting metrics...\")\n",
    "xgb_metrics_list = []\n",
    "lstm_metrics_list = []\n",
    "\n",
    "for item in model_metrics.keys():\n",
    "    m = model_metrics[item]\n",
    "    xgb_metrics_list.append({\n",
    "        'item': item,\n",
    "        'model': 'XGBoost',\n",
    "        'test_r2': m['test_r2'],\n",
    "        'rmse': m['rmse'],\n",
    "        'mae': m['mae'],\n",
    "        'mape': m['mape']\n",
    "    })\n",
    "\n",
    "for item in lstm_metrics.keys():\n",
    "    m = lstm_metrics[item]\n",
    "    lstm_metrics_list.append({\n",
    "        'item': item,\n",
    "        'model': 'LSTM',\n",
    "        'test_r2': m['test_r2'],\n",
    "        'rmse': m['rmse'],\n",
    "        'mae': m['mae'],\n",
    "        'mape': m['mape']\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(xgb_metrics_list + lstm_metrics_list)\n",
    "metrics_df.to_csv(os.path.join('output', 'metrics.csv'), index=False)\n",
    "print(f\"âœ“ Saved metrics to output/metrics.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPORT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"âœ“ XGBoost models: models/xgb_models/\")\n",
    "print(f\"âœ“ LSTM models: models/lstm_models/\")\n",
    "print(f\"âœ“ Scalers: models/scalers/\")\n",
    "print(f\"âœ“ Predictions: output/predictions.csv\")\n",
    "print(f\"âœ“ Metrics: output/metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5a12bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 11: VISUALIZATIONS\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metrics_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# RÂ² Comparison\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m metrics_pivot \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics_df\u001b[49m\u001b[38;5;241m.\u001b[39mpivot(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem\u001b[39m\u001b[38;5;124m'\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_r2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m metrics_pivot\u001b[38;5;241m.\u001b[39mplot(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m, ax\u001b[38;5;241m=\u001b[39maxes[\u001b[38;5;241m0\u001b[39m], color\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#1f77b4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#ff7f0e\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     12\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest RÂ² Comparison\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, fontweight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbold\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metrics_df' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1296x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 11: VISUALIZATIONS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 11: VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Plot 1: Model Performance Comparison (RÂ², RMSE, MAE)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# RÂ² Comparison\n",
    "metrics_pivot = metrics_df.pivot(index='item', columns='model', values='test_r2')\n",
    "metrics_pivot.plot(kind='bar', ax=axes[0], color=['#1f77b4', '#ff7f0e'])\n",
    "axes[0].set_title('Test RÂ² Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('RÂ² Score')\n",
    "axes[0].legend(title='Model')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# RMSE Comparison\n",
    "metrics_pivot = metrics_df.pivot(index='item', columns='model', values='rmse')\n",
    "metrics_pivot.plot(kind='bar', ax=axes[1], color=['#1f77b4', '#ff7f0e'])\n",
    "axes[1].set_title('RMSE Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('RMSE (units)')\n",
    "axes[1].legend(title='Model')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MAE Comparison\n",
    "metrics_pivot = metrics_df.pivot(index='item', columns='model', values='mae')\n",
    "metrics_pivot.plot(kind='bar', ax=axes[2], color=['#1f77b4', '#ff7f0e'])\n",
    "axes[2].set_title('MAE Comparison (Confidence Interval)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('MAE (units)')\n",
    "axes[2].legend(title='Model')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join('output', 'plots', 'model_comparison.png'), dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Saved model_comparison.png\")\n",
    "\n",
    "# Plot 2: Actual vs Predicted (for each model)\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, item in enumerate(categories):\n",
    "    # XGBoost\n",
    "    xgb_data = all_preds_df[(all_preds_df['item'] == item) & (all_preds_df['model'] == 'XGBoost')]\n",
    "    if not xgb_data.empty:\n",
    "        axes[idx].scatter(xgb_data['actual'], xgb_data['predicted'], alpha=0.6, label='XGBoost', s=50)\n",
    "    \n",
    "    # LSTM\n",
    "    lstm_data = all_preds_df[(all_preds_df['item'] == item) & (all_preds_df['model'] == 'LSTM')]\n",
    "    if not lstm_data.empty:\n",
    "        axes[idx].scatter(lstm_data['actual'], lstm_data['predicted'], alpha=0.6, label='LSTM', s=50)\n",
    "    \n",
    "    # y=x line\n",
    "    data_combined = pd.concat([xgb_data, lstm_data])\n",
    "    if not data_combined.empty:\n",
    "        mins = min(data_combined['actual'].min(), data_combined['predicted'].min())\n",
    "        maxs = max(data_combined['actual'].max(), data_combined['predicted'].max())\n",
    "        axes[idx].plot([mins, maxs], [mins, maxs], 'r--', alpha=0.5, linewidth=2)\n",
    "        axes[idx].set_xlim(mins - 5, maxs + 5)\n",
    "        axes[idx].set_ylim(mins - 5, maxs + 5)\n",
    "    \n",
    "    axes[idx].set_title(item, fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Actual')\n",
    "    axes[idx].set_ylabel('Predicted')\n",
    "    axes[idx].legend(fontsize=8)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join('output', 'plots', 'actual_vs_predicted.png'), dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Saved actual_vs_predicted.png\")\n",
    "\n",
    "# Plot 3: Metrics Heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "metrics_pivot = metrics_df.pivot(index='item', columns='model', values='test_r2')\n",
    "sns.heatmap(metrics_pivot, annot=True, fmt='.3f', cmap='RdYlGn', ax=ax, cbar_kws={'label': 'RÂ² Score'})\n",
    "ax.set_title('Model Performance Heatmap (RÂ² Scores)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join('output', 'plots', 'performance_heatmap.png'), dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Saved performance_heatmap.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUALIZATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"âœ“ model_comparison.png â€” RÂ², RMSE, MAE by item and model\")\n",
    "print(\"âœ“ actual_vs_predicted.png â€” Scatter plots for all items\")\n",
    "print(\"âœ“ performance_heatmap.png â€” RÂ² scores heatmap\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bc4a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
