{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c86ffb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sys\n",
    "#to include data_loader\n",
    "sys.path.append('D:/Deloitte/src/models/')\n",
    "from data_loader import *\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "print('âœ“ All libraries imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a136ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dim_add_ons.csv: 9731 rows\n",
      "Successfully loaded dim_campaigns.csv: 641 rows\n",
      "Successfully loaded dim_items.csv: 87266 rows\n",
      "Successfully loaded dim_menu_item_add_ons.csv: 2459 rows\n",
      "Successfully loaded dim_menu_items.csv: 30407 rows\n",
      "Successfully loaded dim_places.csv: 1056 rows\n",
      "Successfully loaded dim_skus.csv: 4 rows\n",
      "Successfully loaded dim_stock_categories.csv: 3 rows\n",
      "Successfully loaded dim_taxonomy_terms.csv: 904 rows\n",
      "Successfully loaded dim_users.csv: 22762 rows\n",
      "Successfully loaded fct_bonus_codes.csv: 6 rows\n",
      "Successfully loaded fct_campaigns.csv: 641 rows\n",
      "Successfully loaded fct_invoice_items.csv: 2918 rows\n",
      "Successfully loaded fct_order_items.csv: 1974592 rows\n",
      "Successfully loaded fct_orders.csv: 371667 rows\n",
      "Successfully loaded most_ordered.csv: 93048 rows\n"
     ]
    }
   ],
   "source": [
    "#we will load datasets first\n",
    "data_loader = DataLoader(\"D:/Deloitte/data/Old data\")\n",
    "additions_on_food = data_loader.load_csv('dim_add_ons.csv') #it contains the price and status and id and category and if it is deleted or not\n",
    "campaigns_offers = data_loader.load_csv('dim_campaigns.csv') #it contains campaigns (offers) and their places and if it is active or not\n",
    "stock_items_info = data_loader.load_csv('dim_items.csv')  #it contains items and if there is delivery or takeaway and if it is there and \n",
    "#types of additions and if it is displayed for customers\n",
    "menu_additions = data_loader.load_csv('dim_menu_item_add_ons.csv') #specific add_ons to menu and the price and all are active\n",
    "menu_items_info = data_loader.load_csv('dim_menu_items.csv') #it contains price and rating and votes and if it is acitve\n",
    "places_info = data_loader.load_csv(\"dim_places.csv\") #it contains  places and if it is bankrupt and visit_duration and vip threshold\n",
    "#and waiting_time and binding_period and their title\n",
    "stock_internal_measuring_units = data_loader.load_csv(\"dim_skus.csv\") #units for measuring stock\n",
    "stock_categories = data_loader.load_csv(\"dim_stock_categories.csv\") #stock categories ids and titles\n",
    "taxonomy_terms = data_loader.load_csv(\"dim_taxonomy_terms.csv\") #contains ids that translates into vocab (male,female,bar,cafe,lounge)\n",
    "users_info = data_loader.load_csv(\"dim_users.csv\") #if account closed,name,country,currency,points,savings,and the role\n",
    "bonus_codes = data_loader.load_csv(\"fct_bonus_codes.csv\") #bonus ids, points, and duration\n",
    "campaigns_info = data_loader.load_csv(\"fct_campaigns.csv\") #campaigns titles and items, delivery, what original campaing,the place\n",
    "#and redemptions and redemptions per_customer and if it is active or not.\n",
    "#cash_balances = data_loader.load_csv(\"fct_cash_balances.csv\")\n",
    "#cash_balances.head() #it shows transactions and opening and closing balances\n",
    "invoice_items = data_loader.load_csv(\"fct_invoice_items.csv\") #it shows specific items with the amount (tablets)\n",
    "order_items_info = data_loader.load_csv(\"fct_order_items.csv\") #order_tiems and cost, commision, discount, points earned, redeemed,\n",
    "#quanity, price and status and id of orders and title\n",
    "orders_info  = data_loader.load_csv(\"fct_orders.csv\") #orders cash amount, cashier, channel, points earned and redeemed, promise_time,\n",
    "#source and status and total amount and type (takeaway or eat in)\n",
    "most_ordered = data_loader.load_csv(\"most_ordered.csv\") #most ordered items and count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f124ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add encoding=\"utf-8\" here\n",
    "with open(\"list.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(order_items_info['title'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75f4b908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Processed 19089 unique items into 'full_menu_mapping.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def categorize_item(item):\n",
    "    item_lower = item.lower()\n",
    "    \n",
    "    # Logic to assign categories based on keywords\n",
    "    if any(k in item_lower for k in [\"soda\", \"cola\", \"vin\", \"beer\", \"Ã¸l\", \"juice\", \"coffee\", \"kaffe\", \"tea\", \"the\", \"latte\", \"drink\", \"water\", \"vand\", \"lassi\", \"shake\"]):\n",
    "        return \"Beverages\"\n",
    "    elif any(k in item_lower for k in [\"sushi\", \"maki\", \"nigiri\", \"gyoza\", \"tempura\", \"sashimi\", \"edamame\", \"hosomaki\"]):\n",
    "        return \"Sushi & Asian\"\n",
    "    elif any(k in item_lower for k in [\"sandwich\", \"wrap\", \"burger\", \"pita\", \"durum\", \"rulle\"]):\n",
    "        return \"Handhelds\"\n",
    "    elif any(k in item_lower for k in [\"pizza\", \"pasta\", \"steak\", \"chicken\", \"meal\", \"kylling\", \"kebab\", \"curry\", \"bolognese\", \"lasagna\"]):\n",
    "        return \"Main Courses\"\n",
    "    elif any(k in item_lower for k in [\"cake\", \"kage\", \"dessert\", \"mousse\", \"sweet\", \"oreo\", \"cookie\", \"is\", \"gelato\", \"cheesecake\"]):\n",
    "        return \"Desserts & Sweets\"\n",
    "    elif any(k in item_lower for k in [\"brunch\", \"breakfast\", \"morning\", \"morgen\", \"egg\", \"bowl\", \"yogurt\", \"pancake\"]):\n",
    "        return \"Breakfast & Brunch\"\n",
    "    elif any(k in item_lower for k in [\"salat\", \"salad\", \"greens\", \"asparges\"]):\n",
    "        return \"Salads & Greens\"\n",
    "    elif any(k in item_lower for k in [\"fries\", \"fritter\", \"pommes\", \"snacks\", \"dip\", \"oliven\", \"mandler\", \"nuggets\", \"samosa\"]):\n",
    "        return \"Sides & Snacks\"\n",
    "    elif any(k in item_lower for k in [\"klip\", \"powerbank\", \"pose\", \"lighter\", \"levering\", \"personale\", \"deposit\"]):\n",
    "        return \"Misc/Services\"\n",
    "    else:\n",
    "        return \"Other/Uncategorized\"\n",
    "\n",
    "# Read your file and create the mapping\n",
    "item_mapping = {}\n",
    "\n",
    "try:\n",
    "    with open('list.txt', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            item = line.strip()\n",
    "            if item:\n",
    "                # Map the item (Key) to the category (Value)\n",
    "                category = categorize_item(item)\n",
    "                item_mapping[item] = category\n",
    "\n",
    "    # Save to JSON\n",
    "    with open('full_menu_mapping.json', 'w', encoding='utf-8') as json_f:\n",
    "        json.dump(item_mapping, json_f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Success! Processed {len(item_mapping)} unique items into 'full_menu_mapping.json'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: list.txt not found. Please place the script in the same folder as your file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad0985ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: ITEM-LEVEL FEATURE ENGINEERING\n",
      "======================================================================\n",
      "\n",
      "âœ“ FILLING DATE GAPS FOR EACH ITEM...\n",
      "start filling\n",
      "finish filling\n",
      "\n",
      "âœ“ ITEM-LEVEL DATA PREPARED WITH DATE GAPS FILLED\n",
      "   â”œâ”€ Items: 10\n",
      "   â”œâ”€ Total records: 11870 (including 0 zero-sale days)\n",
      "   â”œâ”€ Date range: 2021-02-12 to 2024-05-13\n",
      "   â”œâ”€ Unique days: 1187\n",
      "   â””â”€ Avg daily sales per item: 227.72 units\n",
      "\n",
      "Sample Item Sales Data (with gaps filled):\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: ITEM-LEVEL FEATURE ENGINEERING\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: ITEM-LEVEL FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert timestamp\n",
    "stock_data = order_items_info.copy()\n",
    "stock_data['date'] = pd.to_datetime(stock_data['created'], errors='coerce')\n",
    "stock_data['date_only'] = stock_data['date'].dt.date\n",
    "\n",
    "# Create time-based features\n",
    "stock_data['day_of_week'] = stock_data['date'].dt.dayofweek\n",
    "stock_data['is_weekend'] = stock_data['day_of_week'].isin([5, 6]).astype(int)\n",
    "stock_data['month'] = stock_data['date'].dt.month\n",
    "stock_data = stock_data[stock_data['title'].isin(item_mapping.keys())]\n",
    "stock_data['category'] = stock_data['title'].map(item_mapping)\n",
    "# Holiday features\n",
    "holidays_2024 = [\n",
    "    '2023-01-01', '2023-02-21', '2023-03-30', '2023-04-10', '2023-05-01',\n",
    "    '2023-06-12', '2023-12-25', '2024-01-01', '2024-02-10', '2024-03-30',\n",
    "    '2024-05-01', '2024-12-25'\n",
    "]\n",
    "holidays_dates = pd.to_datetime(holidays_2024)\n",
    "stock_data['is_holiday'] = stock_data['date_only'].astype('datetime64[ns]').isin(holidays_dates).astype(int)\n",
    "\n",
    "# Aggregate by item and date\n",
    "item_sales = stock_data.groupby(['date_only', 'category']).agg({\n",
    "    'quantity': 'sum',\n",
    "    'price': 'mean',\n",
    "    'day_of_week': 'first',\n",
    "    'is_weekend': 'first',\n",
    "    'is_holiday': 'first',\n",
    "    'month': 'first'\n",
    "}).reset_index()\n",
    "#highest ten items for training\n",
    "categories = stock_data['category'].unique()\n",
    "item_sales.columns = ['date', 'category_name', 'quantity_sold', 'avg_price', 'day_of_week', 'is_weekend', 'is_holiday', 'month']\n",
    "item_sales['date'] = pd.to_datetime(item_sales['date'])\n",
    "item_sales = item_sales.sort_values(['category_name', 'date'])\n",
    "\n",
    "# FILL DATE GAPS FOR EACH ITEM\n",
    "print(f\"\\nâœ“ FILLING DATE GAPS FOR EACH ITEM...\")\n",
    "min_date = item_sales['date'].min()\n",
    "max_date = item_sales['date'].max()\n",
    "date_range = pd.date_range(min_date, max_date, freq='D')\n",
    "\n",
    "# Create complete date range for each item\n",
    "complete_dates = []\n",
    "for category in categories:\n",
    "    for date in date_range:\n",
    "        complete_dates.append({'date': date, 'category_name': category})\n",
    "\n",
    "complete_df = pd.DataFrame(complete_dates)\n",
    "\n",
    "# Merge with existing sales data\n",
    "item_sales_filled = complete_df.merge(\n",
    "    item_sales[['date', 'category_name', 'quantity_sold', 'avg_price']],\n",
    "    on=['date', 'category_name'],\n",
    "    how='left'\n",
    ")\n",
    "# Fill missing quantities with previous row values\n",
    "print(\"start filling\")\n",
    "item_sales_filled['quantity_sold'] = item_sales_filled['quantity_sold'].ffill()\n",
    "item_sales_filled['quantity_sold'] = item_sales_filled['quantity_sold'].bfill()\n",
    "print(\"finish filling\")\n",
    "item_sales_filled['avg_price']=item_sales_filled['avg_price'].mean()\n",
    "# Recalculate temporal features for all rows\n",
    "item_sales_filled['day_of_week'] = item_sales_filled['date'].dt.dayofweek\n",
    "item_sales_filled['is_weekend'] = item_sales_filled['day_of_week'].isin([5, 6]).astype(int)\n",
    "item_sales_filled['is_holiday'] = item_sales_filled['date'].dt.date.astype('datetime64[ns]').isin(holidays_dates).astype(int)\n",
    "item_sales_filled['month'] = item_sales_filled['date'].dt.month\n",
    "\n",
    "# Use filled data\n",
    "item_sales = item_sales_filled.sort_values(['category_name', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nâœ“ ITEM-LEVEL DATA PREPARED WITH DATE GAPS FILLED\")\n",
    "print(f\"   â”œâ”€ Items: {item_sales['category_name'].nunique()}\")\n",
    "print(f\"   â”œâ”€ Total records: {len(item_sales)} (including {len(item_sales[item_sales['quantity_sold']==0])} zero-sale days)\")\n",
    "print(f\"   â”œâ”€ Date range: {item_sales['date'].min().date()} to {item_sales['date'].max().date()}\")\n",
    "print(f\"   â”œâ”€ Unique days: {len(date_range)}\")\n",
    "print(f\"   â””â”€ Avg daily sales per item: {item_sales[item_sales['quantity_sold']>0]['quantity_sold'].mean():.2f} units\")\n",
    "\n",
    "print(f\"\\nSample Item Sales Data (with gaps filled):\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "892a02b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: PREPARE DATA FOR PER-ITEM XGBOOST MODELS\n",
      "======================================================================\n",
      "Axes(0.125,0.11;0.62x0.77)\n",
      "[[ 4.          0.          0.          2.         -0.34684872]\n",
      " [ 5.          1.          0.          2.         -0.3572497 ]\n",
      " [ 6.          1.          0.          2.         -0.35670228]\n",
      " ...\n",
      " [ 4.          0.          0.          5.          5.18373258]\n",
      " [ 5.          1.          0.          5.          4.98556664]\n",
      " [ 6.          1.          0.          5.          2.01690957]]\n",
      "Axes(0.125,0.11;0.496x0.77)\n",
      "[[ 4.          0.          0.          2.         -0.36513575]\n",
      " [ 5.          1.          0.          2.         -0.36546145]\n",
      " [ 6.          1.          0.          2.         -0.36546145]\n",
      " ...\n",
      " [ 4.          0.          0.          5.          5.05808657]\n",
      " [ 5.          1.          0.          5.          4.82781703]\n",
      " [ 6.          1.          0.          5.          2.71858708]]\n",
      "Axes(0.125,0.11;0.3968x0.77)\n",
      "[[4.         0.         0.         2.         9.56194545]\n",
      " [5.         1.         0.         2.         9.56194545]\n",
      " [6.         1.         0.         2.         9.56194545]\n",
      " ...\n",
      " [4.         0.         0.         5.         3.3417696 ]\n",
      " [5.         1.         0.         5.         2.9631502 ]\n",
      " [6.         1.         0.         5.         2.36817686]]\n",
      "Axes(0.125,0.11;0.31744x0.77)\n",
      "[[ 4.          0.          0.          2.         -0.3848971 ]\n",
      " [ 5.          1.          0.          2.         -0.3848971 ]\n",
      " [ 6.          1.          0.          2.         -0.3848971 ]\n",
      " ...\n",
      " [ 4.          0.          0.          5.          5.92468792]\n",
      " [ 5.          1.          0.          5.          5.45647215]\n",
      " [ 6.          1.          0.          5.          2.97190786]]\n",
      "Axes(0.125,0.11;0.253952x0.77)\n",
      "[[ 4.          0.          0.          2.         -0.03005542]\n",
      " [ 5.          1.          0.          2.         -0.03005542]\n",
      " [ 6.          1.          0.          2.         -0.03005542]\n",
      " ...\n",
      " [ 4.          0.          0.          5.          3.94499264]\n",
      " [ 5.          1.          0.          5.          3.33365395]\n",
      " [ 6.          1.          0.          5.          2.53379671]]\n",
      "Axes(0.125,0.11;0.203162x0.77)\n",
      "[[4.         0.         0.         2.         0.09973123]\n",
      " [5.         1.         0.         2.         0.09973123]\n",
      " [6.         1.         0.         2.         0.09973123]\n",
      " ...\n",
      " [4.         0.         0.         5.         3.99002289]\n",
      " [5.         1.         0.         5.         3.36185775]\n",
      " [6.         1.         0.         5.         3.51781599]]\n",
      "Axes(0.125,0.11;0.162529x0.77)\n",
      "[[4.         0.         0.         2.         0.46955349]\n",
      " [5.         1.         0.         2.         0.46955349]\n",
      " [6.         1.         0.         2.         0.46955349]\n",
      " ...\n",
      " [4.         0.         0.         5.         3.35304183]\n",
      " [5.         1.         0.         5.         3.26150252]\n",
      " [6.         1.         0.         5.         2.7580363 ]]\n",
      "Axes(0.125,0.11;0.130023x0.77)\n",
      "[[4.         0.         0.         2.         0.52045794]\n",
      " [5.         1.         0.         2.         0.52045794]\n",
      " [6.         1.         0.         2.         0.52045794]\n",
      " ...\n",
      " [4.         0.         0.         5.         4.24552785]\n",
      " [5.         1.         0.         5.         5.35876713]\n",
      " [6.         1.         0.         5.         1.99764083]]\n",
      "Axes(0.125,0.11;0.104019x0.77)\n",
      "[[4.         0.         0.         2.         1.77200498]\n",
      " [5.         1.         0.         2.         1.77200498]\n",
      " [6.         1.         0.         2.         1.77200498]\n",
      " ...\n",
      " [4.         0.         0.         5.         1.94779266]\n",
      " [5.         1.         0.         5.         1.80716252]\n",
      " [6.         1.         0.         5.         2.3696831 ]]\n",
      "Axes(0.125,0.11;0.083215x0.77)\n",
      "[[4.         0.         0.         2.         0.56924188]\n",
      " [5.         1.         0.         2.         0.56924188]\n",
      " [6.         1.         0.         2.         0.56924188]\n",
      " ...\n",
      " [4.         0.         0.         5.         3.19965731]\n",
      " [5.         1.         0.         5.         2.39162119]\n",
      " [6.         1.         0.         5.         1.44604702]]\n",
      "\n",
      "âœ“ PER-ITEM DATA PREPARED\n",
      "   â”œâ”€ Items in models: 10\n",
      "   â”œâ”€ Feature Names: day_of_week, is_weekend, is_holiday, month, quantity_sold\n",
      "   â””â”€ Train/Test Split: 80/20\n",
      "\n",
      "ðŸ“Š DATA SUMMARY BY ITEM:\n",
      "   Beverages:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 3085.94 units\n",
      "      â””â”€ Std dev: 3038.65 units\n",
      "   Other/Uncategorized:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 5343.75 units\n",
      "      â””â”€ Std dev: 4970.71 units\n",
      "   Sides & Snacks:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 218.61 units\n",
      "      â””â”€ Std dev: 196.09 units\n",
      "   Desserts & Sweets:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 480.70 units\n",
      "      â””â”€ Std dev: 416.69 units\n",
      "   Handhelds:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 660.94 units\n",
      "      â””â”€ Std dev: 587.51 units\n",
      "   Main Courses:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 428.09 units\n",
      "      â””â”€ Std dev: 355.99 units\n",
      "   Salads & Greens:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 76.28 units\n",
      "      â””â”€ Std dev: 70.87 units\n",
      "   Breakfast & Brunch:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 70.69 units\n",
      "      â””â”€ Std dev: 85.78 units\n",
      "   Sushi & Asian:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 47.26 units\n",
      "      â””â”€ Std dev: 43.25 units\n",
      "   Misc/Services:\n",
      "      â”œâ”€ Train samples: 948\n",
      "      â”œâ”€ Test samples: 238\n",
      "      â”œâ”€ Mean sales: 100.61 units\n",
      "      â””â”€ Std dev: 87.36 units\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 11 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 3: DATA PREPARATION FOR PER-ITEM MODELING\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: PREPARE DATA FOR PER-ITEM XGBOOST MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "item_sales_filled = item_sales_filled\n",
    "\n",
    "# Dictionary to store data for each item\n",
    "item_data_dict = {}\n",
    "scaler_price_dict = {}\n",
    "feature_names = ['day_of_week', 'is_weekend', 'is_holiday', 'month', 'quantity_sold']\n",
    "\n",
    "for category in categories:\n",
    "    # Get data for this item only\n",
    "    item_df = item_sales_filled[item_sales_filled['category_name'] == category].copy()\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X_features = item_df[feature_names].copy()\n",
    "    \n",
    "    # Normalize price for this item\n",
    "    scaler = StandardScaler()\n",
    "    X_features['quantity'] = scaler.fit_transform(X_features[['quantity_sold']])\n",
    "    scaler_price_dict[category] = scaler\n",
    "    X_features.drop('quantity_sold',axis=1,inplace=True)\n",
    "    # Target: quantity sold (shift by 1 to predict next day)\n",
    "    y = item_df['quantity_sold'].values[1:]\n",
    "    X_features = X_features[:-1].values\n",
    "    df = pd.DataFrame(X_features,columns=['day_of_week', 'is_weekend', 'is_holiday', 'month', 'quantity_sold'])\n",
    "    df['val']=pd.Series(y)\n",
    "    #show most correlated items\n",
    "    print(sns.heatmap(df.corr(),annot=True))\n",
    "    print(X_features)\n",
    "    # Train/test split (80/20)\n",
    "    X_features = X_features[:,3:5]\n",
    "    train_size = int(len(X_features) * 0.8)\n",
    "    X_train = X_features[:train_size]\n",
    "    X_test = X_features[train_size:]\n",
    "    y_train = y[:train_size]\n",
    "    y_test = y[train_size:]\n",
    "    \n",
    "    # Store item data\n",
    "    item_data_dict[category] = {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'total_samples': len(X_features)\n",
    "    }\n",
    "\n",
    "print(f\"\\nâœ“ PER-ITEM DATA PREPARED\")\n",
    "print(f\"   â”œâ”€ Items in models: {len(item_data_dict)}\")\n",
    "print(f\"   â”œâ”€ Feature Names: {', '.join(feature_names)}\")\n",
    "print(f\"   â””â”€ Train/Test Split: 80/20\")\n",
    "\n",
    "print(f\"\\nðŸ“Š DATA SUMMARY BY ITEM:\")\n",
    "for item, data in item_data_dict.items():\n",
    "    print(f\"   {item}:\")\n",
    "    print(f\"      â”œâ”€ Train samples: {len(data['y_train'])}\")\n",
    "    print(f\"      â”œâ”€ Test samples: {len(data['y_test'])}\")\n",
    "    print(f\"      â”œâ”€ Mean sales: {data['y_test'].mean():.2f} units\")\n",
    "    print(f\"      â””â”€ Std dev: {data['y_test'].std():.2f} units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f1277ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>total_qt</th>\n",
       "      <th>lagged_qt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.720177</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.788946</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.788946</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.720177</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.685793</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.340044</td>\n",
       "      <td>189.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.374429</td>\n",
       "      <td>237.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.199657</td>\n",
       "      <td>190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.391621</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.446047</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     month  total_qt  lagged_qt\n",
       "0      9.0 -0.720177        5.0\n",
       "1      9.0 -0.788946        5.0\n",
       "2      9.0 -0.788946        9.0\n",
       "3      9.0 -0.720177       11.0\n",
       "4      9.0 -0.685793       15.0\n",
       "..     ...       ...        ...\n",
       "233    5.0  2.340044      189.0\n",
       "234    5.0  2.374429      237.0\n",
       "235    5.0  3.199657      190.0\n",
       "236    5.0  2.391621      135.0\n",
       "237    5.0  1.446047       13.0\n",
       "\n",
       "[238 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X_test,columns=['month','total_qt'])\n",
    "df['lagged_qt'] = pd.Series(y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b91ee2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Stock_Forecasting_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b25117ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 4: TRAIN PER-ITEM XGBOOST MODELS\n",
      "======================================================================\n",
      "\n",
      "ðŸ”„ Training 10 models...\n",
      "âœ“ All models trained successfully!\n",
      "\n",
      "======================================================================\n",
      "PER-ITEM MODEL PERFORMANCE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.8870\n",
      "   â”œâ”€ Test RÂ²: -0.9425\n",
      "   â”œâ”€ RMSE: 4235.05 units\n",
      "   â”œâ”€ MAE: 2952.37 units\n",
      "   â””â”€ MAPE: 73.71%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.8181\n",
      "   â”œâ”€ Test RÂ²: -0.0606\n",
      "   â”œâ”€ RMSE: 88.34 units\n",
      "   â”œâ”€ MAE: 48.09 units\n",
      "   â””â”€ MAPE: 58.96%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.7289\n",
      "   â”œâ”€ Test RÂ²: -1.1350\n",
      "   â”œâ”€ RMSE: 608.86 units\n",
      "   â”œâ”€ MAE: 444.91 units\n",
      "   â””â”€ MAPE: 72.64%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.7936\n",
      "   â”œâ”€ Test RÂ²: -0.8729\n",
      "   â”œâ”€ RMSE: 804.02 units\n",
      "   â”œâ”€ MAE: 586.93 units\n",
      "   â””â”€ MAPE: 80.92%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.9084\n",
      "   â”œâ”€ Test RÂ²: -0.7545\n",
      "   â”œâ”€ RMSE: 471.54 units\n",
      "   â”œâ”€ MAE: 344.94 units\n",
      "   â””â”€ MAPE: 68.98%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.9918\n",
      "   â”œâ”€ Test RÂ²: 0.1811\n",
      "   â”œâ”€ RMSE: 79.06 units\n",
      "   â”œâ”€ MAE: 54.79 units\n",
      "   â””â”€ MAPE: 52.56%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.9162\n",
      "   â”œâ”€ Test RÂ²: -0.9848\n",
      "   â”œâ”€ RMSE: 7002.88 units\n",
      "   â”œâ”€ MAE: 4965.27 units\n",
      "   â””â”€ MAPE: 69.41%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.9535\n",
      "   â”œâ”€ Test RÂ²: -0.2394\n",
      "   â”œâ”€ RMSE: 78.90 units\n",
      "   â”œâ”€ MAE: 54.89 units\n",
      "   â””â”€ MAPE: 58.30%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.7473\n",
      "   â”œâ”€ Test RÂ²: -1.2211\n",
      "   â”œâ”€ RMSE: 292.24 units\n",
      "   â”œâ”€ MAE: 216.46 units\n",
      "   â””â”€ MAPE: 92.16%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.9764\n",
      "   â”œâ”€ Test RÂ²: 0.3076\n",
      "   â”œâ”€ RMSE: 35.99 units\n",
      "   â”œâ”€ MAE: 23.32 units\n",
      "   â””â”€ MAPE: 58.15%\n",
      "\n",
      "======================================================================\n",
      "FEATURE IMPORTANCE BY ITEM\n",
      "======================================================================\n",
      "\n",
      "Beverages:\n",
      "   is_weekend: 0.7676\n",
      "   day_of_week: 0.2324\n",
      "\n",
      "Breakfast & Brunch:\n",
      "   is_weekend: 0.8226\n",
      "   day_of_week: 0.1774\n",
      "\n",
      "Desserts & Sweets:\n",
      "   is_weekend: 0.8243\n",
      "   day_of_week: 0.1757\n",
      "\n",
      "Handhelds:\n",
      "   is_weekend: 0.8831\n",
      "   day_of_week: 0.1169\n",
      "\n",
      "Main Courses:\n",
      "   is_weekend: 0.8707\n",
      "   day_of_week: 0.1293\n",
      "\n",
      "Misc/Services:\n",
      "   is_weekend: 0.9660\n",
      "   day_of_week: 0.0340\n",
      "\n",
      "Other/Uncategorized:\n",
      "   is_weekend: 0.7045\n",
      "   day_of_week: 0.2955\n",
      "\n",
      "Salads & Greens:\n",
      "   is_weekend: 0.9320\n",
      "   day_of_week: 0.0680\n",
      "\n",
      "Sides & Snacks:\n",
      "   is_weekend: 0.9522\n",
      "   day_of_week: 0.0478\n",
      "\n",
      "Sushi & Asian:\n",
      "   is_weekend: 0.8493\n",
      "   day_of_week: 0.1507\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: TRAIN PER-ITEM XGBOOST MODELS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: TRAIN PER-ITEM XGBOOST MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dictionary to store trained models and metrics\n",
    "xgb_models_dict = {}\n",
    "model_metrics = {}\n",
    "\n",
    "print(f\"\\nðŸ”„ Training {len(item_data_dict)} models...\")\n",
    "\n",
    "for item, data in item_data_dict.items():\n",
    "    # Initialize model for this item\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=10000,\n",
    "        learning_rate=0.005,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(data['X_train'], data['y_train'], verbose=False)\n",
    "    xgb_models_dict[item] = model\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(data['X_train'])\n",
    "    y_pred_test = model.predict(data['X_test'])\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    train_r2 = r2_score(data['y_train'], y_pred_train)\n",
    "    test_r2 = r2_score(data['y_test'], y_pred_test)\n",
    "    rmse = np.sqrt(mean_squared_error(data['y_test'], y_pred_test))\n",
    "    mae = mean_absolute_error(data['y_test'], y_pred_test)\n",
    "    mape = np.mean(np.abs((data['y_test'] - y_pred_test) / (data['y_test'] + 1))) * 100\n",
    "    \n",
    "    model_metrics[item] = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mape': mape,\n",
    "        'importances': model.feature_importances_\n",
    "    }\n",
    "\n",
    "print(\"âœ“ All models trained successfully!\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"PER-ITEM MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for category in sorted(model_metrics.keys()):\n",
    "    metrics = model_metrics[category]\n",
    "    print(f\"\\n{item}:\")\n",
    "    print(f\"   â”œâ”€ Train RÂ²: {metrics['train_r2']:.4f}\")\n",
    "    print(f\"   â”œâ”€ Test RÂ²: {metrics['test_r2']:.4f}\")\n",
    "    print(f\"   â”œâ”€ RMSE: {metrics['rmse']:.2f} units\")\n",
    "    print(f\"   â”œâ”€ MAE: {metrics['mae']:.2f} units\")\n",
    "    print(f\"   â””â”€ MAPE: {metrics['mape']:.2f}%\")\n",
    "\n",
    "# Feature importance across all models\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE BY ITEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for item in sorted(model_metrics.keys()):\n",
    "    importances = model_metrics[item]['importances']\n",
    "    print(f\"\\n{item}:\")\n",
    "    for fname, imp in sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   {fname}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1b59a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def predict_next_day(current_date, category_name, verbose=True):\n",
    "    \"\"\"\n",
    "    Predict sales volume for the next day for a specific item\n",
    "    Uses the item-specific trained XGBoost model.\n",
    "    \"\"\"\n",
    "    # 1. Validate item exists in trained models\n",
    "    if category_name not in xgb_models_dict:\n",
    "        return {'error': f'Item {category_name} not in trained models.'}\n",
    "\n",
    "    # 2. Convert to datetime if string\n",
    "    current_dt = pd.to_datetime(current_date)\n",
    "    next_dt = current_dt + pd.Timedelta(days=1)\n",
    "\n",
    "    # 3. Extract time-based features for the prediction date (Next Day)\n",
    "    day_of_week = next_dt.dayofweek\n",
    "    is_weekend = 1 if next_dt.dayofweek in [5, 6] else 0\n",
    "    # Assumes holidays_dates is a list of datetime objects available in scope\n",
    "    is_holiday = 1 if next_dt.date() in [d.date() for d in holidays_dates] else 0\n",
    "    month = next_dt.month\n",
    "\n",
    "    # 4. Get the feature: \"Quantity Sold\" from the current_date (Today)\n",
    "    # We filter for the specific item\n",
    "    item_data = item_sales_filled[item_sales_filled['category_name'] == category_name]\n",
    "    \n",
    "    if item_data.empty:\n",
    "        return {'error': f'No historical data for item {category_name}'}\n",
    "\n",
    "    # Find the row corresponding to \"today\" to use its quantity as a feature for \"tomorrow\"\n",
    "    selected_row = item_data[item_data['date'] == current_dt]\n",
    "\n",
    "    if not selected_row.empty:\n",
    "        last_qty = float(selected_row['quantity_sold'].iloc[0])\n",
    "    else:\n",
    "        # Fallback: Find the most recent date available BEFORE or ON the current_dt\n",
    "        recent_data = item_data[item_data['date'] <= current_dt].sort_values('date')\n",
    "        if not recent_data.empty:\n",
    "            last_qty = float(recent_data['quantity_sold'].iloc[-1])\n",
    "        else:\n",
    "            last_qty = 0.0\n",
    "\n",
    "    # 5. Scale the quantity feature\n",
    "    scaler = scaler_price_dict.get(category_name)\n",
    "    if scaler is not None:\n",
    "        # Reshape for scaler: [[value]]\n",
    "        qty_scaled = float(scaler.transform([[last_qty]])[0][0])\n",
    "    else:\n",
    "        qty_scaled = last_qty\n",
    "\n",
    "    # 6. Create feature vector (Must match training order exactly)\n",
    "    features = np.array([[\n",
    "        month,\n",
    "        qty_scaled\n",
    "    ]])\n",
    "\n",
    "    # 7. Get model and performance metrics\n",
    "    model = xgb_models_dict[category_name]\n",
    "    metrics = model_metrics.get(category_name, {})\n",
    "    mae = metrics.get('mae', 0)\n",
    "    r2 = metrics.get('test_r2', 0)\n",
    "\n",
    "    # 8. Make prediction\n",
    "    # XGBoost returns an array, we take the first element\n",
    "    prediction = float(model.predict(features)[0])\n",
    "    \n",
    "    # Ensure we don't predict negative sales\n",
    "    prediction = max(0, prediction)\n",
    "\n",
    "    # 9. Calculate Bounds and Safety Stock\n",
    "    lower_bound = max(0, prediction - mae)\n",
    "    upper_bound = prediction + mae\n",
    "    safety_stock = prediction + mae # Simple recommendation: Predict + Error Margin\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nðŸ“… WALKFORWARD PREDICTION ({category_name})\")\n",
    "        print(f\" Â  Current Date: {current_dt.date()} (Qty: {last_qty})\")\n",
    "        print(f\" Â  Predict For: {next_dt.date()}\")\n",
    "        print(f\"\\nðŸ“Š FEATURES:\")\n",
    "        print(f\" Â  â”œâ”€ Day: {next_dt.day_name()} | Month: {next_dt.month}\")\n",
    "        print(f\" Â  â””â”€ Holiday: {'Yes' if is_holiday else 'No'}\")\n",
    "        print(f\"\\nðŸŽ¯ PREDICTION:\")\n",
    "        print(f\" Â  â”œâ”€ Expected Sales: {prediction:.2f} units\")\n",
    "        print(f\" Â  â”œâ”€ Confidence Range: [{lower_bound:.2f} - {upper_bound:.2f}]\")\n",
    "        print(f\" Â  â””â”€ Model Accuracy (RÂ²): {r2:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'date': next_dt.date(),\n",
    "        'category_name': category_name,\n",
    "        'predicted_quantity': prediction,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound,\n",
    "        'safety_stock': safety_stock,\n",
    "        'model_r2': r2,\n",
    "        'confidence_margin': (upper_bound - lower_bound),\n",
    "        'feature_used': last_qty\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f789d350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 9: WALKFORWARD PREDICTIONS FOR TOP 5 ITEMS (PER-ITEM MODELS)\n",
      "======================================================================\n",
      "\n",
      "ðŸ“… WALKFORWARD PREDICTION (Beverages)\n",
      " Â  Current Date: 2024-05-13 (Qty: 764.0)\n",
      " Â  Predict For: 2024-05-14\n",
      "\n",
      "ðŸ“Š FEATURES:\n",
      " Â  â”œâ”€ Day: Tuesday | Month: 5\n",
      " Â  â””â”€ Holiday: No\n",
      "\n",
      "ðŸŽ¯ PREDICTION:\n",
      " Â  â”œâ”€ Expected Sales: 135.87 units\n",
      " Â  â”œâ”€ Confidence Range: [0.00 - 3088.24]\n",
      " Â  â””â”€ Model Accuracy (RÂ²): -0.9425\n",
      "\n",
      "ðŸ“… WALKFORWARD PREDICTION (Other/Uncategorized)\n",
      " Â  Current Date: 2024-05-13 (Qty: 1466.0)\n",
      " Â  Predict For: 2024-05-14\n",
      "\n",
      "ðŸ“Š FEATURES:\n",
      " Â  â”œâ”€ Day: Tuesday | Month: 5\n",
      " Â  â””â”€ Holiday: No\n",
      "\n",
      "ðŸŽ¯ PREDICTION:\n",
      " Â  â”œâ”€ Expected Sales: 411.71 units\n",
      " Â  â”œâ”€ Confidence Range: [0.00 - 5376.98]\n",
      " Â  â””â”€ Model Accuracy (RÂ²): -0.9848\n",
      "\n",
      "ðŸ“… WALKFORWARD PREDICTION (Sides & Snacks)\n",
      " Â  Current Date: 2024-05-13 (Qty: 4.0)\n",
      " Â  Predict For: 2024-05-14\n",
      "\n",
      "ðŸ“Š FEATURES:\n",
      " Â  â”œâ”€ Day: Tuesday | Month: 5\n",
      " Â  â””â”€ Holiday: No\n",
      "\n",
      "ðŸŽ¯ PREDICTION:\n",
      " Â  â”œâ”€ Expected Sales: 4.44 units\n",
      " Â  â”œâ”€ Confidence Range: [0.00 - 220.90]\n",
      " Â  â””â”€ Model Accuracy (RÂ²): -1.2211\n",
      "\n",
      "ðŸ“… WALKFORWARD PREDICTION (Desserts & Sweets)\n",
      " Â  Current Date: 2024-05-13 (Qty: 127.0)\n",
      " Â  Predict For: 2024-05-14\n",
      "\n",
      "ðŸ“Š FEATURES:\n",
      " Â  â”œâ”€ Day: Tuesday | Month: 5\n",
      " Â  â””â”€ Holiday: No\n",
      "\n",
      "ðŸŽ¯ PREDICTION:\n",
      " Â  â”œâ”€ Expected Sales: 35.87 units\n",
      " Â  â”œâ”€ Confidence Range: [0.00 - 480.78]\n",
      " Â  â””â”€ Model Accuracy (RÂ²): -1.1350\n",
      "\n",
      "ðŸ“… WALKFORWARD PREDICTION (Handhelds)\n",
      " Â  Current Date: 2024-05-13 (Qty: 121.0)\n",
      " Â  Predict For: 2024-05-14\n",
      "\n",
      "ðŸ“Š FEATURES:\n",
      " Â  â”œâ”€ Day: Tuesday | Month: 5\n",
      " Â  â””â”€ Holiday: No\n",
      "\n",
      "ðŸŽ¯ PREDICTION:\n",
      " Â  â”œâ”€ Expected Sales: 15.23 units\n",
      " Â  â”œâ”€ Confidence Range: [0.00 - 602.17]\n",
      " Â  â””â”€ Model Accuracy (RÂ²): -0.8729\n",
      "[{'date': datetime.date(2024, 5, 14), 'category_name': 'Beverages', 'predicted_quantity': 135.87437438964844, 'lower_bound': 0, 'upper_bound': 3088.243343321215, 'safety_stock': 3088.243343321215, 'model_r2': -0.942479951834786, 'confidence_margin': 3088.243343321215, 'feature_used': 764.0}, {'date': datetime.date(2024, 5, 14), 'category_name': 'Other/Uncategorized', 'predicted_quantity': 411.7144470214844, 'lower_bound': 0, 'upper_bound': 5376.984943806624, 'safety_stock': 5376.984943806624, 'model_r2': -0.9848001126805883, 'confidence_margin': 5376.984943806624, 'feature_used': 1466.0}, {'date': datetime.date(2024, 5, 14), 'category_name': 'Sides & Snacks', 'predicted_quantity': 4.43956184387207, 'lower_bound': 0, 'upper_bound': 220.8964933958374, 'safety_stock': 220.8964933958374, 'model_r2': -1.2211181423796784, 'confidence_margin': 220.8964933958374, 'feature_used': 4.0}, {'date': datetime.date(2024, 5, 14), 'category_name': 'Desserts & Sweets', 'predicted_quantity': 35.87357711791992, 'lower_bound': 0, 'upper_bound': 480.78413709271854, 'safety_stock': 480.78413709271854, 'model_r2': -1.1350073378524805, 'confidence_margin': 480.78413709271854, 'feature_used': 127.0}, {'date': datetime.date(2024, 5, 14), 'category_name': 'Handhelds', 'predicted_quantity': 15.231729507446289, 'lower_bound': 0, 'upper_bound': 602.1663748737143, 'safety_stock': 602.1663748737143, 'model_r2': -0.8728896330581666, 'confidence_margin': 602.1663748737143, 'feature_used': 121.0}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Predicted Date</th>\n",
       "      <th>Predicted Quantity</th>\n",
       "      <th>Lower Bound</th>\n",
       "      <th>Upper Bound</th>\n",
       "      <th>Safety Stock</th>\n",
       "      <th>Confidence (Â±)</th>\n",
       "      <th>Model RÂ²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beverages</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3088.0</td>\n",
       "      <td>3088.0</td>\n",
       "      <td>3088.0</td>\n",
       "      <td>-0.9425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Other/Uncategorized</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>412.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5377.0</td>\n",
       "      <td>5377.0</td>\n",
       "      <td>5377.0</td>\n",
       "      <td>-0.9848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sides &amp; Snacks</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>-1.2211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Desserts &amp; Sweets</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>-1.1350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Handhelds</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>-0.8729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Category Predicted Date  Predicted Quantity  Lower Bound  \\\n",
       "0            Beverages     2024-05-14               136.0            0   \n",
       "1  Other/Uncategorized     2024-05-14               412.0            0   \n",
       "2       Sides & Snacks     2024-05-14                 4.0            0   \n",
       "3    Desserts & Sweets     2024-05-14                36.0            0   \n",
       "4            Handhelds     2024-05-14                15.0            0   \n",
       "\n",
       "   Upper Bound  Safety Stock  Confidence (Â±)  Model RÂ²  \n",
       "0       3088.0        3088.0          3088.0   -0.9425  \n",
       "1       5377.0        5377.0          5377.0   -0.9848  \n",
       "2        221.0         221.0           221.0   -1.2211  \n",
       "3        481.0         481.0           481.0   -1.1350  \n",
       "4        602.0         602.0           602.0   -0.8729  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 9: DEMONSTRATION - WALKFORWARD PREDICTIONS WITH PER-ITEM MODELS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 9: WALKFORWARD PREDICTIONS FOR TOP 5 ITEMS (PER-ITEM MODELS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get the latest date in data\n",
    "latest_date = item_sales_filled['date'].max()\n",
    "\n",
    "# Make predictions for top 5 items using their specific models\n",
    "results_list = []\n",
    "for category in categories[:5]:\n",
    "    result = predict_next_day(latest_date,category)\n",
    "    results_list.append(result)\n",
    "print(results_list)\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Category': r['category_name'],\n",
    "        'Predicted Date': r['date'],\n",
    "        'Predicted Quantity': round(r['predicted_quantity'], 0),\n",
    "        'Lower Bound': round(r['lower_bound'], 0),\n",
    "        'Upper Bound': round(r['upper_bound'], 0),\n",
    "        'Safety Stock': round(r['safety_stock'], 0),\n",
    "        'Confidence (Â±)': round(r['confidence_margin'], 0),\n",
    "        'Model RÂ²': round(r['model_r2'], 4)\n",
    "    }\n",
    "    for r in results_list\n",
    "])\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e6bca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 5: PREPARE SEQUENCES FOR LSTM MODELS\n",
      "======================================================================\n",
      "\n",
      "ðŸ”„ Creating sequences (lookback window: 10 days)...\n",
      "{'Beverages': {'X_train_seq': array([[[ 2.        ,  0.11960078],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.51323487],\n",
      "        ...,\n",
      "        [ 2.        , -0.1616595 ],\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.51323487]],\n",
      "\n",
      "       [[ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.51323487],\n",
      "        ...,\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.5483924 ]],\n",
      "\n",
      "       [[ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.51323487],\n",
      "        ...,\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.5483924 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        ,  2.29936802],\n",
      "        [ 9.        ,  2.61578585],\n",
      "        [ 9.        ,  1.94779266],\n",
      "        ...,\n",
      "        [ 9.        ,  3.77598454],\n",
      "        [ 9.        ,  2.05326527],\n",
      "        [ 9.        ,  5.42838874]],\n",
      "\n",
      "       [[ 9.        ,  2.61578585],\n",
      "        [ 9.        ,  1.94779266],\n",
      "        [ 9.        ,  2.01810774],\n",
      "        ...,\n",
      "        [ 9.        ,  2.05326527],\n",
      "        [ 9.        ,  5.42838874],\n",
      "        [ 9.        ,  1.66653237]],\n",
      "\n",
      "       [[ 9.        ,  1.94779266],\n",
      "        [ 9.        ,  2.01810774],\n",
      "        [ 9.        ,  3.14314889],\n",
      "        ...,\n",
      "        [ 9.        ,  5.42838874],\n",
      "        [ 9.        ,  1.66653237],\n",
      "        [ 9.        ,  3.38925164]]]), 'X_test_seq': array([[[  9.        ,   2.01810774],\n",
      "        [  9.        ,   3.14314889],\n",
      "        [  9.        ,   2.65094339],\n",
      "        ...,\n",
      "        [  9.        ,   1.66653237],\n",
      "        [  9.        ,   3.38925164],\n",
      "        [  9.        ,   5.00649831]],\n",
      "\n",
      "       [[  9.        ,   3.14314889],\n",
      "        [  9.        ,   2.65094339],\n",
      "        [  9.        ,   3.45956672],\n",
      "        ...,\n",
      "        [  9.        ,   3.38925164],\n",
      "        [  9.        ,   5.00649831],\n",
      "        [  9.        ,   2.58062831]],\n",
      "\n",
      "       [[  9.        ,   2.65094339],\n",
      "        [  9.        ,   3.45956672],\n",
      "        [  9.        ,   3.77598454],\n",
      "        ...,\n",
      "        [  9.        ,   5.00649831],\n",
      "        [  9.        ,   2.58062831],\n",
      "        [  9.        ,   3.45956672]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[  5.        , 279.9735884 ],\n",
      "        [  5.        , 221.08471538],\n",
      "        [  5.        , 398.34901255],\n",
      "        ...,\n",
      "        [  5.        , 296.63826052],\n",
      "        [  5.        , 241.61671648],\n",
      "        [  5.        , 355.31618833]],\n",
      "\n",
      "       [[  5.        , 221.08471538],\n",
      "        [  5.        , 398.34901255],\n",
      "        [  5.        , 352.22232515],\n",
      "        ...,\n",
      "        [  5.        , 241.61671648],\n",
      "        [  5.        , 355.31618833],\n",
      "        [  5.        , 342.58916025]],\n",
      "\n",
      "       [[  5.        , 398.34901255],\n",
      "        [  5.        , 352.22232515],\n",
      "        [  5.        , 146.30463602],\n",
      "        ...,\n",
      "        [  5.        , 355.31618833],\n",
      "        [  5.        , 342.58916025],\n",
      "        [  5.        , 151.9298418 ]]]), 'y_train_seq': array([  1.,   1.,   1.,   1.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "         4.,   4.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   1.,\n",
      "         7.,   2.,   1.,   2.,   2.,   2.,   2.,   2.,   2.,   5.,   1.,\n",
      "         1.,   1.,   1.,   4.,   4.,   4.,   4.,   4.,   3.,   3.,   2.,\n",
      "         2.,   1.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   1.,   2.,\n",
      "         2.,   2.,   2.,   2.,   2.,   1.,   3.,   3.,   3.,   1.,   3.,\n",
      "         8.,   3.,   2.,   2.,   2.,   2.,   6.,   6.,   6.,   6.,   6.,\n",
      "         6.,   6.,   6.,   6.,   6.,   2.,   5.,   5.,   5.,   5.,   5.,\n",
      "         5.,   5.,   1.,   4.,   4.,   1.,   1.,   2.,   6.,   6.,   6.,\n",
      "         7.,   2.,   8.,   1.,   6.,   6.,   3.,   3.,   3.,   2.,   2.,\n",
      "         2.,   1.,   1.,   1.,   1.,   1.,   4.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   6.,   6.,   1.,   2.,   2.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   2.,   3.,   3.,   3.,   3.,\n",
      "         3.,   6.,   7.,   6.,   1.,   1.,   9.,   6.,   2.,   4.,   4.,\n",
      "         3.,   3.,   3.,   7.,   7.,   8.,   5.,   5.,   2.,   2.,   1.,\n",
      "         4.,   7.,   7.,   2.,   2.,   2.,   2.,   6.,   2.,   7.,   5.,\n",
      "         3.,  12.,  12.,  13.,   4.,   1.,  11.,   6.,   3.,   7.,   3.,\n",
      "         7.,   7.,   7.,   5.,   6.,   1.,   1.,   1.,   2.,   3.,   3.,\n",
      "         3.,   3.,   3.,   3.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   2.,   2.,\n",
      "         2.,   2.,   4.,   4.,   2.,   3.,   3.,   6.,   9.,   7.,   3.,\n",
      "         2.,   1.,   1.,   1.,   1.,   6.,   3.,   3.,   3.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   7.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   2.,   3.,   3.,   1.,   2.,   2.,   2.,   2.,   1.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   1.,   1.,   1.,   1.,   1.,\n",
      "         2.,   2.,   2.,   4.,   4.,   1.,   1.,   2.,   1.,   2.,   1.,\n",
      "         1.,   1.,   1.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   3.,\n",
      "         3.,   2.,   2.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   3.,   2.,\n",
      "         2.,   1.,   6.,   6.,   1.,   1.,   1.,   1.,   1.,   3.,  14.,\n",
      "        14.,  14.,   5.,   5.,   1.,   3.,   1.,   1.,   1.,   1.,   1.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   4.,   4.,   4.,   2.,   2.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   3.,   3.,  28.,\n",
      "        41.,  70.,  52.,  34.,  29.,  40.,  54.,  49.,  56.,  83.,   9.,\n",
      "        35.,  34.,  33.,  21.,  10.,  10.,  22.,  23.,  32.,  29.,  26.,\n",
      "        68.,  57.,  23.,  33.,  30.,  21.,  32.,  56.,  50.,  29.,  30.,\n",
      "        39.,  51.,  26.,  43.,  67.,  52.,  50.,  20.,  70.,  29.,  25.,\n",
      "        47.,  15.,   6.,  55.,  16.,  36.,  47.,  37.,  23.,  21.,  37.,\n",
      "        90.,  83., 104.,  57.,  14.,  26.,  18.,   7.,  16.,  28.,  31.,\n",
      "         5.,   2.,  33.,  20.,  32.,  52.,  32.,  23.,   9.,  47.,  42.,\n",
      "        56.,  74.,  91.,  34.,  32.,  51.,  49.,  87.,  80.,  76.,  45.,\n",
      "        41.,  51.,  56.,  79.,  74.,  56.,  50.,  49.,  36.,  58.,  34.,\n",
      "        48.,  66.,  62.,  76.,  43.,  58.,  63.,  56.,  63.,  57.,  55.,\n",
      "        35.,  49.,  49.,  72.,  57.,  36.,  53.,  31.,  30.,  49., 126.,\n",
      "       142., 114., 121., 165., 122., 112., 144., 156., 115., 144., 191.,\n",
      "       153., 144., 190., 138., 112., 124., 135., 225., 144., 192., 112.,\n",
      "       113., 161., 156., 154., 168., 197., 196., 145., 130., 143., 189.,\n",
      "       180., 205., 197., 101., 154., 208., 168., 236., 253., 236., 126.,\n",
      "       152., 168., 189., 180., 168., 263., 145., 167., 175., 200., 141.,\n",
      "       237., 184., 157., 108., 142., 137., 173., 213., 183.,  94., 125.,\n",
      "       117., 107., 104., 162., 172., 102., 108., 120., 125.,  95., 123.,\n",
      "       203., 127.,  99., 126.,  80., 159., 164., 182.,  89.,  74.,  91.,\n",
      "        82.,  81., 150., 124.,  86.,  92.,  82.,  97.,  68., 144., 175.,\n",
      "       117.,  93.,  58.,  61.,  68., 202., 132.,  95.,  67., 100., 104.,\n",
      "        96., 111., 156.,  56.,  58.,  58.,  67.,  98., 139., 151.,  63.,\n",
      "        41.,  41.,  71.,  49., 102.,  72.,  57.,  54.,  69.,  73.,  65.,\n",
      "        85.,  94.,  41.,  40.,  66.,  43.,  36.,  85., 109.,  54.,  37.,\n",
      "        50.,  59.,  40.,  80.,  82.,  43.,  55.,  42.,  22.,  50.,  64.,\n",
      "       109.,  43.,  38.,  50.,  56.,  30.,  55., 106.,  41.,  42.,  54.,\n",
      "        37.,  48., 101.,  71.,  33.,  50.,  61.,  39.,  53.,  22.,   5.,\n",
      "        13.,  30.,  33.,  61.,  70.,  91.,  43.,  50.,  33.,  37.,  51.,\n",
      "        41.,  89., 113.,  44.,  51.,  33.,  67.,  27., 153.,  92.,  51.,\n",
      "        49.,  56.,  52.,  54.,  75., 116.,  31.,  43.,  35.,  46.,  39.,\n",
      "        85., 120.,  64.,  55.,  58.,  72.,  52., 103., 112.,  34.,  40.,\n",
      "        57.,  68.,  40.,  80.,  69.,  38.,  42.,  96.,  55.,  57.,  87.,\n",
      "        96.,  51.,  24.,  31.,  47.,  44.,  55.,  76.,  47.,  43.,  54.,\n",
      "        40.,  46., 119.,  91.,  51.,  45.,  36.,  37.,  42.,  85., 133.,\n",
      "        44.,  33.,  41.,  32.,  31.,  65.,  58.,  47.,  40.,  40.,  54.,\n",
      "        46.,  41.,  61.,  36.,  36.,  45.,  53.,  32.,  69.,  66.,  53.,\n",
      "        35.,  54.,  34.,  40.,  50.,  62.,  37.,  38.,  35.,  31.,  35.,\n",
      "        57.,  34.,  41.,  43.,  27.,  26.,  28.,  30.,  45.,  46.,  37.,\n",
      "        45.,  64.,  40.,  55.,  60.,  42.,  46.,  67.,  47.,  31.,  51.,\n",
      "        54.,  29.,  34.,  22.,  23.,  28.,  41.,  54.,  39.,  32.,  37.,\n",
      "        44.,  39.,  34.,  37.,  20.,  60.,  23.,  52.,  56.,  52.,  67.,\n",
      "        26.,  60.,  74.,  57.,  56.,  81., 102.,  37.,  85.,  60.,  59.,\n",
      "        70.,  70.,  59.,  29.,  49.,  43.,  50.,  44.,  69.,  96.,  30.,\n",
      "        63.,  67.,  38.,  57.,  45.,  74.,  24.,  46.,  70.,  72.,  92.,\n",
      "        77.,  39.,  24.,  56.,  60.,  45.,  48.,  70.,  61.,  27.,  39.,\n",
      "        31.,  27.,  45.,  64.,  53.,  26.,  32.,  67.,  33.,  44.,  56.,\n",
      "        26.,  14.,  34.,  55.,  50.,  47.,  57.,  43.,  32.,  73.,  55.,\n",
      "        57.,  59.,  48.,  48.,  14.,  43.,  50.,  65.,  55.,  54.,  71.,\n",
      "        16.,  65.,  41.,  55.,  39.,  54., 159.,  74.,  52., 105., 118.,\n",
      "       134.,  87.,  69.,  82.,  62.,  72.,  68.,  52., 130., 106.,  95.,\n",
      "        81.,  57.,  64.,  76.,  82.,  91.,  72.,  74., 106.,  92., 115.,\n",
      "       124.,  75., 171.,  64., 113., 159.]), 'y_test_seq': array([   90.,   115.,   165.,   167.,    98.,    80.,    81.,    87.,\n",
      "         205.,   144.,   197.,   138.,   134.,   161.,   132.,   170.,\n",
      "         199.,   185.,   156.,   144.,   154.,   129.,   159.,   222.,\n",
      "         200.,   146.,   156.,   132.,   168.,   166.,   176.,   181.,\n",
      "         191.,   170.,   145.,   227.,   216.,   218.,   294.,   174.,\n",
      "         172.,   196.,   243.,   233.,   198.,   259.,   155.,   156.,\n",
      "         183.,   201.,   337.,   224.,   149.,   204.,   185.,   222.,\n",
      "         197.,   241.,   391.,   162.,   145.,   283.,   233.,   267.,\n",
      "         262.,   307.,   295.,   207.,   284.,   368.,   372.,   475.,\n",
      "         686.,   681.,   317.,   416.,   415.,   643.,   792.,   859.,\n",
      "         620.,   355.,   648.,   610.,   784.,   679.,   832.,   559.,\n",
      "         543.,   427.,   595.,   545.,   756.,   472.,   116.,   908.,\n",
      "         565.,   936.,   994.,  1776.,   795.,   419.,   469.,   934.,\n",
      "        1063.,  1817.,  3390.,  3874.,  1924.,  1766.,  2374.,  2812.,\n",
      "        3055.,  5259.,  5043.,  2184.,  1955.,  2561.,  2397.,  3414.,\n",
      "        5404.,  5474.,  1951.,  2588.,  3084.,  3512.,  4371.,  6158.,\n",
      "        7162.,  3086.,  2693.,  3460.,  5119.,  4517.,  8091.,  7872.,\n",
      "        3359.,  3166.,  3515.,  4322.,  4774.,  6529.,  6411.,  3117.,\n",
      "        3176.,  4141.,  4673.,  4546.,  6955.,  7323.,  2644.,  3288.,\n",
      "        4257.,  4441.,  4458.,  7483.,  6934.,  2716.,  3116.,  3945.,\n",
      "        4409.,  5695.,  7443.,  8222.,  3793.,  3236.,  4144.,  5135.,\n",
      "        5073.,  8014.,  7880.,  3596.,  3939.,  3937.,  4530.,  4849.,\n",
      "        8402.,  9482.,  4882.,  4048.,  4421.,  4777.,  5288.,  8044.,\n",
      "        9122.,  4183.,  4566.,  5736.,  8340.,  6419.,  7035.,  8247.,\n",
      "        4450.,  3733.,  4097.,  5436.,  4996.,  9797., 11067.,  4273.,\n",
      "        4755.,  4309.,  5292.,  5527., 10199., 10954.,  3479.,  4451.,\n",
      "        4905.,  5493.,  6043.,  9847., 10983.,  4041.,  3903.,  4251.,\n",
      "        5130.,  5708.,  9731., 10198.,  4022.,  4613.,  7180.,  7980.,\n",
      "        6305., 11347., 10035.,  4178.,  4212.,  5903.,  8454.,  6889.,\n",
      "       10123.,  9761.,  4338.,   764.]), 'n_features': 2, 'n_sequences': 1177}, 'Other/Uncategorized': {'X_train_seq': array([[[ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        ...,\n",
      "        [ 2.        , -0.23197458],\n",
      "        [ 2.        , -0.40776226],\n",
      "        [ 2.        , -0.5483924 ]],\n",
      "\n",
      "       [[ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.51323487],\n",
      "        ...,\n",
      "        [ 2.        , -0.40776226],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.5483924 ]],\n",
      "\n",
      "       [[ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        ...,\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        [ 2.        , -0.5483924 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        ,  9.92855337],\n",
      "        [ 9.        , 16.71395784],\n",
      "        [ 9.        , 14.32324538],\n",
      "        ...,\n",
      "        [ 9.        , 13.02241654],\n",
      "        [ 9.        , 12.84662886],\n",
      "        [ 9.        , 14.85060842]],\n",
      "\n",
      "       [[ 9.        , 16.71395784],\n",
      "        [ 9.        , 14.32324538],\n",
      "        [ 9.        ,  9.43634786],\n",
      "        ...,\n",
      "        [ 9.        , 12.84662886],\n",
      "        [ 9.        , 14.85060842],\n",
      "        [ 9.        ,  6.97532033]],\n",
      "\n",
      "       [[ 9.        , 14.32324538],\n",
      "        [ 9.        ,  9.43634786],\n",
      "        [ 9.        ,  9.75276568],\n",
      "        ...,\n",
      "        [ 9.        , 14.85060842],\n",
      "        [ 9.        ,  6.97532033],\n",
      "        [ 9.        ,  8.06520395]]]), 'X_test_seq': array([[[  9.        ,   9.43634786],\n",
      "        [  9.        ,   9.75276568],\n",
      "        [  9.        ,   8.87382728],\n",
      "        ...,\n",
      "        [  9.        ,   6.97532033],\n",
      "        [  9.        ,   8.06520395],\n",
      "        [  9.        ,   9.19024511]],\n",
      "\n",
      "       [[  9.        ,   9.75276568],\n",
      "        [  9.        ,   8.87382728],\n",
      "        [  9.        ,   9.40119032],\n",
      "        ...,\n",
      "        [  9.        ,   8.06520395],\n",
      "        [  9.        ,   9.19024511],\n",
      "        [  9.        ,   7.85425873]],\n",
      "\n",
      "       [[  9.        ,   8.87382728],\n",
      "        [  9.        ,   9.40119032],\n",
      "        [  9.        ,  13.02241654],\n",
      "        ...,\n",
      "        [  9.        ,   9.19024511],\n",
      "        [  9.        ,   7.85425873],\n",
      "        [  9.        ,  11.79190278]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[  5.        , 509.27103905],\n",
      "        [  5.        , 449.85480299],\n",
      "        [  5.        , 655.03418386],\n",
      "        ...,\n",
      "        [  5.        , 502.90752501],\n",
      "        [  5.        , 435.68631593],\n",
      "        [  5.        , 584.89489927]],\n",
      "\n",
      "       [[  5.        , 449.85480299],\n",
      "        [  5.        , 655.03418386],\n",
      "        [  5.        , 549.31547271],\n",
      "        ...,\n",
      "        [  5.        , 435.68631593],\n",
      "        [  5.        , 584.89489927],\n",
      "        [  5.        , 560.03852123]],\n",
      "\n",
      "       [[  5.        , 655.03418386],\n",
      "        [  5.        , 549.31547271],\n",
      "        [  5.        , 277.72350608],\n",
      "        ...,\n",
      "        [  5.        , 584.89489927],\n",
      "        [  5.        , 560.03852123],\n",
      "        [  5.        , 332.35831723]]]), 'y_train_seq': array([  1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   4.,   4.,   3.,\n",
      "         4.,   6.,   6.,   1.,   1.,   7.,   7.,   2.,   2.,   2.,   1.,\n",
      "         4.,   7.,   3.,   1.,   9.,   2.,   6.,   2.,  19.,   7.,   7.,\n",
      "         2.,   3.,   3.,   7.,   4.,  11.,  11.,   5.,   5.,   8.,   1.,\n",
      "         2.,  17.,   6.,   1.,   4.,   3.,   2.,  11.,   3.,  10.,   8.,\n",
      "         3.,   3.,   2.,   1.,   5.,   5.,   7.,   7.,   1.,   2.,   4.,\n",
      "         3.,   3.,   3.,   3.,   2.,   1.,   1.,   1.,   2.,   2.,   2.,\n",
      "         2.,   2.,   4.,   2.,   2.,   2.,   1.,   1.,   1.,   1.,  11.,\n",
      "        11.,   6.,  13.,   5.,   9.,   8.,   8.,   8.,   2.,   1.,   1.,\n",
      "        10.,  14.,   1.,  17.,   8.,   6.,   4.,   7.,   2.,  10.,   7.,\n",
      "         6.,   2.,   4.,   4.,   4.,   7.,   7.,   4.,   2.,   2.,   2.,\n",
      "         1.,   3.,   5.,   5.,   5.,   5.,   2.,   2.,   3.,   8.,   8.,\n",
      "         1.,   1.,   1.,   2.,   2.,   3.,   3.,   4.,   6.,   6.,   6.,\n",
      "         4.,  16.,  15.,   9.,   2.,   2.,  16.,  13.,  12.,  10.,   3.,\n",
      "         5.,   5.,   3.,  13.,  18.,   4.,   4.,   4.,   1.,   7.,   2.,\n",
      "         5.,   2.,   3.,   2.,   2.,   1.,   4.,   8.,   3.,   4.,   6.,\n",
      "         2.,  11.,  13.,  15.,   1.,   2.,  12.,   5.,   4.,   6.,   5.,\n",
      "         4.,   4.,   4.,   8.,   5.,   4.,  13.,   2.,  11.,   9.,   9.,\n",
      "         9.,   9.,   9.,   9.,   4.,   4.,   4.,   4.,   1.,   1.,   1.,\n",
      "         1.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   3.,   1.,   1.,\n",
      "         1.,   3.,   2.,   1.,   3.,   2.,   2.,   4.,  10.,  16.,   4.,\n",
      "         2.,   5.,  17.,  17.,   2.,  13.,  10.,   2.,   2.,   1.,   1.,\n",
      "         1.,   5.,   1.,   1.,   3.,   1.,   1.,   6.,   6.,   6.,   6.,\n",
      "         1.,   1.,  10.,  10.,  10.,  10.,   3.,   3.,   3.,   2.,   2.,\n",
      "        10.,  10.,   1.,   1.,   1.,   8.,   8.,   8.,   8.,   5.,   5.,\n",
      "         3.,   3.,   3.,   9.,   4.,   1.,   3.,   3.,   3.,   9.,   4.,\n",
      "         1.,   1.,   1.,   4.,   4.,   2.,   2.,   2.,   2.,   2.,   7.,\n",
      "         3.,   3.,   3.,   3.,  10.,  10.,  10.,  10.,  10.,  10.,  10.,\n",
      "        10.,  10.,  10.,  10.,  10.,  10.,  10.,  10.,   2.,   2.,   2.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   3.,   3.,   3.,\n",
      "         3.,   3.,   2.,   2.,   1.,   1.,   2.,   2.,   2.,   4.,   4.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   3.,   3.,\n",
      "         3.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   4.,   3.,   3.,\n",
      "         3.,   1.,   1.,   1.,   1.,   1.,   4.,   4.,   9.,   9.,  24.,\n",
      "        22.,  43.,  34.,  17.,  36.,  29.,  46.,  43.,  46.,  65.,   8.,\n",
      "        22.,  22.,  20.,  16.,  16.,  16.,   6.,  19.,  29.,  18.,  22.,\n",
      "        49.,  51.,  20.,  29.,  26.,   9.,  28.,  42.,  25.,  11.,  15.,\n",
      "        24.,  74.,  26.,  30.,  50.,  36.,  23.,  16., 123.,  19.,  12.,\n",
      "        26.,  11.,   7.,  34.,  11.,  24.,  33.,  15., 100.,  24.,  18.,\n",
      "       154., 112., 126.,  50.,  13.,  14.,   9.,  12.,  12.,  16.,  22.,\n",
      "         5.,   3.,  21.,  12.,  32.,  32.,  26.,  20.,   8.,  41.,  42.,\n",
      "        63.,  67.,  96.,  38.,  48.,  64.,  54.,  71.,  87.,  79.,  54.,\n",
      "        44.,  61.,  42.,  65.,  64.,  59.,  53.,  53.,  45.,  44.,  43.,\n",
      "        40.,  54.,  62.,  62.,  49.,  67.,  62.,  66.,  70.,  69.,  46.,\n",
      "        31.,  54.,  37.,  82.,  43.,  44.,  63.,  33.,  23.,  43., 171.,\n",
      "       180., 115., 128., 145., 135., 139., 191., 138., 108., 184., 174.,\n",
      "       202., 150., 171., 201., 121., 144., 156., 176., 197., 262., 165.,\n",
      "       135., 190., 196., 209., 258., 287., 232., 174., 190., 147., 209.,\n",
      "       229., 214., 238., 144., 173., 263., 216., 233., 292., 254., 156.,\n",
      "       150., 181., 181., 176., 261., 288., 155., 227., 212., 196., 187.,\n",
      "       309., 278., 180., 141., 148., 232., 226., 252., 256., 161., 137.,\n",
      "       141., 177., 144., 233., 268., 164., 144., 124., 142., 136., 216.,\n",
      "       264., 180., 110., 123., 149., 144., 281., 228., 110.,  88., 112.,\n",
      "       150., 114., 193., 151., 118., 154., 139., 131., 111., 202., 195.,\n",
      "       176.,  87.,  67.,  91.,  73., 261., 213., 131., 126., 113., 135.,\n",
      "       127., 130., 164.,  88.,  96.,  96., 128., 142., 219., 241.,  92.,\n",
      "        89.,  89.,  93.,  85., 117., 113.,  66.,  69.,  95.,  61., 101.,\n",
      "        78., 116.,  67., 116.,  79.,  50.,  63.,  94.,  79.,  65.,  50.,\n",
      "        55.,  53.,  60.,  82., 102.,  68.,  61.,  52.,  45.,  50.,  77.,\n",
      "       125.,  58.,  44.,  75.,  64.,  59., 145., 114.,  49.,  62.,  79.,\n",
      "        61.,  69.,  78.,  70.,  43.,  91.,  66.,  62.,  67.,  47.,  13.,\n",
      "        19.,  19.,  36.,  62.,  84., 100.,  65.,  55.,  75.,  53.,  91.,\n",
      "        35., 117., 105.,  60.,  75.,  57.,  67.,  46., 144.,  99.,  54.,\n",
      "        59.,  67.,  84.,  52.,  66., 153.,  80.,  75.,  68.,  65.,  64.,\n",
      "       102., 115.,  76.,  52.,  68.,  73.,  43.,  96., 120.,  69.,  61.,\n",
      "        55.,  71.,  44.,  63.,  77.,  61.,  59.,  79.,  56.,  41.,  72.,\n",
      "        92.,  70.,  52.,  54.,  52.,  56.,  81., 106.,  60.,  66.,  56.,\n",
      "        69.,  75.,  90.,  92.,  66.,  58.,  61.,  51.,  35.,  85., 105.,\n",
      "        59.,  51.,  51.,  63.,  58.,  68.,  76.,  49.,  59.,  37.,  70.,\n",
      "        32.,  71.,  63.,  66.,  43.,  57.,  67.,  56.,  95.,  85.,  49.,\n",
      "        49.,  53.,  40.,  46.,  58.,  57.,  58.,  51.,  50.,  55.,  38.,\n",
      "        55.,  52.,  66.,  70.,  36.,  51.,  35.,  37.,  59.,  65.,  55.,\n",
      "        74.,  95.,  63.,  72.,  64.,  84.,  51.,  60.,  31.,  54.,  66.,\n",
      "        76.,  37.,  66.,  53.,  41.,  35.,  56.,  43.,  51.,  47.,  56.,\n",
      "        63.,  46.,  44.,  81.,  45.,  89.,  58., 121., 107., 102., 137.,\n",
      "        48., 187., 138., 228., 179., 194., 222.,  33., 152., 115., 143.,\n",
      "       112., 166., 103.,  58.,  76.,  86.,  85.,  75., 121., 155.,  40.,\n",
      "       105., 101.,  70., 122.,  76., 188.,  32.,  66.,  95., 117., 135.,\n",
      "       156.,  92.,  83., 117., 116.,  80.,  90., 107.,  60.,  39.,  81.,\n",
      "        90.,  58.,  85., 134.,  90.,  39.,  85.,  98.,  82., 187.,  98.,\n",
      "        51.,  72., 143., 106.,  75.,  84., 145.,  75.,  83., 113., 157.,\n",
      "       118., 131., 117.,  95.,  67., 126., 125., 127.,  98., 257., 169.,\n",
      "        48., 142., 146., 113., 193., 323., 355., 273., 205., 259., 286.,\n",
      "       288., 439., 387., 286., 269., 240., 255., 330., 468., 478., 336.,\n",
      "       249., 263., 196., 276., 299., 492., 424., 285., 294., 269., 284.,\n",
      "       387., 382., 439., 215., 246., 278.]), 'y_test_seq': array([  240.,   352.,   338.,   339.,   319.,   308.,   290.,   302.,\n",
      "         459.,   398.,   416.,   268.,   305.,   353.,   402.,   523.,\n",
      "         453.,   511.,   317.,   296.,   338.,   300.,   397.,   464.,\n",
      "         403.,   274.,   376.,   366.,   330.,   436.,   345.,   548.,\n",
      "         361.,   397.,   389.,   430.,   508.,   444.,   778.,   462.,\n",
      "         455.,   489.,   474.,   533.,   429.,   580.,   334.,   353.,\n",
      "         414.,   474.,   502.,   436.,   434.,   322.,   388.,   562.,\n",
      "         509.,   494.,   786.,   490.,   378.,   513.,   585.,   636.,\n",
      "         720.,   677.,   681.,   542.,   616.,   859.,   726.,  1038.,\n",
      "        1264.,  1077.,   732.,   620.,   786.,   980.,  1318.,  1194.,\n",
      "         995.,   598.,   867.,   958.,  1271.,  1410.,  1157.,   800.,\n",
      "         842.,   793.,  1000.,   925.,  1388.,   860.,   241.,   616.,\n",
      "         678.,  1210.,  1352.,  2315.,  1679.,   810.,  1891.,  2767.,\n",
      "        2781.,  3690.,  6281.,  6111.,  3219.,  3649.,  4286.,  4965.,\n",
      "        5244.,  8660.,  7248.,  4752.,  4356.,  4957.,  4983.,  6133.,\n",
      "        9519.,  8921.,  4275.,  4702.,  5746.,  5851.,  7049.,  9701.,\n",
      "        9147.,  5863.,  5396.,  6056.,  8286.,  7478., 12598., 10985.,\n",
      "        6098.,  5857.,  6435.,  7572.,  8130., 10834.,  9975.,  6009.,\n",
      "        5606.,  7020.,  8061.,  8046., 11735., 11496.,  4781.,  6078.,\n",
      "        7506.,  8091.,  8188., 12426., 11988.,  6081.,  6132.,  6918.,\n",
      "        7674.,  9581., 14394., 13128.,  7379.,  7490.,  7924.,  8603.,\n",
      "        9460., 15818., 13123.,  7064.,  7063.,  7107.,  8384.,  8609.,\n",
      "       14708., 13861.,  8324.,  7273.,  7793.,  9393.,  9498., 14852.,\n",
      "       14597.,  7969.,  8924.,  9479., 14461., 10576., 10575., 11910.,\n",
      "        7731.,  6031.,  7260.,  9129.,  8961., 15168., 15579.,  7663.,\n",
      "        8271.,  7950.,  9701.,  9793., 16446., 16403.,  7271.,  7488.,\n",
      "        8287., 10016., 10078., 15786., 16102.,  7857.,  7093.,  7719.,\n",
      "        8963., 10221., 15675., 15386.,  8749.,  8393., 12579., 14502.,\n",
      "       12812., 18648., 15641.,  7916.,  8398., 10054., 14321., 12409.,\n",
      "       16653., 15946.,  9470.,  1466.]), 'n_features': 2, 'n_sequences': 1177}, 'Sides & Snacks': {'X_train_seq': array([[[ 2.        , 50.95739803],\n",
      "        [ 2.        , 50.95739803],\n",
      "        [ 2.        , 50.95739803],\n",
      "        ...,\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979]],\n",
      "\n",
      "       [[ 2.        , 50.95739803],\n",
      "        [ 2.        , 50.95739803],\n",
      "        [ 2.        , 50.95739803],\n",
      "        ...,\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979]],\n",
      "\n",
      "       [[ 2.        , 50.95739803],\n",
      "        [ 2.        , 50.95739803],\n",
      "        [ 2.        , -0.47807733],\n",
      "        ...,\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        , -0.51323487],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.40776226],\n",
      "        ...,\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.1616595 ],\n",
      "        [ 9.        , -0.33744719]],\n",
      "\n",
      "       [[ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.44291979],\n",
      "        ...,\n",
      "        [ 9.        , -0.1616595 ],\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.51323487]],\n",
      "\n",
      "       [[ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.51323487],\n",
      "        ...,\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.51323487],\n",
      "        [ 9.        , -0.51323487]]]), 'X_test_seq': array([[[ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.51323487],\n",
      "        [ 9.        , -0.5483924 ],\n",
      "        ...,\n",
      "        [ 9.        , -0.51323487],\n",
      "        [ 9.        , -0.51323487],\n",
      "        [ 9.        , -0.26713211]],\n",
      "\n",
      "       [[ 9.        , -0.51323487],\n",
      "        [ 9.        , -0.5483924 ],\n",
      "        [ 9.        , -0.51323487],\n",
      "        ...,\n",
      "        [ 9.        , -0.51323487],\n",
      "        [ 9.        , -0.26713211],\n",
      "        [ 9.        , -0.51323487]],\n",
      "\n",
      "       [[ 9.        , -0.5483924 ],\n",
      "        [ 9.        , -0.51323487],\n",
      "        [ 9.        , -0.30228965],\n",
      "        ...,\n",
      "        [ 9.        , -0.26713211],\n",
      "        [ 9.        , -0.51323487],\n",
      "        [ 9.        ,  0.15475832]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 5.        , 15.7998619 ],\n",
      "        [ 5.        , 12.53021104],\n",
      "        [ 5.        , 22.26884855],\n",
      "        ...,\n",
      "        [ 5.        , 15.06155364],\n",
      "        [ 5.        , 19.70234841],\n",
      "        [ 5.        , 18.61246479]],\n",
      "\n",
      "       [[ 5.        , 12.53021104],\n",
      "        [ 5.        , 22.26884855],\n",
      "        [ 5.        , 21.53054029],\n",
      "        ...,\n",
      "        [ 5.        , 19.70234841],\n",
      "        [ 5.        , 18.61246479],\n",
      "        [ 5.        , 16.64364277]],\n",
      "\n",
      "       [[ 5.        , 22.26884855],\n",
      "        [ 5.        , 21.53054029],\n",
      "        [ 5.        , 13.40914944],\n",
      "        ...,\n",
      "        [ 5.        , 18.61246479],\n",
      "        [ 5.        , 16.64364277],\n",
      "        [ 5.        , 13.54977959]]]), 'y_train_seq': array([ 4.,  4.,  4.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        3.,  3.,  3.,  3.,  3.,  3.,  3.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  2.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  2.,  2.,  2.,  2.,  3.,  3.,  3.,  2.,  3.,  3.,  3.,\n",
      "        5.,  5.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  2.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  4.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
      "        3.,  3.,  3.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  1.,  1.,  3.,  2.,  4.,  6.,  6.,  6.,  6.,\n",
      "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
      "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
      "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
      "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  1.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  1.,  1.,  3.,  3.,  6.,  3.,  1.,  5.,  1.,\n",
      "        1.,  9.,  6.,  3.,  9.,  2.,  2., 10.,  7.,  6.,  2.,  3.,  5.,\n",
      "        4., 10.,  6., 13., 11.,  8.,  4.,  4.,  4.,  8.,  7., 14.,  8.,\n",
      "        8.,  5.,  6.,  6., 10.,  3., 13.,  8.,  6., 11., 11., 11.,  6.,\n",
      "       10.,  4.,  6.,  6.,  8., 14.,  5.,  2.,  5.,  5.,  5.,  8., 12.,\n",
      "        5.,  8.,  8.,  6.,  1.,  8., 13.,  1.,  2.,  7.,  4.,  5.,  4.,\n",
      "        4.,  2.,  4.,  3.,  2.,  3.,  8.,  5.,  8.,  6.,  6.,  6.,  6.,\n",
      "        1., 14.,  8.,  4.,  3.,  1.,  6.,  2.,  2.,  1.,  1.,  4.,  4.,\n",
      "        4.,  1.,  4.,  7., 10., 10.,  9.,  1.,  6.,  1.,  4.,  4.,  4.,\n",
      "        4.,  4.,  2.,  3.,  7.,  3.,  3.,  3.,  3.,  4.,  3.,  1.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  3.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
      "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  1.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  1.,  3.,  5.,  1.,  9., 13.,  8., 10.,  6.,  7.,\n",
      "        3.,  8.,  7.,  9.,  9.,  9., 10.,  2.,  2.,  7.,  3.,  3.,  4.,\n",
      "        7.,  2.,  3.,  4.,  5.,  2.,  5.,  2.,  3.,  1.,  4., 11.,  8.,\n",
      "        5., 13., 11., 11.,  1.,  3., 10., 12.,  3.,  7.,  4.,  2.,  9.,\n",
      "        3.,  3.,  6.,  1.,  3.,  2.,  4.,  7.,  7.,  4., 10.,  3.,  1.,\n",
      "        3.,  4.,  7.,  5.,  7.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
      "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
      "        6.,  6.,  6.,  3., 15., 15., 17.,  2.,  1.,  1.,  6.,  5.,  7.,\n",
      "        2.,  4.,  1.,  1.,  1.,  2.,  4.,  5.,  4.,  2.,  1.,  2.,  8.,\n",
      "       12.,  7.,  2.,  2.,  9.]), 'y_test_seq': array([  2.,  21.,  17.,  17.,   7.,   4.,   4.,   5.,  14.,  18.,  26.,\n",
      "        12.,   9.,  21.,  15.,  26.,  22.,  32.,   9.,  17.,  18.,  22.,\n",
      "        27.,  20.,  26.,  19.,  16.,  18.,  18.,  19.,  32.,  17.,  21.,\n",
      "        30.,  26.,  25.,  28.,  25.,  95.,  17.,  14.,  28.,  42.,  34.,\n",
      "        41.,  45.,  20.,  19.,  31.,  37.,  27.,  23.,  28.,  18.,  36.,\n",
      "        45.,  28.,  17.,  46.,  21.,  19.,  50.,  39.,  29.,  33.,  43.,\n",
      "        38.,  16.,  25.,  33.,  18.,  38.,  47.,  54.,  24.,  34.,  37.,\n",
      "        36.,  51.,  39.,  64.,  20.,  74.,  48., 114.,  43.,  75., 120.,\n",
      "        28.,  37.,  28.,  49.,  39.,  15.,   4.,   2.,   6., 202., 105.,\n",
      "       177.,  74.,   7., 191.,  67.,  77., 100., 235., 391., 200., 120.,\n",
      "       145., 209., 220., 326., 519., 301., 138., 227., 218., 232., 457.,\n",
      "       380., 348., 159., 203., 219., 251., 384., 385., 460., 171., 237.,\n",
      "       280., 257., 446., 455., 358., 174., 276., 261., 227., 445., 360.,\n",
      "       380., 224., 299., 324., 332., 408., 427., 298., 216., 473., 301.,\n",
      "       288., 438., 492., 381., 233., 267., 273., 378., 513., 551., 467.,\n",
      "       237., 313., 295., 326., 513., 599., 390., 238., 225., 303., 308.,\n",
      "       610., 755., 630., 220., 262., 337., 439., 497., 742., 584., 302.,\n",
      "       418., 471., 413., 401., 429., 384., 348., 219., 353., 368., 519.,\n",
      "       645., 442., 248., 269., 356., 363., 563., 592., 435., 233., 316.,\n",
      "       351., 341., 520., 655., 399., 301., 325., 338., 483., 571., 547.,\n",
      "       502., 282., 376., 466., 373., 650., 629., 398., 267., 347., 445.,\n",
      "       577., 546., 490., 402.,   4.]), 'n_features': 2, 'n_sequences': 1177}, 'Desserts & Sweets': {'X_train_seq': array([[[ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979],\n",
      "        ...,\n",
      "        [ 2.        , -0.40776226],\n",
      "        [ 2.        , -0.47807733],\n",
      "        [ 2.        , -0.47807733]],\n",
      "\n",
      "       [[ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979],\n",
      "        ...,\n",
      "        [ 2.        , -0.47807733],\n",
      "        [ 2.        , -0.47807733],\n",
      "        [ 2.        , -0.47807733]],\n",
      "\n",
      "       [[ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.44291979],\n",
      "        [ 2.        , -0.5483924 ],\n",
      "        ...,\n",
      "        [ 2.        , -0.47807733],\n",
      "        [ 2.        , -0.47807733],\n",
      "        [ 2.        , -0.47807733]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        ,  0.75243643],\n",
      "        [ 9.        ,  1.24464194],\n",
      "        [ 9.        ,  0.47117615],\n",
      "        ...,\n",
      "        [ 9.        ,  1.1040118 ],\n",
      "        [ 9.        ,  1.03369672],\n",
      "        [ 9.        ,  1.70168991]],\n",
      "\n",
      "       [[ 9.        ,  1.24464194],\n",
      "        [ 9.        ,  0.47117615],\n",
      "        [ 9.        ,  0.40086107],\n",
      "        ...,\n",
      "        [ 9.        ,  1.03369672],\n",
      "        [ 9.        ,  1.70168991],\n",
      "        [ 9.        ,  0.82275151]],\n",
      "\n",
      "       [[ 9.        ,  0.47117615],\n",
      "        [ 9.        ,  0.40086107],\n",
      "        [ 9.        ,  0.92822412],\n",
      "        ...,\n",
      "        [ 9.        ,  1.70168991],\n",
      "        [ 9.        ,  0.82275151],\n",
      "        [ 9.        ,  0.78759397]]]), 'X_test_seq': array([[[ 9.        ,  0.40086107],\n",
      "        [ 9.        ,  0.92822412],\n",
      "        [ 9.        ,  0.15475832],\n",
      "        ...,\n",
      "        [ 9.        ,  0.82275151],\n",
      "        [ 9.        ,  0.78759397],\n",
      "        [ 9.        ,  0.7172789 ]],\n",
      "\n",
      "       [[ 9.        ,  0.92822412],\n",
      "        [ 9.        ,  0.15475832],\n",
      "        [ 9.        ,  0.7172789 ],\n",
      "        ...,\n",
      "        [ 9.        ,  0.78759397],\n",
      "        [ 9.        ,  0.7172789 ],\n",
      "        [ 9.        ,  0.29538846]],\n",
      "\n",
      "       [[ 9.        ,  0.15475832],\n",
      "        [ 9.        ,  0.7172789 ],\n",
      "        [ 9.        ,  1.1040118 ],\n",
      "        ...,\n",
      "        [ 9.        ,  0.7172789 ],\n",
      "        [ 9.        ,  0.29538846],\n",
      "        [ 9.        ,  1.17432687]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 5.        , 36.50765068],\n",
      "        [ 5.        , 35.62871228],\n",
      "        [ 5.        , 44.13683602],\n",
      "        ...,\n",
      "        [ 5.        , 36.78891097],\n",
      "        [ 5.        , 36.47249314],\n",
      "        [ 5.        , 58.30532308]],\n",
      "\n",
      "       [[ 5.        , 35.62871228],\n",
      "        [ 5.        , 44.13683602],\n",
      "        [ 5.        , 39.07415082],\n",
      "        ...,\n",
      "        [ 5.        , 36.47249314],\n",
      "        [ 5.        , 58.30532308],\n",
      "        [ 5.        , 53.9457886 ]],\n",
      "\n",
      "       [[ 5.        , 44.13683602],\n",
      "        [ 5.        , 39.07415082],\n",
      "        [ 5.        , 23.04231434],\n",
      "        ...,\n",
      "        [ 5.        , 58.30532308],\n",
      "        [ 5.        , 53.9457886 ],\n",
      "        [ 5.        , 30.81212983]]]), 'y_train_seq': array([ 3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  2.,  2.,  2.,  2., 12.,\n",
      "       12., 12., 12.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  1.,  1.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  2.,  3.,  1.,  1.,  1.,  1.,  4.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  5.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  3.,  1.,  1.,  3.,  3.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  2.,  1.,  5.,  1.,  1.,  1.,  1.,  1.,  1.,  7.,  7.,  7.,\n",
      "        1.,  2.,  1.,  5.,  3.,  1.,  2.,  2.,  2.,  8.,  2.,  2.,  1.,\n",
      "        3.,  1.,  1.,  5.,  5.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  4.,  4.,  4.,\n",
      "        4.,  4.,  1.,  1.,  1.,  1.,  2.,  1.,  1.,  1.,  3.,  1.,  1.,\n",
      "        1.,  1.,  2.,  4.,  4.,  4.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,\n",
      "        2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  4.,  4.,  4.,  4.,  4.,  4.,  3.,  3.,\n",
      "        2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  7., 20., 10., 14.,\n",
      "        8., 11.,  3.,  6.,  8., 15.,  8.,  3.,  7.,  7.,  8.,  5.,  5.,\n",
      "        5.,  1.,  4.,  2., 13.,  4., 13.,  9.,  6., 10.,  6.,  4.,  4.,\n",
      "        5., 10., 10.,  5., 11., 14.,  5.,  9., 14.,  7.,  9.,  8., 25.,\n",
      "        8.,  9.,  7.,  6.,  4.,  4.,  1.,  6., 21.,  9., 12.,  8., 10.,\n",
      "       16.,  5., 14., 10.,  4., 11.,  5.,  2.,  3.,  5.,  7.,  7.,  7.,\n",
      "        5.,  8.,  7., 13.,  8.,  5.,  5.,  9., 15., 14., 24., 22., 11.,\n",
      "        7., 17., 13., 25., 25., 26., 25., 15., 16., 15., 29., 16., 14.,\n",
      "       12., 13., 17., 17., 10., 14.,  9., 18., 19., 11., 17., 20., 15.,\n",
      "       15., 23., 15., 14., 17., 16., 14., 19., 14.,  9., 12., 19., 12.,\n",
      "       28., 14., 15., 29., 21., 14., 18., 11., 23., 17., 17., 22., 19.,\n",
      "       23., 20., 23., 30.,  8.,  6., 18., 18., 31., 39., 23., 13., 24.,\n",
      "       30., 28., 14., 19., 24., 24., 23.,  6., 12., 19., 21., 13., 14.,\n",
      "       32., 17., 16., 16., 32., 11., 11., 27., 22., 15., 16., 40., 29.,\n",
      "       22., 16., 21., 26., 28., 32., 24., 14., 18., 14., 19., 28., 23.,\n",
      "       21., 18., 14., 21., 15., 26., 30., 21., 20., 19., 19., 18., 22.,\n",
      "       30., 27., 16., 23., 16., 19., 36., 22., 17., 17., 21., 24., 27.,\n",
      "       31., 17., 17., 27., 21., 17., 22., 32., 31., 19., 17., 18., 12.,\n",
      "       13., 29., 22., 10., 10., 23., 23., 34., 27., 23., 21., 13., 26.,\n",
      "       20., 25., 20., 23., 38., 23., 15., 38., 26., 39., 26., 16., 27.,\n",
      "       14., 21., 21., 26., 38., 17., 12., 39., 20., 14., 21., 37., 20.,\n",
      "       17., 18., 25., 17., 27., 29., 32., 20., 18., 17., 13., 21., 25.,\n",
      "       12., 10., 19., 17.,  9., 24., 27., 17., 18., 25., 12., 21., 31.,\n",
      "       29., 12., 19., 24., 11., 17., 11.,  2., 14., 11., 15., 18., 26.,\n",
      "       30., 19., 24., 15., 19., 26., 17., 30., 44., 17., 25., 12., 15.,\n",
      "       19., 41., 33., 24., 21., 23., 22., 18., 18., 46., 20., 23., 23.,\n",
      "       23., 19., 23., 41., 41., 12., 21., 21., 22., 26., 41., 21., 14.,\n",
      "       17., 25., 24., 13., 23.,  9., 13., 19., 26., 18., 23., 37., 18.,\n",
      "       14., 15., 28., 18., 22., 34., 14., 19., 15.,  9., 20., 31., 36.,\n",
      "       18., 14., 17., 18., 20., 34., 36., 26., 13., 13., 14., 18., 21.,\n",
      "       19., 16., 12.,  9., 14., 17., 11., 21., 15., 15., 10., 21., 19.,\n",
      "       19., 33., 19., 19., 19., 12., 11., 20., 29., 15., 16., 12., 11.,\n",
      "       14., 28., 12., 11., 10., 13., 10., 16.,  9.,  8., 19., 15., 17.,\n",
      "       27., 22., 34., 26., 16., 21., 22., 19., 14.,  9., 20., 15., 13.,\n",
      "       12., 13., 13., 15., 19., 16., 13., 11., 10., 11.,  9., 16., 11.,\n",
      "       13.,  6., 11.,  9., 14., 14., 13., 28., 12., 14.,  7., 19., 11.,\n",
      "       14., 24., 13.,  8.,  7., 13., 32., 15., 10., 14., 22., 12.,  7.,\n",
      "       15.,  9., 10., 18.,  5., 15., 11., 25.,  4., 22., 17.,  5., 25.,\n",
      "       14., 11., 13., 16., 33., 27., 20., 25., 21.,  8., 12.,  9.,  9.,\n",
      "       14., 16., 16., 15., 17., 20., 14.,  8.,  9., 15., 13., 16., 16.,\n",
      "       21., 33., 19., 23., 15., 32., 21., 19., 21., 23., 24., 12., 15.,\n",
      "       28., 16., 24., 20., 31.,  6., 15., 27., 20., 29., 35., 42., 38.,\n",
      "       42., 22., 37., 23., 33., 37., 54., 28., 44., 42., 51., 45., 53.,\n",
      "       43., 24., 43., 33., 27., 38., 52., 30., 28., 43., 21., 37., 48.,\n",
      "       46., 65., 40., 39., 37.]), 'y_test_seq': array([  25.,   50.,   86.,   55.,   41.,   33.,   32.,   24.,   53.,\n",
      "         62.,   70.,   40.,   63.,   48.,   69.,   87.,   73.,   74.,\n",
      "         41.,   64.,   50.,   57.,   52.,  113.,   70.,   45.,   50.,\n",
      "         43.,   47.,   46.,   58.,   52.,   55.,   40.,   46.,   63.,\n",
      "         71.,   66.,   66.,   62.,   56.,   69.,   60.,   65.,   69.,\n",
      "         86.,   74.,   40.,   76.,   57.,   37.,   53.,   48.,   48.,\n",
      "         47.,   88.,   53.,   58.,  112.,   52.,   63.,   64.,   59.,\n",
      "         93.,   80.,  143.,   93.,   74.,   68.,  101.,   72.,   97.,\n",
      "        124.,   96.,   95.,   90.,   88.,   97.,  122.,  154.,   88.,\n",
      "         71.,   95.,   97.,   85.,  114.,  107.,   49.,  170.,   84.,\n",
      "        103.,  117.,  115.,   38.,   14.,   28.,   28.,   72.,   97.,\n",
      "        138.,   86.,   35.,  303.,  614.,  430.,  584.,  646.,  602.,\n",
      "        335.,  418.,  489.,  421.,  509.,  651.,  655.,  456.,  404.,\n",
      "        428.,  355.,  536.,  726.,  895.,  500.,  460.,  554.,  570.,\n",
      "        635.,  870.,  951.,  675.,  491.,  560.,  670.,  625.,  871.,\n",
      "        952.,  650.,  578.,  698.,  810.,  763.,  668.,  841.,  440.,\n",
      "        519.,  780.,  867.,  758.,  957.,  937.,  406.,  663.,  721.,\n",
      "        821.,  751., 1067.,  962.,  506.,  596.,  661.,  753.,  801.,\n",
      "       1109., 1173.,  664.,  744.,  787.,  873.,  861., 1250., 1352.,\n",
      "        649.,  650.,  679.,  654.,  762., 1118., 1319.,  830.,  721.,\n",
      "        780.,  732.,  745., 1212., 1280.,  775.,  812.,  969., 1048.,\n",
      "        710.,  769., 1110.,  557.,  433.,  668.,  824.,  831., 1124.,\n",
      "       1355.,  767.,  735.,  773., 1009.,  848., 1366., 1425.,  770.,\n",
      "        603.,  860.,  870.,  886., 1187., 1467.,  754.,  728.,  767.,\n",
      "        853.,  822., 1277., 1176.,  695.,  740.,  948., 1055., 1030.,\n",
      "       1272., 1128.,  672.,  682.,  864., 1063., 1054., 1675., 1551.,\n",
      "        893.,  127.]), 'n_features': 2, 'n_sequences': 1177}, 'Handhelds': {'X_train_seq': array([[[ 2.        ,  3.88145715],\n",
      "        [ 2.        ,  3.88145715],\n",
      "        [ 2.        ,  3.88145715],\n",
      "        ...,\n",
      "        [ 2.        ,  3.88145715],\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.51323487]],\n",
      "\n",
      "       [[ 2.        ,  3.88145715],\n",
      "        [ 2.        ,  3.88145715],\n",
      "        [ 2.        ,  3.88145715],\n",
      "        ...,\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.51323487]],\n",
      "\n",
      "       [[ 2.        ,  3.88145715],\n",
      "        [ 2.        ,  3.88145715],\n",
      "        [ 2.        ,  3.88145715],\n",
      "        ...,\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.51323487],\n",
      "        [ 2.        , -0.51323487]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.1616595 ],\n",
      "        ...,\n",
      "        [ 9.        ,  0.22507339],\n",
      "        [ 9.        ,  0.40086107],\n",
      "        [ 9.        ,  0.64696383]],\n",
      "\n",
      "       [[ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.1616595 ],\n",
      "        [ 9.        , -0.23197458],\n",
      "        ...,\n",
      "        [ 9.        ,  0.40086107],\n",
      "        [ 9.        ,  0.64696383],\n",
      "        [ 9.        , -0.1616595 ]],\n",
      "\n",
      "       [[ 9.        , -0.1616595 ],\n",
      "        [ 9.        , -0.23197458],\n",
      "        [ 9.        ,  0.29538846],\n",
      "        ...,\n",
      "        [ 9.        ,  0.64696383],\n",
      "        [ 9.        , -0.1616595 ],\n",
      "        [ 9.        ,  0.50633368]]]), 'X_test_seq': array([[[ 9.        , -0.23197458],\n",
      "        [ 9.        ,  0.29538846],\n",
      "        [ 9.        , -0.33744719],\n",
      "        ...,\n",
      "        [ 9.        , -0.1616595 ],\n",
      "        [ 9.        ,  0.50633368],\n",
      "        [ 9.        ,  0.57664875]],\n",
      "\n",
      "       [[ 9.        ,  0.29538846],\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        ,  0.36570354],\n",
      "        ...,\n",
      "        [ 9.        ,  0.50633368],\n",
      "        [ 9.        ,  0.57664875],\n",
      "        [ 9.        ,  0.40086107]],\n",
      "\n",
      "       [[ 9.        , -0.33744719],\n",
      "        [ 9.        ,  0.36570354],\n",
      "        [ 9.        ,  0.22507339],\n",
      "        ...,\n",
      "        [ 9.        ,  0.57664875],\n",
      "        [ 9.        ,  0.40086107],\n",
      "        [ 9.        ,  0.92822412]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 5.        , 67.62207016],\n",
      "        [ 5.        , 57.70764497],\n",
      "        [ 5.        , 77.57165288],\n",
      "        ...,\n",
      "        [ 5.        , 68.32522088],\n",
      "        [ 5.        , 47.47680195],\n",
      "        [ 5.        , 55.77398048]],\n",
      "\n",
      "       [[ 5.        , 57.70764497],\n",
      "        [ 5.        , 77.57165288],\n",
      "        [ 5.        , 48.95341847],\n",
      "        ...,\n",
      "        [ 5.        , 47.47680195],\n",
      "        [ 5.        , 55.77398048],\n",
      "        [ 5.        , 47.79321978]],\n",
      "\n",
      "       [[ 5.        , 77.57165288],\n",
      "        [ 5.        , 48.95341847],\n",
      "        [ 5.        , 35.62871228],\n",
      "        ...,\n",
      "        [ 5.        , 55.77398048],\n",
      "        [ 5.        , 47.79321978],\n",
      "        [ 5.        , 37.35143155]]]), 'y_train_seq': array([ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        3.,  2.,  2.,  2.,  2.,  2.,  3.,  2.,  3.,  3.,  3.,  2.,  2.,\n",
      "        4.,  2.,  2.,  2.,  2.,  2.,  2.,  3.,  3.,  2.,  1.,  1.,  3.,\n",
      "        2.,  2.,  2.,  3.,  3.,  3.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,\n",
      "        1.,  3.,  3.,  3.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  2.,  1.,  1., 13.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  4.,  2.,  2.,  2.,  2.,  3.,  3.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  2., 11., 11., 11.,\n",
      "       11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
      "       11., 11., 11.,  5.,  5.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  3.,  3.,  3.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  6.,\n",
      "        6.,  2.,  2.,  2.,  1.,  1.,  3.,  4., 10.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  3.,  3.,\n",
      "        3.,  3.,  3.,  3.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  4.,  7., 25.,  5.,\n",
      "        5., 14.,  5.,  4.,  3.,  7.,  8.,  5.,  6.,  6.,  5., 12., 13.,\n",
      "        6.,  7., 10., 10.,  6.,  7.,  8.,  7.,  8.,  5.,  2.,  4.,  1.,\n",
      "        5., 12., 17., 17.,  4., 11., 13.,  6.,  8.,  4.,  3.,  1.,  7.,\n",
      "       11.,  4.,  5.,  8., 10.,  3.,  8.,  6., 11.,  6.,  6., 11., 10.,\n",
      "        7.,  6.,  6.,  9.,  3.,  8.,  4.,  7.,  7., 12.,  9.,  1.,  8.,\n",
      "        6., 11.,  7.,  1.,  6.,  9.,  3.,  1.,  4.,  7.,  4., 13.,  8.,\n",
      "        3.,  5.,  8.,  1.,  4.,  2.,  2.,  9.,  7.,  9.,  9., 28.,  2.,\n",
      "        8., 10.,  3.,  5.,  5.,  9., 17., 13.,  3.,  1.,  8.,  8.,  2.,\n",
      "        4.,  5.,  8.,  3.,  7.,  5.,  5., 16.,  5.,  5.,  9.,  2.,  4.,\n",
      "       11., 14.,  2.,  6.,  7.,  4.,  5., 11.,  7.,  4.,  4.,  3., 14.,\n",
      "        5., 13.,  4.,  2.,  8.,  3., 10.,  6.,  8.,  6.,  1.,  4.,  2.,\n",
      "        2.,  5.,  7.,  6.,  7.,  1.,  1.,  2.,  8., 11.,  6.,  6., 10.,\n",
      "        3.,  7.,  4.,  7., 15.,  6.,  5.,  9.,  9.,  8., 10.,  8., 12.,\n",
      "        3.,  7., 10.,  2., 13.,  9.,  7.,  5.,  6.,  8.,  8.,  9.,  4.,\n",
      "        2.,  2.,  1.,  8.,  6., 14.,  3.,  7.,  5.,  2.,  1., 11., 12.,\n",
      "        7.,  4.,  4.,  5.,  5.,  4.,  8.,  4.,  2.,  4.,  4.,  5.,  5.,\n",
      "        9.,  4., 12.,  4.,  4., 10.,  3.,  3.,  2.,  5.,  2.,  8.,  8.,\n",
      "       10.,  2.,  8., 18., 10.,  9., 12.,  2., 13.,  1.,  3.,  1.,  1.,\n",
      "        1.,  5.,  8.,  7.,  9.,  8.,  5., 10.,  6., 24.,  5.,  8.,  4.,\n",
      "        6.,  4., 16.,  9., 15., 10., 10.,  4.,  7., 12.,  8.,  5.,  3.,\n",
      "        6.,  3.,  6.,  3., 14.,  6.,  6.,  3., 14., 12., 12., 14.,  4.,\n",
      "        3.,  5., 21., 11.,  4.,  6., 15.,  3.,  8.,  9.,  1., 10., 13.,\n",
      "        6.,  8.,  8.,  2.,  1.,  8., 11.,  7., 12.,  4.,  6., 14.,  2.,\n",
      "        5., 14., 12.,  5., 10., 13.,  3., 16.,  5.,  6., 12.,  7.,  7.,\n",
      "       16., 17., 10.,  8., 10.,  4.,  3., 14., 12.,  3.,  1.,  5.,  8.,\n",
      "        6., 16., 11.,  6.,  4.,  6.,  7.,  1.,  4.,  8.,  4.,  6., 10.,\n",
      "       13., 11.,  5.,  5., 12.,  3.,  6., 15.,  4.,  5.,  9.,  5., 14.,\n",
      "        1.,  9.,  8.,  5., 11.,  1., 10., 13.,  8.,  3., 12.,  5.,  7.,\n",
      "        8.,  4., 11., 18., 19., 23., 10., 21., 22., 19., 18., 25., 22.,\n",
      "       26., 35., 17., 19., 16., 15., 22., 19., 13.,  9., 20., 23., 19.,\n",
      "       15., 26., 19., 14., 20., 12., 17., 18., 17., 13., 21., 22., 21.,\n",
      "       27., 52., 16., 29., 27., 17., 24., 29., 23., 35., 16., 27., 22.,\n",
      "       22., 32., 25., 10., 20., 21., 24., 11., 20., 12., 27., 17., 15.,\n",
      "       19., 16., 24., 19., 21., 28.,  1.,  7.,  8.,  4., 17.,  6.,  6.,\n",
      "        7., 10., 12.,  6.,  9.,  1.,  9.,  5.,  6., 11., 10., 13.,  9.,\n",
      "        3.,  9., 13.,  2., 11., 12.,  7.,  7., 15., 10., 19., 11., 19.,\n",
      "       15.,  9.,  9., 13.,  3.,  7.,  4., 12., 10., 25.,  7., 27., 23.,\n",
      "       28., 35., 12., 31., 33.]), 'y_test_seq': array([  28.,   43.,   42.,   46.,   23.,   30.,   47.,   31.,   37.,\n",
      "         51.,   95.,   71.,   71.,   86.,   56.,  117.,   88.,   65.,\n",
      "         52.,   73.,   66.,   51.,   72.,   47.,   77.,   38.,   78.,\n",
      "         63.,   73.,   75.,   76.,   75.,   70.,   53.,   66.,   61.,\n",
      "         99.,   84.,   76.,   66.,   75.,   97.,   78.,   88.,   77.,\n",
      "         74.,   43.,   74.,   91.,   91.,   76.,   84.,   71.,   76.,\n",
      "         65.,   86.,   78.,   66.,   88.,   69.,   54.,   89.,  103.,\n",
      "        137.,  142.,   67.,   70.,  109.,  134.,  129.,   94.,  165.,\n",
      "        105.,   52.,  102.,  116.,  126.,  118.,  156.,   97.,   73.,\n",
      "        101.,  126.,  130.,  139.,  182.,   83.,   98.,  111.,  170.,\n",
      "        150.,  124.,   95.,   30.,    3.,   15.,   14.,  122.,  129.,\n",
      "        147.,  113.,   24.,  362.,  315.,  312.,  462.,  668.,  653.,\n",
      "        426.,  393.,  562.,  579.,  756.,  887.,  827.,  630.,  471.,\n",
      "        644.,  651.,  809., 1056.,  940.,  604.,  531.,  716.,  750.,\n",
      "        798., 1002.,  871.,  769.,  669.,  721.,  957.,  951., 1325.,\n",
      "        969.,  702.,  920., 1097., 1146., 1262., 1391.,  943.,  785.,\n",
      "        822., 1011., 1028., 1065., 1371., 1148.,  764.,  809., 1143.,\n",
      "       1246., 1249., 1538., 1196.,  867.,  833., 1097., 1263., 1388.,\n",
      "       1625., 1471., 1006., 1083., 1239., 1288., 1309., 1815., 1217.,\n",
      "        838.,  819., 1040., 1284., 1369., 1765., 1460., 1198.,  932.,\n",
      "       1048., 1330., 1432., 1618., 1389.,  984., 1079., 1295., 1569.,\n",
      "        904., 1033., 1015.,  868.,  913., 1053., 1331., 1261., 1660.,\n",
      "       1552., 1012., 1016., 1112., 1425., 1358., 1791., 1499., 1047.,\n",
      "        966., 1266., 1287., 1399., 1742., 1465., 1193.,  960.,  969.,\n",
      "       1347., 1457., 1851., 2145., 1355., 1078., 1513., 1940., 1658.,\n",
      "       2223., 1409., 1030., 1134., 1465., 1960., 1367., 1603., 1376.,\n",
      "       1079.,  121.]), 'n_features': 2, 'n_sequences': 1177}, 'Main Courses': {'X_train_seq': array([[[ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        ...,\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193]],\n",
      "\n",
      "       [[ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        ...,\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193]],\n",
      "\n",
      "       [[ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        ...,\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193],\n",
      "        [ 2.        ,  3.67051193]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        ,  0.11960078],\n",
      "        [ 9.        ,  0.78759397],\n",
      "        [ 9.        ,  0.54149122],\n",
      "        ...,\n",
      "        [ 9.        ,  0.40086107],\n",
      "        [ 9.        ,  0.47117615],\n",
      "        [ 9.        ,  0.04928571]],\n",
      "\n",
      "       [[ 9.        ,  0.78759397],\n",
      "        [ 9.        ,  0.54149122],\n",
      "        [ 9.        ,  0.04928571],\n",
      "        ...,\n",
      "        [ 9.        ,  0.47117615],\n",
      "        [ 9.        ,  0.04928571],\n",
      "        [ 9.        ,  0.08444325]],\n",
      "\n",
      "       [[ 9.        ,  0.54149122],\n",
      "        [ 9.        ,  0.04928571],\n",
      "        [ 9.        , -0.09134443],\n",
      "        ...,\n",
      "        [ 9.        ,  0.04928571],\n",
      "        [ 9.        ,  0.08444325],\n",
      "        [ 9.        ,  0.18991586]]]), 'X_test_seq': array([[[ 9.        ,  0.04928571],\n",
      "        [ 9.        , -0.09134443],\n",
      "        [ 9.        ,  0.08444325],\n",
      "        ...,\n",
      "        [ 9.        ,  0.08444325],\n",
      "        [ 9.        ,  0.18991586],\n",
      "        [ 9.        ,  0.54149122]],\n",
      "\n",
      "       [[ 9.        , -0.09134443],\n",
      "        [ 9.        ,  0.08444325],\n",
      "        [ 9.        ,  0.11960078],\n",
      "        ...,\n",
      "        [ 9.        ,  0.18991586],\n",
      "        [ 9.        ,  0.54149122],\n",
      "        [ 9.        ,  0.08444325]],\n",
      "\n",
      "       [[ 9.        ,  0.08444325],\n",
      "        [ 9.        ,  0.11960078],\n",
      "        [ 9.        ,  0.40086107],\n",
      "        ...,\n",
      "        [ 9.        ,  0.54149122],\n",
      "        [ 9.        ,  0.08444325],\n",
      "        [ 9.        ,  0.26023093]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 5.        , 30.39023939],\n",
      "        [ 5.        , 30.53086954],\n",
      "        [ 5.        , 46.70333616],\n",
      "        ...,\n",
      "        [ 5.        , 32.04264359],\n",
      "        [ 5.        , 36.26154793],\n",
      "        [ 5.        , 35.24197938]],\n",
      "\n",
      "       [[ 5.        , 30.53086954],\n",
      "        [ 5.        , 46.70333616],\n",
      "        [ 5.        , 33.69504779],\n",
      "        ...,\n",
      "        [ 5.        , 36.26154793],\n",
      "        [ 5.        , 35.24197938],\n",
      "        [ 5.        , 30.14413664]],\n",
      "\n",
      "       [[ 5.        , 46.70333616],\n",
      "        [ 5.        , 33.69504779],\n",
      "        [ 5.        , 28.52688998],\n",
      "        ...,\n",
      "        [ 5.        , 35.24197938],\n",
      "        [ 5.        , 30.14413664],\n",
      "        [ 5.        , 31.40980794]]]), 'y_train_seq': array([121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n",
      "       121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n",
      "        13.,   6.,   6.,   6.,   6.,   6.,   6.,   1.,   3.,   3.,   3.,\n",
      "         3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,\n",
      "         3.,   3.,   3.,   3.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   2.,   2.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "         2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   3.,   3.,   3.,\n",
      "         3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   2.,   2.,   2.,\n",
      "         7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,\n",
      "         7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   2.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   3.,\n",
      "         3.,   1.,   3.,   2.,   2.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   2.,   2.,   2.,   2.,   2.,\n",
      "         2.,   2.,   1.,   2.,   2.,   2.,   1.,   1.,   2.,   2.,   2.,\n",
      "         2.,   1.,   3.,   3.,   3.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "         1.,   1.,   1.,   1.,   1.,   1.,   1.,   2.,   2.,   8.,   5.,\n",
      "         4.,   5.,   3.,  11.,  11.,   4.,  11.,  10.,   6.,   5.,   7.,\n",
      "         7.,   3.,   6.,   4.,   8.,  14.,  15.,  10.,  12.,  12.,  10.,\n",
      "         6.,  16.,   5.,   6.,  18.,  14.,   9.,  19.,   8.,   6.,   8.,\n",
      "         9.,   7.,  11.,   6.,   4.,   6.,   9.,   7.,   8.,  12.,   9.,\n",
      "         8.,   3.,  10.,   8.,   4.,  19.,   6.,   5.,   5.,   6.,  15.,\n",
      "         3.,   9.,   7.,   4.,   4.,   8.,  26.,   3.,  24.,  13.,  15.,\n",
      "        20.,  22.,  40.,  15.,  28.,  34.,  32.,  26.,  31.,  30.,  16.,\n",
      "        21.,  20.,  23.,  13.,  35.,  50.,  16.,   9.,  36.,  29.,  36.,\n",
      "        26.,  27.,   6.,  27.,  28.,  27.,  33.,  33.,  24.,  32.,  30.,\n",
      "        33.,  32.,  24.,  27.,  42.,  22.,  20.,  21.,  25.,  14.,  31.,\n",
      "        16.,  31.,  15.,  45.,  22.,  45.,  17.,  32.,  20.,  21.,  33.,\n",
      "        19.,  28.,  22.,  36.,  11.,  20.,  17.,  14.,  20.,  17.,  10.,\n",
      "        17.,  10.,  20.,  22.,  13.,  15.,  16.,  21.,   6.,  10.,  22.,\n",
      "        16.,  32.,  16.,  16.,   7.,  15.,  26.,  29.,  33.,  22.,   3.,\n",
      "         3.,   2.,  13.,  21.,  23.,  24.,   3.,  26.,  41.,  11.,   9.,\n",
      "        24.,  41.,  14.,  17.,  28.,  21.,   6.,  16.,  21.,  10.,  22.,\n",
      "        16.,  10.,  12.,  10.,  21.,  24.,  19.,  17.,   3.,  30.,  22.,\n",
      "        18.,  28.,  13.,  16.,  28.,  10.,   2.,  30.,  13.,  14.,  12.,\n",
      "         7.,  15.,   9.,  21.,  17.,  15.,  18.,  10.,  17.,  11.,  10.,\n",
      "         6.,  15.,   6.,   7.,   8.,  20.,   7.,  14.,  12.,   9.,  19.,\n",
      "        19.,  12.,  19.,   8.,   7.,  12.,  12.,  13.,   4.,  16.,   9.,\n",
      "         7.,  15.,  11.,   8.,   8.,  12.,   6.,  16.,   6.,   5.,  12.,\n",
      "        10.,  24.,   8.,  16.,   6.,  23.,   8.,  21.,  23.,   7.,  10.,\n",
      "        13.,   8.,   6.,   6.,  10.,   9.,  20.,  21.,   7.,  13.,   9.,\n",
      "        17.,   2.,  17.,  11.,  10.,  12.,  13.,  11.,   5.,  17.,  13.,\n",
      "        14.,  19.,  10.,  16.,   5.,  13.,   3.,   5.,   9.,  19.,  21.,\n",
      "        10.,  18.,  16.,   4.,  11.,  18.,  10.,   5.,  21.,   7.,  21.,\n",
      "        12.,  13.,   5.,   6.,  24.,  20.,  12.,  26.,  43.,  56.,  85.,\n",
      "        44.,  96.,  24.,  56.,  51.,  53.,  65.,  49.,  73.,  44.,  65.,\n",
      "        31.,  55.,  43.,  31.,  40.,  37.,  33.,  39.,  41.,  72.,  62.,\n",
      "        42.,  35.,  32.,  30.,  45.,  54.,  44.,  42.,  51.,  86.,  84.,\n",
      "       101., 109., 114.,  43.,  52.,  45.,  85.,  94.,  67.,  83.,  38.,\n",
      "        49.,  42.,  54.,  87., 114.,  58.,  61.,  38.,  43.,  66.,  94.,\n",
      "        48.,  60.,  38.,  41.,  37.,  43.,  57.,  56.,  69.,  85.,  16.,\n",
      "         7.,  20.,  12.,   9.,  10.,  10.,  32.,  16.,   7.,  17.,  21.,\n",
      "         8.,  20.,  13.,  14.,  16.,  11.,  11.,  11.,   7.,  10.,  16.,\n",
      "        22.,  78.,  23.,  74.,  25.,  19.,  14.,  35.,  76.,  39.,  41.,\n",
      "        26.,  16.,  10.,  36.,  20.,  39.,  32.,  18.,  14.,  19.,  20.,\n",
      "        28.,  30.,  18.,  19.,  22.,  32.]), 'y_test_seq': array([  19.,   24.,   39.,   28.,   21.,   26.,   21.,   16.,   41.,\n",
      "         45.,   45.,   26.,   30.,   42.,   37.,  192.,   81.,   76.,\n",
      "         37.,   71.,   66.,   46.,   74.,   58.,   60.,   34.,   37.,\n",
      "         71.,   49.,   38.,   66.,   58.,   54.,   55.,   63.,   64.,\n",
      "         80.,   65.,  106.,   60.,   77.,   93.,   68.,   84.,   57.,\n",
      "         77.,   37.,   66.,   52.,   70.,   53.,   72.,   66.,   51.,\n",
      "         54.,  103.,   77.,   81.,   72.,   76.,   51.,   69.,   54.,\n",
      "        115.,  135.,   84.,   57.,   45.,   76.,   81.,  112.,   92.,\n",
      "        123.,   85.,   57.,   76.,   86.,  105.,  115.,  127.,  129.,\n",
      "         67.,   75.,   85.,   72.,  145.,  129.,  116.,   75.,   75.,\n",
      "        137.,  101.,  118.,   73.,    4.,   41.,   43.,  100.,  155.,\n",
      "        211.,  162.,   15.,  293.,  244.,  232.,  312.,  610.,  528.,\n",
      "        394.,  312.,  432.,  481.,  586.,  730.,  643.,  600.,  375.,\n",
      "        479.,  440.,  532.,  926.,  806.,  541.,  353.,  508.,  488.,\n",
      "        629.,  842.,  951.,  810.,  425.,  538.,  711.,  601., 1091.,\n",
      "        930.,  642.,  435.,  584.,  637.,  687.,  845.,  751.,  630.,\n",
      "        440.,  583.,  654.,  687.,  955.,  929.,  661.,  427.,  487.,\n",
      "        625.,  658.,  893.,  961.,  710.,  497.,  582.,  693.,  845.,\n",
      "       1084., 1015.,  777.,  497.,  613.,  641.,  692., 1085., 1036.,\n",
      "        730.,  443.,  533.,  683.,  700., 1043.,  946.,  806.,  452.,\n",
      "        651.,  615.,  783.,  980.,  888.,  704.,  511.,  725.,  955.,\n",
      "        723.,  772.,  724.,  693.,  774.,  469.,  603.,  662., 1032.,\n",
      "        941.,  739.,  567.,  641.,  760.,  747., 1198.,  965.,  829.,\n",
      "        492.,  677.,  659.,  753., 1126., 1010.,  814.,  484.,  620.,\n",
      "        631.,  717., 1081.,  924.,  773.,  526.,  820.,  881.,  885.,\n",
      "       1345.,  975.,  828.,  591.,  724.,  928., 1048., 1019.,  874.,\n",
      "        910.,   38.]), 'n_features': 2, 'n_sequences': 1177}, 'Salads & Greens': {'X_train_seq': array([[[ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        ...,\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643]],\n",
      "\n",
      "       [[ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        ...,\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643]],\n",
      "\n",
      "       [[ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        ...,\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643],\n",
      "        [ 2.        ,  0.75243643]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.44291979],\n",
      "        ...,\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.47807733],\n",
      "        [ 9.        , -0.40776226]],\n",
      "\n",
      "       [[ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.44291979],\n",
      "        ...,\n",
      "        [ 9.        , -0.47807733],\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.47807733]],\n",
      "\n",
      "       [[ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.47807733],\n",
      "        ...,\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.47807733],\n",
      "        [ 9.        , -0.5483924 ]]]), 'X_test_seq': array([[[ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.47807733],\n",
      "        [ 9.        , -0.5483924 ],\n",
      "        ...,\n",
      "        [ 9.        , -0.47807733],\n",
      "        [ 9.        , -0.5483924 ],\n",
      "        [ 9.        , -0.40776226]],\n",
      "\n",
      "       [[ 9.        , -0.47807733],\n",
      "        [ 9.        , -0.5483924 ],\n",
      "        [ 9.        , -0.40776226],\n",
      "        ...,\n",
      "        [ 9.        , -0.5483924 ],\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.44291979]],\n",
      "\n",
      "       [[ 9.        , -0.5483924 ],\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.44291979],\n",
      "        ...,\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.44291979]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 5.        ,  7.08079294],\n",
      "        [ 5.        ,  6.69406004],\n",
      "        [ 5.        ,  9.11993003],\n",
      "        ...,\n",
      "        [ 5.        ,  6.69406004],\n",
      "        [ 5.        ,  3.38925164],\n",
      "        [ 5.        ,  5.18228599]],\n",
      "\n",
      "       [[ 5.        ,  6.69406004],\n",
      "        [ 5.        ,  9.11993003],\n",
      "        [ 5.        ,  4.44397773],\n",
      "        ...,\n",
      "        [ 5.        ,  3.38925164],\n",
      "        [ 5.        ,  5.18228599],\n",
      "        [ 5.        ,  5.04165584]],\n",
      "\n",
      "       [[ 5.        ,  9.11993003],\n",
      "        [ 5.        ,  4.44397773],\n",
      "        [ 5.        ,  2.8618886 ],\n",
      "        ...,\n",
      "        [ 5.        ,  5.18228599],\n",
      "        [ 5.        ,  5.04165584],\n",
      "        [ 5.        ,  4.26819005]]]), 'y_train_seq': array([38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "       38., 38., 38., 38., 38., 38., 38., 38., 38.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  4.,  4.,\n",
      "        1.,  2.,  2.,  2.,  2.,  5.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
      "        3.,  3.,  3.,  3.,  3.,  3.,  3.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  4.,  1.,\n",
      "        1.,  1.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  1.,  2.,\n",
      "        2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  4.,  4.,  4.,\n",
      "        2.,  2.,  3.,  3.,  3.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,\n",
      "        1.,  1.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  3.,\n",
      "        2.,  2.,  2.,  6.,  6.,  1.,  2.,  2.,  2.,  2.,  1.,  2.,  2.,\n",
      "        3.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  3.,  1.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  3.,  2.,  2.,  2.,  1.,  3.,  3.,\n",
      "        3.,  3.,  3.,  3.,  1.,  1.,  5.,  5.,  3.,  3.,  2.,  2.,  2.,\n",
      "        2.,  2.,  1.,  1.,  1.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  2.,  2.,  2.,  3.,  3.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  3.,  3.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  1.,\n",
      "        2.,  1.,  1.,  2.,  4.,  1.,  1.,  2.,  2.,  4.,  2.,  1.,  1.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  3.,  3.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        3.,  3.,  3.,  1.,  3.,  4.,  4.,  1.,  2.,  2.,  1.,  4.,  2.,\n",
      "        2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  2.,  2.,  1.,  3.,  1.,\n",
      "        3.,  3.,  1.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  2.,  1.,  1.,  2.,  2.,  1.,  2.,  2.,  2.,  2.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  2.,  1.,  1.,  4.,  4.,  4.,  1.,\n",
      "        1.,  2.,  1.,  1.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  3.,  4.,  1.,  1.,  1.,  1.,  2.,  1.,  1.,  1.,\n",
      "        1.,  1.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,\n",
      "        1.,  2.,  2.,  6.,  6.,  6.,  6.,  6.,  3.,  7.,  3.,  1.,  1.,\n",
      "        1.,  4.,  8.,  2.,  2.,  2.,  1.,  1.,  1.,  2.,  3.,  1.,  4.,\n",
      "        5.,  2.,  1.,  4.,  4.,  2.,  2.,  3.,  1.,  1.,  2.,  5.,  2.,\n",
      "        3.,  3.,  3.,  2.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  2.,  2.,  2.,  1.,  1.,  3.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        7.,  3.,  1.,  4.,  1.,  1.,  2.,  4.,  4.,  3.,  3.,  3.,  3.,\n",
      "        3.,  3.,  1.,  4.,  3.,  3.,  1.,  5.,  2.,  2.,  2.,  9.,  6.,\n",
      "        1.,  1.,  3.,  5.,  5.,  5.,  5.,  4.,  4.,  3.,  1.,  5.,  4.,\n",
      "        3.,  5.,  3.,  1.,  5.]), 'y_test_seq': array([  4.,   4.,   7.,   7.,   2.,   2.,   1.,   6.,   2.,   1.,   2.,\n",
      "         1.,   1.,   2.,   2.,   6.,   4.,   3.,   3.,  13.,  16.,   2.,\n",
      "         4.,   3.,   3.,   6.,   4.,   4.,   1.,   2.,   5.,   4.,   1.,\n",
      "         3.,   5.,   2.,   5.,   1.,   8.,   4.,   1.,   4.,   2.,   2.,\n",
      "         6.,   4.,   3.,   2.,   5.,   3.,   5.,   5.,   1.,   1.,   1.,\n",
      "         1.,   1.,   3.,   2.,   3.,  23.,  31.,  30.,  28.,   5.,   3.,\n",
      "         2.,  29.,  21.,  18.,  15.,   6.,  13.,   3.,  15.,  16.,  22.,\n",
      "        18.,   4.,   4.,   9.,  23.,   9.,  15.,  20.,   9.,  13.,   4.,\n",
      "         8.,  15.,  12.,  10.,  11.,   9.,  16.,   1.,   1.,  17.,  15.,\n",
      "         9.,   7.,  22.,  26.,  40.,  41.,  53., 101.,  83.,  26.,  71.,\n",
      "        57.,  97.,  91., 113.,  73.,  58.,  76., 106.,  85.,  84., 103.,\n",
      "        78.,  62.,  81., 129.,  98., 120., 137.,  92.,  87., 122., 135.,\n",
      "       165., 133., 144., 111.,  79., 155., 133., 153., 152., 148., 106.,\n",
      "        71.,  99., 103., 110.,  99., 148., 107.,  68., 132., 148., 170.,\n",
      "       158., 166., 120.,  91., 121., 183., 172., 161., 192., 111.,  88.,\n",
      "       143., 188., 165., 135., 237., 141.,  71., 142., 140., 181., 136.,\n",
      "       190., 138.,  58., 123., 166., 150., 164., 157., 139.,  47., 114.,\n",
      "       122., 145.,  89.,  81.,  77.,  56.,  81., 130., 146., 186., 167.,\n",
      "       105.,  96., 156., 172., 183., 172., 220., 137., 112., 150., 182.,\n",
      "       144., 184., 169., 109.,  53., 149., 158., 163., 195., 222., 205.,\n",
      "       118., 205., 228., 218., 207., 276., 143.,  98., 177., 215., 207.,\n",
      "       113., 164., 160., 138.,  46.]), 'n_features': 2, 'n_sequences': 1177}, 'Breakfast & Brunch': {'X_train_seq': array([[[ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        ...,\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672]],\n",
      "\n",
      "       [[ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        ...,\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672]],\n",
      "\n",
      "       [[ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        ...,\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672],\n",
      "        [ 2.        ,  1.03369672]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        , -0.23197458],\n",
      "        [ 9.        , -0.0561869 ],\n",
      "        [ 9.        , -0.30228965],\n",
      "        ...,\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.19681704],\n",
      "        [ 9.        ,  0.18991586]],\n",
      "\n",
      "       [[ 9.        , -0.0561869 ],\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.19681704],\n",
      "        ...,\n",
      "        [ 9.        , -0.19681704],\n",
      "        [ 9.        ,  0.18991586],\n",
      "        [ 9.        , -0.30228965]],\n",
      "\n",
      "       [[ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.19681704],\n",
      "        [ 9.        , -0.12650197],\n",
      "        ...,\n",
      "        [ 9.        ,  0.18991586],\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.33744719]]]), 'X_test_seq': array([[[ 9.        , -0.19681704],\n",
      "        [ 9.        , -0.12650197],\n",
      "        [ 9.        , -0.44291979],\n",
      "        ...,\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.33744719]],\n",
      "\n",
      "       [[ 9.        , -0.12650197],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.19681704],\n",
      "        ...,\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.23197458]],\n",
      "\n",
      "       [[ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.19681704],\n",
      "        [ 9.        , -0.30228965],\n",
      "        ...,\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.23197458],\n",
      "        [ 9.        , -0.26713211]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 5.        ,  4.44397773],\n",
      "        [ 5.        ,  3.10799136],\n",
      "        [ 5.        ,  4.97134077],\n",
      "        ...,\n",
      "        [ 5.        ,  3.21346396],\n",
      "        [ 5.        ,  9.54182047],\n",
      "        [ 5.        ,  7.15110801]],\n",
      "\n",
      "       [[ 5.        ,  3.10799136],\n",
      "        [ 5.        ,  4.97134077],\n",
      "        [ 5.        ,  8.3113067 ],\n",
      "        ...,\n",
      "        [ 5.        ,  9.54182047],\n",
      "        [ 5.        ,  7.15110801],\n",
      "        [ 5.        ,  8.97929989]],\n",
      "\n",
      "       [[ 5.        ,  4.97134077],\n",
      "        [ 5.        ,  8.3113067 ],\n",
      "        [ 5.        ,  0.61180629],\n",
      "        ...,\n",
      "        [ 5.        ,  7.15110801],\n",
      "        [ 5.        ,  8.97929989],\n",
      "        [ 5.        ,  3.45956672]]]), 'y_train_seq': array([46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46.,\n",
      "       46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46.,\n",
      "       46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46., 46.,\n",
      "       46., 46., 46., 46., 46.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
      "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
      "        5.,  5.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,\n",
      "        1.,  1.,  1.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
      "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  2.,  2.,  2.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  2.,  4.,  3., 11., 17., 21., 14.,\n",
      "        7., 10.,  7., 21., 30., 24., 14., 13., 10., 12., 10.,  9., 18.,\n",
      "       10., 14.,  3.,  8.,  8., 19., 14., 17.,  9.,  2.,  7., 13., 12.,\n",
      "       23., 12.,  9.,  4.,  5.,  8., 11., 13., 13.,  7.,  5.,  6., 10.,\n",
      "        9., 16., 17., 15., 23., 19., 16., 19., 23., 17., 11., 11., 12.,\n",
      "       18., 15., 20., 24.,  4.,  5., 15., 16., 17.,  8., 21., 17., 14.,\n",
      "       24., 11., 14., 27.,  9.,  8.,  8.,  8., 20., 13., 26., 14., 17.,\n",
      "       17., 14., 17., 14., 22., 15.,  9., 10., 16., 16., 16., 30., 27.,\n",
      "       15.,  9., 13., 10., 13., 31., 20.,  6.,  5., 16.,  8., 15., 11.,\n",
      "       11.,  4.,  8., 11., 22., 19., 21., 14., 11.,  6.,  6., 11., 14.,\n",
      "       26., 16.,  6.,  4.,  5.,  6., 19., 14., 27., 13.,  7., 13.,  4.,\n",
      "        8., 10., 16.,  9.,  3.,  7.,  7., 15., 13.,  8.,  5., 10.,  2.,\n",
      "       10.,  6., 20., 19.,  6.,  4., 11.,  4., 11., 26., 16.,  4.,  7.,\n",
      "        8.,  7.,  9., 13., 17., 11., 17., 18., 13., 19., 10., 19., 14.,\n",
      "        6.,  5.,  6.,  9., 24., 22.,  9.,  8.,  2.,  4.,  1., 19., 16.,\n",
      "        3.,  5.,  6.,  9., 16., 14., 29., 12.,  4.,  4.,  6.,  4., 29.,\n",
      "       32.,  4.,  3., 12.,  7.,  4., 19., 22., 11.,  1.,  5., 11., 11.,\n",
      "       21.,  8., 12.,  9.,  5.,  8.,  6.,  6., 14.,  7., 12., 15., 12.,\n",
      "       26., 15., 11.,  2.,  3., 10.,  1., 10., 14., 27.,  9.,  1.,  5.,\n",
      "        3.,  8.,  7., 27.,  9.,  1., 12.,  8.,  3., 30., 23.,  3.,  4.,\n",
      "       10.,  9.,  4., 12., 36.,  7.,  7.,  4.,  5.,  5., 17.,  9.,  5.,\n",
      "        9.,  5.,  6.,  6., 11., 11.,  5., 14.,  9.,  6.,  6., 23., 20.,\n",
      "        4.,  6., 11., 14., 11., 20., 15.,  6.,  9.,  7.,  9., 17., 19.,\n",
      "       24.,  2., 11.,  8.,  5., 10., 15., 19.,  7.,  5.,  9.,  5., 12.,\n",
      "       18., 18., 15.,  5.,  1.,  2., 11., 21., 18.,  9.,  9., 10.,  7.,\n",
      "        4., 20., 12.,  5., 12.,  7., 15., 23., 22., 23.,  6., 12., 11.,\n",
      "       10., 15., 18., 29., 12., 15.,  9.,  3.,  3., 27.,  9.,  9., 18.,\n",
      "       22., 14., 21., 14., 39., 18., 21.,  7., 10., 22., 22., 14., 12.,\n",
      "        5.,  5.,  4.,  8., 12., 17., 12.,  4.,  7., 15., 15., 22., 11.,\n",
      "        9.,  1.,  7.,  4., 14., 18., 17., 17.,  2., 15.,  3., 14.,  7.,\n",
      "        7., 13.,  4.,  8.,  6.,  6., 18., 29.,  8.,  3.,  5.,  5., 12.,\n",
      "       13., 16.,  5., 17., 16.,  5.,  2., 21., 11.,  8., 13., 10., 11.,\n",
      "        6., 17., 11., 13., 15., 10., 15., 17., 14.,  2., 14., 15., 10.,\n",
      "        7., 17., 33.,  7.,  9., 17.,  4., 23., 20., 11.,  9., 15., 10.,\n",
      "       14.,  5., 16., 21., 13., 25., 10., 25., 10.,  8., 18.,  6.,  9.,\n",
      "       19., 10.,  9., 16., 15.,  6.,  6.,  6., 10.,  7.,  9.,  6., 13.,\n",
      "        7.,  4., 22., 10.,  7., 12., 18.,  7.,  5.,  4., 15., 10., 20.,\n",
      "        5.,  4.,  2.,  7.,  7., 10., 15.,  8., 11., 13.,  4., 11.,  8.,\n",
      "       11., 22.,  8.,  7.,  7.]), 'y_test_seq': array([ 10.,   9.,  16.,  14.,   4.,   5.,   7.,  12.,   9.,  10.,  25.,\n",
      "        11.,  12.,  12.,  22.,  24.,  13.,  12.,   5.,   5.,   9.,   7.,\n",
      "        10.,  15.,  10.,   9.,  14.,  22.,  13.,  22.,  24.,  14.,   9.,\n",
      "        14.,   9.,   6.,   7.,  17.,  20.,  18.,   8.,   8.,  13.,  15.,\n",
      "        21.,  26.,   6.,   4.,   6.,   8.,  12.,  15.,  18.,   5.,   5.,\n",
      "         6.,   2.,  12.,  21.,  25.,  15.,   4.,   4.,  11.,   5.,  23.,\n",
      "        33.,   8.,  19.,  19.,   6.,   8.,  24.,  25.,   6.,  13.,   5.,\n",
      "        14.,   8.,  17.,  21.,   4.,   6.,  10.,   9.,   8.,  23.,  11.,\n",
      "         3.,   1.,   6.,   6.,   7.,   3.,  12.,  14.,   7.,  14.,   9.,\n",
      "        10.,  14.,  22.,  14.,  19.,  12.,  58.,  66.,  24.,  17.,  25.,\n",
      "        35.,  54.,  52.,  66.,  76.,  15.,  76.,  54.,  50.,  42.,  96.,\n",
      "        91.,  12.,  75.,  61.,  79.,  73., 128., 115.,  25., 110.,  45.,\n",
      "        92.,  99., 116., 100.,  19.,  81.,  49.,  84.,  93., 100., 100.,\n",
      "        37.,  58.,  75.,  75.,  97., 213., 291.,  32.,  91.,  80., 121.,\n",
      "        73., 149., 226.,  26.,  72.,  46., 108., 116., 181., 286.,  45.,\n",
      "       175., 240., 114., 103., 227., 290.,  59.,  98.,  66.,  83.,  73.,\n",
      "       236., 264.,  47., 100.,  88.,  76.,  85., 171., 295.,  20.,  75.,\n",
      "        98., 128., 230., 187., 215., 218.,  19.,  79.,  88.,  97., 209.,\n",
      "       232.,  30.,  93.,  72., 157., 126., 478., 614.,  28.,  93.,  81.,\n",
      "       132.,  84., 220., 262.,  28., 104., 127.,  92., 104., 227., 298.,\n",
      "        71.,  87., 117., 143., 105., 158., 253.,  34., 102., 110., 108.,\n",
      "       288., 220., 272., 115.,  67.]), 'n_features': 2, 'n_sequences': 1177}, 'Sushi & Asian': {'X_train_seq': array([[[ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        ...,\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498]],\n",
      "\n",
      "       [[ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        ...,\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498]],\n",
      "\n",
      "       [[ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        ...,\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498],\n",
      "        [ 2.        ,  1.77200498]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        ,  0.22507339],\n",
      "        [ 9.        , -0.23197458],\n",
      "        [ 9.        , -0.26713211],\n",
      "        ...,\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.12650197],\n",
      "        [ 9.        , -0.30228965]],\n",
      "\n",
      "       [[ 9.        , -0.23197458],\n",
      "        [ 9.        , -0.26713211],\n",
      "        [ 9.        , -0.30228965],\n",
      "        ...,\n",
      "        [ 9.        , -0.12650197],\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.44291979]],\n",
      "\n",
      "       [[ 9.        , -0.26713211],\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.23197458],\n",
      "        ...,\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.44291979]]]), 'X_test_seq': array([[[ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.23197458],\n",
      "        [ 9.        , -0.33744719],\n",
      "        ...,\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.33744719]],\n",
      "\n",
      "       [[ 9.        , -0.23197458],\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.12650197],\n",
      "        ...,\n",
      "        [ 9.        , -0.44291979],\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.47807733]],\n",
      "\n",
      "       [[ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.12650197],\n",
      "        [ 9.        , -0.44291979],\n",
      "        ...,\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.47807733],\n",
      "        [ 9.        , -0.47807733]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 5.        ,  1.52590223],\n",
      "        [ 5.        ,  1.17432687],\n",
      "        [ 5.        ,  2.29936802],\n",
      "        ...,\n",
      "        [ 5.        ,  1.17432687],\n",
      "        [ 5.        ,  1.27979948],\n",
      "        [ 5.        ,  1.94779266]],\n",
      "\n",
      "       [[ 5.        ,  1.17432687],\n",
      "        [ 5.        ,  2.29936802],\n",
      "        [ 5.        ,  2.08842281],\n",
      "        ...,\n",
      "        [ 5.        ,  1.27979948],\n",
      "        [ 5.        ,  1.94779266],\n",
      "        [ 5.        ,  1.80716252]],\n",
      "\n",
      "       [[ 5.        ,  2.29936802],\n",
      "        [ 5.        ,  2.08842281],\n",
      "        [ 5.        ,  1.45558716],\n",
      "        ...,\n",
      "        [ 5.        ,  1.94779266],\n",
      "        [ 5.        ,  1.80716252],\n",
      "        [ 5.        ,  2.3696831 ]]]), 'y_train_seq': array([67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67.,\n",
      "       67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67.,\n",
      "       67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67.,\n",
      "       67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67., 67.,\n",
      "       67.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4., 20.,\n",
      "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 14., 14., 14.,\n",
      "        8.,  8.,  8.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  6.,  6.,  6.,\n",
      "        6.,  6.,  7.,  7.,  7.,  8.,  8.,  8.,  8.,  8.,  6.,  6.,  6.,\n",
      "        6.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
      "        4.,  1.,  1.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
      "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
      "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
      "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
      "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
      "        5.,  5.,  5.,  5.,  5.,  5.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
      "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  1.,  1.,\n",
      "        1.,  1.,  1.,  3.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  2.,  2.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  4.,  4.,  2.,  4.,  5.,  4.,  4.,\n",
      "        4.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,  3.,\n",
      "        3.,  1.,  5.,  2.,  2.,  1.,  2.,  2.,  1.,  2.,  2., 25., 23.,\n",
      "        8.,  6.,  6., 10., 10., 23., 10.,  9.,  8., 10.,  7., 13.,  4.,\n",
      "       13.,  8.,  4.,  4.,  7.]), 'y_test_seq': array([  3.,   3.,   2.,  10.,   1.,  11.,  11.,  11.,   8.,   4.,   4.,\n",
      "         2.,   4.,   5.,   3.,  22.,  10.,  10.,   1.,  16.,  12.,   1.,\n",
      "         7.,  12.,   9.,   2.,   4.,   2.,   2.,   6.,   5.,   2.,   9.,\n",
      "         4.,   6.,   9.,  15.,   5.,   4.,   5.,  10.,   6.,   7.,  19.,\n",
      "        11.,   8.,   1.,   2.,   5.,   8.,   4.,  18.,  11.,  17.,   1.,\n",
      "         3.,  31.,   8.,   9.,   4.,   2.,   5.,   4.,  10.,  13.,   6.,\n",
      "         7.,   7.,   2.,   5.,   6.,   7.,  10.,   2.,   4.,   6.,   7.,\n",
      "         9.,   7.,   7.,   7.,   7.,   3.,  10.,   7.,   6.,  10.,   6.,\n",
      "         7.,   4.,   7.,   8.,  16.,   5.,   5.,  16.,  12.,  10.,  22.,\n",
      "        13.,   6.,   8.,  40.,  34.,  46.,  90., 116., 112.,  80.,  41.,\n",
      "        63.,  75.,  65., 129., 110.,  88.,  43.,  65.,  83.,  41., 178.,\n",
      "       171.,  79.,  32.,  46.,  81.,  83., 127., 112., 103.,  21.,  58.,\n",
      "        96.,  61., 144., 146.,  79.,  26.,  50.,  68., 107., 100., 103.,\n",
      "        49.,  59.,  55., 148.,  70.,  98., 101.,  83.,  20.,  43.,  64.,\n",
      "        71.,  98., 104., 105.,  24.,  69.,  88., 102., 137., 103.,  84.,\n",
      "        39.,  36., 103.,  78., 147., 131.,  87.,  46.,  38.,  78.,  83.,\n",
      "       136., 112.,  67.,  29.,  49.,  68.,  83., 128., 107.,  69.,  89.,\n",
      "        64., 126.,  62.,  85.,  69.,  85.,  70.,  41.,  55.,  58., 138.,\n",
      "       138.,  97.,  46.,  53.,  95.,  64., 152., 137.,  88.,  47.,  35.,\n",
      "        65.,  79.,  95.,  97.,  48.,  73.,  36.,  34.,  62.,  76.,  75.,\n",
      "        51.,  52.,  33.,  60.,  50.,  82.,  76.,  58.,  53.,  30.,  50.,\n",
      "        53.,  72.,  68.,  84.,  84.]), 'n_features': 2, 'n_sequences': 1177}, 'Misc/Services': {'X_train_seq': array([[[ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        ...,\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ]],\n",
      "\n",
      "       [[ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        ...,\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ]],\n",
      "\n",
      "       [[ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        ...,\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ],\n",
      "        [ 2.        ,  2.3696831 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 9.        , -0.12650197],\n",
      "        [ 9.        , -0.23197458],\n",
      "        [ 9.        , -0.12650197],\n",
      "        ...,\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.37260472]],\n",
      "\n",
      "       [[ 9.        , -0.23197458],\n",
      "        [ 9.        , -0.12650197],\n",
      "        [ 9.        , -0.19681704],\n",
      "        ...,\n",
      "        [ 9.        , -0.33744719],\n",
      "        [ 9.        , -0.37260472],\n",
      "        [ 9.        , -0.26713211]],\n",
      "\n",
      "       [[ 9.        , -0.12650197],\n",
      "        [ 9.        , -0.19681704],\n",
      "        [ 9.        , -0.37260472],\n",
      "        ...,\n",
      "        [ 9.        , -0.37260472],\n",
      "        [ 9.        , -0.26713211],\n",
      "        [ 9.        , -0.40776226]]]), 'X_test_seq': array([[[ 9.        , -0.19681704],\n",
      "        [ 9.        , -0.37260472],\n",
      "        [ 9.        , -0.30228965],\n",
      "        ...,\n",
      "        [ 9.        , -0.26713211],\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.40776226]],\n",
      "\n",
      "       [[ 9.        , -0.37260472],\n",
      "        [ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.26713211],\n",
      "        ...,\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.26713211]],\n",
      "\n",
      "       [[ 9.        , -0.30228965],\n",
      "        [ 9.        , -0.26713211],\n",
      "        [ 9.        , -0.33744719],\n",
      "        ...,\n",
      "        [ 9.        , -0.40776226],\n",
      "        [ 9.        , -0.26713211],\n",
      "        [ 9.        , -0.19681704]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 5.        ,  7.29173816],\n",
      "        [ 5.        ,  5.56901888],\n",
      "        [ 5.        , 10.28012873],\n",
      "        ...,\n",
      "        [ 5.        ,  5.99090932],\n",
      "        [ 5.        ,  6.06122439],\n",
      "        [ 5.        ,  7.74878612]],\n",
      "\n",
      "       [[ 5.        ,  5.56901888],\n",
      "        [ 5.        , 10.28012873],\n",
      "        [ 5.        ,  7.74878612],\n",
      "        ...,\n",
      "        [ 5.        ,  6.06122439],\n",
      "        [ 5.        ,  7.74878612],\n",
      "        [ 5.        ,  6.09638193]],\n",
      "\n",
      "       [[ 5.        , 10.28012873],\n",
      "        [ 5.        ,  7.74878612],\n",
      "        [ 5.        ,  5.21744352],\n",
      "        ...,\n",
      "        [ 5.        ,  7.74878612],\n",
      "        [ 5.        ,  6.09638193],\n",
      "        [ 5.        ,  4.16271744]]]), 'y_train_seq': array([84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84., 84.,\n",
      "       84., 84., 84.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  1.,  1.,\n",
      "        1.,  1.,  1.,  1.,  1.,  1.,  9.,  9., 11.,  4.,  1.,  4., 10.,\n",
      "       14.,  7.,  8., 10., 10.,  9.,  7.,  5.,  4.,  7.,  7.,  7.,  5.,\n",
      "        7., 12.,  4.,  7., 12., 11., 10., 11., 11.,  1., 14.,  8., 10.,\n",
      "        6., 10.,  5.,  8.,  9., 16.,  5., 11., 10., 17., 12.,  7.,  8.,\n",
      "       10.,  1.,  7.,  7.,  8.,  7., 20., 12., 10.,  5.,  6.,  8.,  8.,\n",
      "        8.,  2., 10.,  5.,  6.,  5.,  9.,  3.,  5.,  7.,  5.,  4.,  5.,\n",
      "        2.,  8., 11.,  5.,  8.,  6.,  8.,  4.,  3., 12.,  4., 10.,  5.,\n",
      "        5.,  5.,  8., 13., 13.,  5.,  5.,  1.,  2.,  1.,  3., 10.,  9.,\n",
      "       11.,  3., 12., 11.,  4.,  3.,  6.,  7.,  5.,  6.,  9., 10.,  1.,\n",
      "        8.,  9.,  2.,  4.,  9.,  3.,  6.,  6.,  2., 12.,  4.,  8.,  3.,\n",
      "        8.,  8.,  5.,  5.,  6.,  8.,  7.,  3.,  3.,  7.,  2.,  4.,  5.,\n",
      "        4., 11.,  3.,  5.,  5.,  6.,  5.,  1.,  2.,  3.,  7.,  3.,  3.,\n",
      "        2.,  6.,  2.,  4.,  5.,  5.,  5.,  6.,  5.,  4.,  7.,  5.,  3.,\n",
      "        4.,  4.,  3.,  9.,  3.,  3.,  5.,  5.,  6.,  2.,  8.,  6.,  6.,\n",
      "        2.,  8.,  5.,  1.,  7.,  2.,  9.,  4.,  8.,  1., 10.,  4.,  6.,\n",
      "        8.,  5.,  4.,  5.,  3.,  1.,  3.,  4.,  1.,  4.,  5.,  3.,  2.,\n",
      "        6.,  2.,  2.,  7.,  3.,  6.,  6.,  4.,  8.,  1.,  3.,  8.,  3.,\n",
      "        7.,  3.,  3.,  3.,  5.,  5.,  2.,  4.,  7.,  2.,  4.,  5.,  3.,\n",
      "        2.,  3.,  3.,  1.,  1.,  6.,  4.,  5.,  6.,  6.,  2.,  2.,  5.,\n",
      "        8.,  1.,  5.,  5.,  8., 11.,  5., 23.,  2., 13., 13., 10.,  4.,\n",
      "       13., 15.,  9.,  2.,  7.,  1.,  5.,  5.,  2., 12., 11.,  1.,  6.,\n",
      "        4., 10., 13.,  8.,  9., 10.,  8., 12., 12., 11.,  8.,  3.,  4.,\n",
      "        3., 11.,  5.,  6.,  5.,  5.,  3.,  8.,  4., 12., 10.,  6.,  5.,\n",
      "       13.,  2., 12.,  6.,  3.,  3.,  6.,  5.,  1.,  2., 12.,  8., 15.,\n",
      "        6.,  2.,  8.,  2., 15.,  7., 10.,  3., 13.,  4., 22.,  8.,  6.,\n",
      "       13.,  6.,  2.,  6., 11.,  6.,  8.,  7.,  8.,  6., 11.,  7.,  7.,\n",
      "        4.,  7., 10.,  8.,  7.,  8., 11.,  4.,  6.,  4., 12.,  9., 19.,\n",
      "       14.,  9., 18.,  5., 12., 13., 10., 13., 11.,  6.,  8.,  9.,  7.,\n",
      "        7.,  6.,  9.,  5.,  5.]), 'y_test_seq': array([  9.,  11.,  15.,  17.,   2.,   2.,  10.,   5.,   8.,  14.,   8.,\n",
      "        13.,   7.,  12.,  12.,  13.,  10.,  10.,   5.,   9.,  12.,  11.,\n",
      "        12.,   7.,   6.,   8.,   4.,   9.,   9.,  11.,  13.,   7.,   1.,\n",
      "         4.,   6.,   4.,  14.,   7.,   6.,   8.,   8.,   7.,  14.,   4.,\n",
      "        19.,  10.,  12.,  11.,  12.,  16.,  11.,  12.,   9.,   9.,  18.,\n",
      "        14.,  19.,   4.,  10.,  10.,  13.,  18.,  20.,  19.,  16.,   9.,\n",
      "         7.,  19.,  18.,  17.,  30.,  28.,  16.,  11.,   8.,  19.,  18.,\n",
      "        21.,  20.,   9.,  18.,  15.,  33.,  27.,  39.,  58.,  29.,  36.,\n",
      "        16.,  17.,  27.,  21.,  31.,   7.,   3.,  13.,   3.,  20.,  16.,\n",
      "        38.,  22.,  29.,  48.,  70., 101.,  96., 129., 167.,  84.,  73.,\n",
      "       120., 132., 110., 230., 182., 166.,  69., 141.,  98., 112., 215.,\n",
      "       183., 167.,  71., 129., 114., 164., 223., 200., 158.,  87., 134.,\n",
      "       151., 134., 282., 184., 180.,  83., 153., 147., 178., 190., 196.,\n",
      "       127.,  97., 128., 176., 149., 231., 197., 124.,  92., 157., 139.,\n",
      "       133., 233., 219., 136.,  83., 153., 155., 197., 316., 263., 187.,\n",
      "        99., 149., 169., 186., 246., 230., 121., 132., 160., 150., 160.,\n",
      "       272., 227., 232., 116., 177., 148., 165., 264., 257., 204., 115.,\n",
      "       175., 222., 175., 178., 220., 134., 114., 151., 155., 164., 236.,\n",
      "       315., 186., 141., 160., 160., 174., 277., 248., 161., 149., 161.,\n",
      "       127., 189., 281., 256., 157., 119., 147., 146., 153., 212., 243.,\n",
      "       158., 137., 203., 224., 175., 309., 237., 165., 151., 182., 187.,\n",
      "       189., 237., 190., 135.,  13.]), 'n_features': 2, 'n_sequences': 1177}}\n",
      "\n",
      "ðŸ“Š SEQUENCE SUMMARY BY ITEM:\n",
      "   Beverages:\n",
      "      â”œâ”€ Train sequences: 941\n",
      "      â”œâ”€ Test sequences: 236\n",
      "      â”œâ”€ Sequence shape: (10, 2)\n",
      "      â””â”€ Total sequences: 1177\n",
      "   Other/Uncategorized:\n",
      "      â”œâ”€ Train sequences: 941\n",
      "      â”œâ”€ Test sequences: 236\n",
      "      â”œâ”€ Sequence shape: (10, 2)\n",
      "      â””â”€ Total sequences: 1177\n",
      "   Sides & Snacks:\n",
      "      â”œâ”€ Train sequences: 941\n",
      "      â”œâ”€ Test sequences: 236\n",
      "      â”œâ”€ Sequence shape: (10, 2)\n",
      "      â””â”€ Total sequences: 1177\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: PREPARE SEQUENCES FOR LSTM MODELS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: PREPARE SEQUENCES FOR LSTM MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def create_sequences(data, lookback=10):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM training.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array\n",
    "        Input data (features only)\n",
    "    lookback : int\n",
    "        Number of previous timesteps to use as input\n",
    "    Returns:\n",
    "    --------\n",
    "    X : array of shape (n_sequences, lookback, n_features)\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    for i in range(len(data) - lookback):\n",
    "        X.append(data[i:i + lookback])\n",
    "    return np.array(X)\n",
    "\n",
    "# Dictionary to store LSTM sequence data\n",
    "lstm_sequence_dict = {}\n",
    "lookback_window = 10  # Use 10 days of history to predict next day\n",
    "\n",
    "print(f\"\\nðŸ”„ Creating sequences (lookback window: {lookback_window} days)...\")\n",
    "\n",
    "for category in categories:\n",
    "    # Get data for this item\n",
    "    item_df = item_sales_filled[item_sales_filled['category_name'] == category].copy()\n",
    "    \n",
    "    # Create feature matrix (use the same 5 features as XGBoost after scaling)\n",
    "    features_for_lstm = ['month']\n",
    "    X_lstm = item_df[features_for_lstm].copy().values\n",
    "    \n",
    "    # Add the scaled quantity as 5th feature\n",
    "    qty_scaled_list = []\n",
    "    scaler_temp = scaler_price_dict[item]\n",
    "    for qty in item_df['quantity_sold'].values:\n",
    "        qty_scaled = float(scaler_temp.transform([[qty]])[0][0])\n",
    "        qty_scaled_list.append(qty_scaled)\n",
    "    \n",
    "    X_lstm = np.column_stack([X_lstm, qty_scaled_list])\n",
    "    y = item_df['quantity_sold'].values\n",
    "    \n",
    "    # Create sequences\n",
    "    X_seq = create_sequences(X_lstm, lookback=lookback_window)\n",
    "    y_seq = y[lookback_window:]  # Skip first lookback_window values\n",
    "    \n",
    "    # Train/test split (80/20)\n",
    "    split_idx = int(len(X_seq) * 0.8)\n",
    "    X_train_seq = X_seq[:split_idx]\n",
    "    X_test_seq = X_seq[split_idx:]\n",
    "    y_train_seq = y_seq[:split_idx]\n",
    "    y_test_seq = y_seq[split_idx:]\n",
    "    \n",
    "    # Store sequence data\n",
    "    lstm_sequence_dict[category] = {\n",
    "        'X_train_seq': X_train_seq,\n",
    "        'X_test_seq': X_test_seq,\n",
    "        'y_train_seq': y_train_seq,\n",
    "        'y_test_seq': y_test_seq,\n",
    "        'n_features': X_lstm.shape[1],\n",
    "        'n_sequences': len(X_seq)\n",
    "    }\n",
    "\n",
    "print(lstm_sequence_dict)\n",
    "\n",
    "print(f\"\\nðŸ“Š SEQUENCE SUMMARY BY ITEM:\")\n",
    "for category in categories[:3]:\n",
    "    data = lstm_sequence_dict[category]\n",
    "    print(f\"   {category}:\")\n",
    "    print(f\"      â”œâ”€ Train sequences: {len(data['X_train_seq'])}\")\n",
    "    print(f\"      â”œâ”€ Test sequences: {len(data['X_test_seq'])}\")\n",
    "    print(f\"      â”œâ”€ Sequence shape: ({lookback_window}, {data['n_features']})\")\n",
    "    print(f\"      â””â”€ Total sequences: {data['n_sequences']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bbbdb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 6: BUILD AND TRAIN PER-ITEM LSTM MODELS (ENHANCED)\n",
      "======================================================================\n",
      "\n",
      "ðŸ”„ Building and training 10 LSTM models...\n",
      "   Model Architecture:\n",
      "   â”œâ”€ LSTM 256 + BatchNorm + Dropout(0.15)\n",
      "   â”œâ”€ LSTM 128 + BatchNorm + Dropout(0.15)\n",
      "   â”œâ”€ LSTM 64 + BatchNorm + Dropout(0.2)\n",
      "   â”œâ”€ Dense 64 + Dropout(0.2)\n",
      "   â”œâ”€ Dense 32\n",
      "   â””â”€ Output: Dense 1\n",
      "   Regularization: L1=1e-5, L2=1e-4\n",
      "   Optimizer: Adam(lr=0.0002, clipnorm=1.0)\n",
      "\n",
      "   Training Beverages... âœ“\n",
      "\n",
      "   Training Other/Uncategorized... âœ“\n",
      "\n",
      "   Training Sides & Snacks... âœ“\n",
      "\n",
      "   Training Desserts & Sweets... âœ“\n",
      "\n",
      "   Training Handhelds... âœ“\n",
      "\n",
      "   Training Main Courses... âœ“\n",
      "\n",
      "   Training Salads & Greens... âœ“\n",
      "\n",
      "   Training Breakfast & Brunch... âœ“\n",
      "\n",
      "   Training Sushi & Asian... âœ“\n",
      "\n",
      "   Training Misc/Services... âœ“\n",
      "\n",
      "âœ“ All LSTM models trained successfully!\n",
      "\n",
      "======================================================================\n",
      "PER-ITEM LSTM MODEL PERFORMANCE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Beverages:\n",
      "   â”œâ”€ Train RÂ²: 0.8379\n",
      "   â”œâ”€ Test RÂ²: 0.0325\n",
      "   â”œâ”€ RMSE: 2989.43 units\n",
      "   â”œâ”€ MAE: 1973.67 units\n",
      "   â””â”€ MAPE: 52.15%\n",
      "\n",
      "Breakfast & Brunch:\n",
      "   â”œâ”€ Train RÂ²: 0.7472\n",
      "   â”œâ”€ Test RÂ²: 0.0480\n",
      "   â”œâ”€ RMSE: 83.86 units\n",
      "   â”œâ”€ MAE: 44.27 units\n",
      "   â””â”€ MAPE: 64.21%\n",
      "\n",
      "Desserts & Sweets:\n",
      "   â”œâ”€ Train RÂ²: 0.6425\n",
      "   â”œâ”€ Test RÂ²: -3.3332\n",
      "   â”œâ”€ RMSE: 866.90 units\n",
      "   â”œâ”€ MAE: 507.91 units\n",
      "   â””â”€ MAPE: 103.18%\n",
      "\n",
      "Handhelds:\n",
      "   â”œâ”€ Train RÂ²: 0.4249\n",
      "   â”œâ”€ Test RÂ²: 0.4107\n",
      "   â”œâ”€ RMSE: 450.70 units\n",
      "   â”œâ”€ MAE: 301.00 units\n",
      "   â””â”€ MAPE: 57.65%\n",
      "\n",
      "Main Courses:\n",
      "   â”œâ”€ Train RÂ²: 0.4784\n",
      "   â”œâ”€ Test RÂ²: -0.5280\n",
      "   â”œâ”€ RMSE: 439.52 units\n",
      "   â”œâ”€ MAE: 314.97 units\n",
      "   â””â”€ MAPE: 68.03%\n",
      "\n",
      "Misc/Services:\n",
      "   â”œâ”€ Train RÂ²: 0.9807\n",
      "   â”œâ”€ Test RÂ²: 0.1446\n",
      "   â”œâ”€ RMSE: 80.73 units\n",
      "   â”œâ”€ MAE: 56.83 units\n",
      "   â””â”€ MAPE: 55.52%\n",
      "\n",
      "Other/Uncategorized:\n",
      "   â”œâ”€ Train RÂ²: 0.7927\n",
      "   â”œâ”€ Test RÂ²: 0.2750\n",
      "   â”œâ”€ RMSE: 4231.42 units\n",
      "   â”œâ”€ MAE: 2784.50 units\n",
      "   â””â”€ MAPE: 50.85%\n",
      "\n",
      "Salads & Greens:\n",
      "   â”œâ”€ Train RÂ²: 0.1380\n",
      "   â”œâ”€ Test RÂ²: -0.3323\n",
      "   â”œâ”€ RMSE: 81.77 units\n",
      "   â”œâ”€ MAE: 58.98 units\n",
      "   â””â”€ MAPE: 67.10%\n",
      "\n",
      "Sides & Snacks:\n",
      "   â”œâ”€ Train RÂ²: 0.3414\n",
      "   â”œâ”€ Test RÂ²: -0.8387\n",
      "   â”œâ”€ RMSE: 265.68 units\n",
      "   â”œâ”€ MAE: 189.77 units\n",
      "   â””â”€ MAPE: 76.67%\n",
      "\n",
      "Sushi & Asian:\n",
      "   â”œâ”€ Train RÂ²: 0.9635\n",
      "   â”œâ”€ Test RÂ²: 0.4002\n",
      "   â”œâ”€ RMSE: 33.51 units\n",
      "   â”œâ”€ MAE: 21.52 units\n",
      "   â””â”€ MAPE: 48.06%\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: BUILD AND TRAIN PER-ITEM LSTM MODELS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 6: BUILD AND TRAIN PER-ITEM LSTM MODELS (ENHANCED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dictionary to store trained LSTM models and metrics\n",
    "lstm_models_dict = {}\n",
    "lstm_metrics = {}\n",
    "\n",
    "print(f\"\\nðŸ”„ Building and training {len(lstm_sequence_dict)} LSTM models...\")\n",
    "print(f\"   Model Architecture:\")\n",
    "print(f\"   â”œâ”€ LSTM 256 + BatchNorm + Dropout(0.15)\")\n",
    "print(f\"   â”œâ”€ LSTM 128 + BatchNorm + Dropout(0.15)\")\n",
    "print(f\"   â”œâ”€ LSTM 64 + BatchNorm + Dropout(0.2)\")\n",
    "print(f\"   â”œâ”€ Dense 64 + Dropout(0.2)\")\n",
    "print(f\"   â”œâ”€ Dense 32\")\n",
    "print(f\"   â””â”€ Output: Dense 1\")\n",
    "print(f\"   Regularization: L1=1e-5, L2=1e-4\")\n",
    "print(f\"   Optimizer: Adam(lr=0.0002, clipnorm=1.0)\")\n",
    "\n",
    "for category in categories:\n",
    "    data = lstm_sequence_dict[category]\n",
    "    \n",
    "    # Build LSTM model with exact user-provided architecture\n",
    "    model = Sequential([\n",
    "        LSTM(256, activation='relu', return_sequences=True,\n",
    "             kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "             recurrent_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "             input_shape=(lookback_window, data['n_features'])),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.15),\n",
    "        \n",
    "        LSTM(128, activation='relu', return_sequences=True,\n",
    "             kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "             recurrent_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.15),\n",
    "        \n",
    "        LSTM(64, activation='relu', return_sequences=False,\n",
    "             kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "             recurrent_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # Compile with optimized settings\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0002, clipnorm=1.0),\n",
    "        loss='mse',  # MSE for better precision on predictions\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n   Training {category}...\", end='')\n",
    "    \n",
    "    # Enhanced callbacks\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,            # Increased patience to allow more learning\n",
    "        restore_best_weights=True,\n",
    "        min_delta=0.0001        # Minimum improvement threshold\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,             # Reduce LR by 50% when plateau detected\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        data['X_train_seq'], data['y_train_seq'],\n",
    "        validation_split=0.2,\n",
    "        epochs=150,             # More epochs with early stopping\n",
    "        batch_size=16,          # Smaller batch size for better gradient updates\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "    print(\" âœ“\")\n",
    "    \n",
    "    # Store model\n",
    "    lstm_models_dict[category] = model\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(data['X_train_seq'], verbose=0).flatten()\n",
    "    y_pred_test = model.predict(data['X_test_seq'], verbose=0).flatten()\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    train_r2 = r2_score(data['y_train_seq'], y_pred_train)\n",
    "    test_r2 = r2_score(data['y_test_seq'], y_pred_test)\n",
    "    rmse = np.sqrt(mean_squared_error(data['y_test_seq'], y_pred_test))\n",
    "    mae = mean_absolute_error(data['y_test_seq'], y_pred_test)\n",
    "    mape = np.mean(np.abs((data['y_test_seq'] - y_pred_test) / (data['y_test_seq'] + 1))) * 100\n",
    "    \n",
    "    lstm_metrics[category] = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mape': mape,\n",
    "        'history': history\n",
    "    }\n",
    "\n",
    "print(\"\\nâœ“ All LSTM models trained successfully!\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"PER-ITEM LSTM MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for item in sorted(lstm_metrics.keys()):\n",
    "    metrics = lstm_metrics[item]\n",
    "    print(f\"\\n{item}:\")\n",
    "    print(f\"   â”œâ”€ Train RÂ²: {metrics['train_r2']:.4f}\")\n",
    "    print(f\"   â”œâ”€ Test RÂ²: {metrics['test_r2']:.4f}\")\n",
    "    print(f\"   â”œâ”€ RMSE: {metrics['rmse']:.2f} units\")\n",
    "    print(f\"   â”œâ”€ MAE: {metrics['mae']:.2f} units\")\n",
    "    print(f\"   â””â”€ MAPE: {metrics['mape']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc03808c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 7: LSTM WALKFORWARD PREDICTION FUNCTION\n",
      "======================================================================\n",
      "âœ“ LSTM walkforward prediction function created!\n",
      "Available items: Beverages, Other/Uncategorized, Sides & Snacks, Desserts & Sweets, Handhelds, Main Courses, Salads & Greens, Breakfast & Brunch, Sushi & Asian, Misc/Services\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: LSTM WALKFORWARD PREDICTION FUNCTION\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 7: LSTM WALKFORWARD PREDICTION FUNCTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def predict_next_day_lstm(current_date, category_name, verbose=True):\n",
    "    \"\"\"\n",
    "    Predict sales volume for the next day using LSTM model for a specific item.\n",
    "    Uses the trained per-item LSTM model and historical data.\n",
    "    \"\"\"\n",
    "    # Validate item\n",
    "    if category_name not in lstm_models_dict:\n",
    "        return {'error': f'Item {category_name} not in trained LSTM models. Available: {list(lstm_models_dict.keys())}'}\n",
    "    \n",
    "    # Convert to datetime if string\n",
    "    current_dt = pd.to_datetime(current_date)\n",
    "    next_dt = current_dt + pd.Timedelta(days=1)\n",
    "    \n",
    "    # Extract temporal features for next day\n",
    "    day_of_week = next_dt.dayofweek\n",
    "    is_weekend = 1 if next_dt.dayofweek in [5, 6] else 0\n",
    "    is_holiday = 1 if next_dt.date() in [d.date() for d in holidays_dates] else 0\n",
    "    month = next_dt.month\n",
    "    \n",
    "    # Get item data and last quantity sold\n",
    "    item_data = item_sales_filled[item_sales_filled['category_name'] == category_name]\n",
    "    if item_data.empty:\n",
    "        return {'error': f'No historical data for item {category_name}'}\n",
    "    \n",
    "    # Get recent quantity (last non-null or average of last 7 days)\n",
    "    recent_qty_series = item_data['quantity_sold'].dropna()\n",
    "    if len(recent_qty_series) == 0:\n",
    "        last_qty = 0.0\n",
    "    else:\n",
    "        last_qty = float(recent_qty_series.iloc[-1]) if not np.isnan(recent_qty_series.iloc[-1]) else float(recent_qty_series.tail(7).mean())\n",
    "    \n",
    "    # Scale the quantity using item-specific scaler\n",
    "    scaler = scaler_price_dict.get(category_name)\n",
    "    if scaler is None:\n",
    "        qty_scaled = last_qty\n",
    "    else:\n",
    "        qty_scaled = float(scaler.transform([[last_qty]])[0][0])\n",
    "    \n",
    "    # Get last lookback_window days for LSTM sequence\n",
    "    features_for_lstm = ['month']\n",
    "    historical_data = item_data.tail(lookback_window + 1)[features_for_lstm].values\n",
    "    \n",
    "    # Add scaled quantities to features\n",
    "    hist_qty_values = item_data.tail(lookback_window + 1)['quantity_sold'].values\n",
    "    hist_qty_scaled = [float(scaler.transform([[q]])[0][0]) for q in hist_qty_values]\n",
    "    \n",
    "    sequence_data = np.column_stack([historical_data, hist_qty_scaled])\n",
    "    \n",
    "    # Ensure we have exactly lookback_window timesteps\n",
    "    if len(sequence_data) < lookback_window:\n",
    "        # Pad with first row if not enough data\n",
    "        padding = np.tile(sequence_data[0], (lookback_window - len(sequence_data), 1))\n",
    "        sequence_data = np.vstack([padding, sequence_data])\n",
    "    else:\n",
    "        sequence_data = sequence_data[-lookback_window:]\n",
    "    \n",
    "    # Reshape for LSTM: (1, lookback_window, n_features)\n",
    "    sequence_data = sequence_data.reshape(1, lookback_window, sequence_data.shape[1])\n",
    "    \n",
    "    # Get LSTM model and metrics\n",
    "    model = lstm_models_dict[category_name]\n",
    "    metrics = lstm_metrics[category_name]\n",
    "    mae = metrics['mae']\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = float(model.predict(sequence_data, verbose=0)[0][0])\n",
    "    prediction = max(0, prediction)  # Ensure non-negative\n",
    "    \n",
    "    # Confidence interval\n",
    "    lower_bound = max(0, prediction - mae)\n",
    "    upper_bound = prediction + mae\n",
    "    safety_stock = prediction + mae\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nðŸ“… LSTM WALKFORWARD PREDICTION\")\n",
    "        print(f\"   Current Date: {current_dt.date()}\")\n",
    "        print(f\"   Predict For: {next_dt.date()}\")\n",
    "        print(f\"   Item: {category_name}\")\n",
    "        print(f\"\\nðŸ“Š FEATURES (Next Day):\")\n",
    "        print(f\"   â”œâ”€ Day of Week: {next_dt.day_name()}\")\n",
    "        print(f\"   â”œâ”€ Is Weekend: {'Yes' if is_weekend else 'No'}\")\n",
    "        print(f\"   â”œâ”€ Is Holiday: {'Yes' if is_holiday else 'No'}\")\n",
    "        print(f\"   â””â”€ Month: {next_dt.strftime('%B')}\")\n",
    "        print(f\"\\nðŸŽ¯ LSTM PREDICTION:\")\n",
    "        print(f\"   â”œâ”€ Expected Sales: {prediction:.0f} units\")\n",
    "        print(f\"   â”œâ”€ Confidence Interval: [{lower_bound:.0f}, {upper_bound:.0f}]\")\n",
    "        print(f\"   â”œâ”€ Safety Stock: {safety_stock:.0f} units\")\n",
    "        print(f\"   â”œâ”€ Error Margin (MAE): Â±{mae:.0f} units\")\n",
    "        print(f\"   â””â”€ Model RÂ² Score: {metrics['test_r2']:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'date': next_dt.date(),\n",
    "        'category_name': category_name,\n",
    "        'predicted_quantity': prediction,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound,\n",
    "        'safety_stock': safety_stock,\n",
    "        'confidence_margin': mae,\n",
    "        'model_r2': metrics['test_r2'],\n",
    "        'model_type': 'LSTM',\n",
    "        'features': {\n",
    "            'day_of_week': day_of_week,\n",
    "            'is_weekend': is_weekend,\n",
    "            'is_holiday': is_holiday,\n",
    "            'month': month,\n",
    "            'quantity_used': last_qty\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"âœ“ LSTM walkforward prediction function created!\")\n",
    "print(f\"Available items: {', '.join(list(lstm_models_dict.keys()))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccf3fada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 8: LSTM WALKFORWARD PREDICTIONS FOR TOP 5 ITEMS\n",
      "======================================================================\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Beverages\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 91 units\n",
      "   â”œâ”€ Confidence Interval: [0, 2065]\n",
      "   â”œâ”€ Safety Stock: 2065 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±1974 units\n",
      "   â””â”€ Model RÂ² Score: 0.0325\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Other/Uncategorized\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 80 units\n",
      "   â”œâ”€ Confidence Interval: [0, 2864]\n",
      "   â”œâ”€ Safety Stock: 2864 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±2784 units\n",
      "   â””â”€ Model RÂ² Score: 0.2750\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Sides & Snacks\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 24 units\n",
      "   â”œâ”€ Confidence Interval: [0, 214]\n",
      "   â”œâ”€ Safety Stock: 214 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±190 units\n",
      "   â””â”€ Model RÂ² Score: -0.8387\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Desserts & Sweets\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 63 units\n",
      "   â”œâ”€ Confidence Interval: [0, 570]\n",
      "   â”œâ”€ Safety Stock: 570 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±508 units\n",
      "   â””â”€ Model RÂ² Score: -3.3332\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Handhelds\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 57 units\n",
      "   â”œâ”€ Confidence Interval: [0, 358]\n",
      "   â”œâ”€ Safety Stock: 358 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±301 units\n",
      "   â””â”€ Model RÂ² Score: 0.4107\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Main Courses\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 30 units\n",
      "   â”œâ”€ Confidence Interval: [0, 345]\n",
      "   â”œâ”€ Safety Stock: 345 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±315 units\n",
      "   â””â”€ Model RÂ² Score: -0.5280\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Salads & Greens\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 23 units\n",
      "   â”œâ”€ Confidence Interval: [0, 82]\n",
      "   â”œâ”€ Safety Stock: 82 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±59 units\n",
      "   â””â”€ Model RÂ² Score: -0.3323\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Breakfast & Brunch\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 59 units\n",
      "   â”œâ”€ Confidence Interval: [15, 103]\n",
      "   â”œâ”€ Safety Stock: 103 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±44 units\n",
      "   â””â”€ Model RÂ² Score: 0.0480\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Sushi & Asian\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 67 units\n",
      "   â”œâ”€ Confidence Interval: [45, 89]\n",
      "   â”œâ”€ Safety Stock: 89 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±22 units\n",
      "   â””â”€ Model RÂ² Score: 0.4002\n",
      "\n",
      "ðŸ“… LSTM WALKFORWARD PREDICTION\n",
      "   Current Date: 2024-05-13\n",
      "   Predict For: 2024-05-14\n",
      "   Item: Misc/Services\n",
      "\n",
      "ðŸ“Š FEATURES (Next Day):\n",
      "   â”œâ”€ Day of Week: Tuesday\n",
      "   â”œâ”€ Is Weekend: No\n",
      "   â”œâ”€ Is Holiday: No\n",
      "   â””â”€ Month: May\n",
      "\n",
      "ðŸŽ¯ LSTM PREDICTION:\n",
      "   â”œâ”€ Expected Sales: 28 units\n",
      "   â”œâ”€ Confidence Interval: [0, 84]\n",
      "   â”œâ”€ Safety Stock: 84 units\n",
      "   â”œâ”€ Error Margin (MAE): Â±57 units\n",
      "   â””â”€ Model RÂ² Score: 0.1446\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Predicted Date</th>\n",
       "      <th>LSTM Prediction</th>\n",
       "      <th>Lower Bound</th>\n",
       "      <th>Upper Bound</th>\n",
       "      <th>Safety Stock</th>\n",
       "      <th>Confidence (Â±)</th>\n",
       "      <th>Model RÂ²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beverages</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2065.0</td>\n",
       "      <td>2065.0</td>\n",
       "      <td>1974.0</td>\n",
       "      <td>0.0325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Other/Uncategorized</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2864.0</td>\n",
       "      <td>2864.0</td>\n",
       "      <td>2784.0</td>\n",
       "      <td>0.2750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sides &amp; Snacks</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>-0.8387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Desserts &amp; Sweets</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>508.0</td>\n",
       "      <td>-3.3332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Handhelds</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>0.4107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Main Courses</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>-0.5280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Salads &amp; Greens</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>-0.3323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Breakfast &amp; Brunch</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>59.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sushi &amp; Asian</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>67.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.4002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Misc/Services</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.1446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Category Predicted Date  LSTM Prediction  Lower Bound  \\\n",
       "0            Beverages     2024-05-14             91.0          0.0   \n",
       "1  Other/Uncategorized     2024-05-14             80.0          0.0   \n",
       "2       Sides & Snacks     2024-05-14             24.0          0.0   \n",
       "3    Desserts & Sweets     2024-05-14             63.0          0.0   \n",
       "4            Handhelds     2024-05-14             57.0          0.0   \n",
       "5         Main Courses     2024-05-14             30.0          0.0   \n",
       "6      Salads & Greens     2024-05-14             23.0          0.0   \n",
       "7   Breakfast & Brunch     2024-05-14             59.0         15.0   \n",
       "8        Sushi & Asian     2024-05-14             67.0         45.0   \n",
       "9        Misc/Services     2024-05-14             28.0          0.0   \n",
       "\n",
       "   Upper Bound  Safety Stock  Confidence (Â±)  Model RÂ²  \n",
       "0       2065.0        2065.0          1974.0    0.0325  \n",
       "1       2864.0        2864.0          2784.0    0.2750  \n",
       "2        214.0         214.0           190.0   -0.8387  \n",
       "3        570.0         570.0           508.0   -3.3332  \n",
       "4        358.0         358.0           301.0    0.4107  \n",
       "5        345.0         345.0           315.0   -0.5280  \n",
       "6         82.0          82.0            59.0   -0.3323  \n",
       "7        103.0         103.0            44.0    0.0480  \n",
       "8         89.0          89.0            22.0    0.4002  \n",
       "9         84.0          84.0            57.0    0.1446  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 8: LSTM DEMONSTRATION - WALKFORWARD PREDICTIONS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 8: LSTM WALKFORWARD PREDICTIONS FOR TOP 5 ITEMS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get the latest date in data\n",
    "latest_date = item_sales_filled['date'].max()\n",
    "\n",
    "# Make LSTM predictions for top 5 items\n",
    "lstm_results_list = []\n",
    "for category in categories:\n",
    "    result = predict_next_day_lstm(latest_date, category, verbose=True)\n",
    "    lstm_results_list.append(result)\n",
    "# Create LSTM results dataframe\n",
    "lstm_results_df =  pd.DataFrame([\n",
    "    {\n",
    "        'Category': r['category_name'],\n",
    "        'Predicted Date': r['date'],\n",
    "        'LSTM Prediction': round(r['predicted_quantity'], 0),\n",
    "        'Lower Bound': round(r['lower_bound'], 0),\n",
    "        'Upper Bound': round(r['upper_bound'], 0),\n",
    "        'Safety Stock': round(r['safety_stock'], 0),\n",
    "        'Confidence (Â±)': round(r['confidence_margin'], 0),\n",
    "        'Model RÂ²': round(r['model_r2'], 4)\n",
    "    }\n",
    "    for r in lstm_results_list\n",
    "])\n",
    "    \n",
    "\n",
    "lstm_results_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f619b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 10: EXPORT CONSOLIDATED DICTIONARIES\n",
      "======================================================================\n",
      "\n",
      "ðŸ”„ Exporting all XGBoost models into one file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Exported 10 models to models/consolidated/xgb_models_dict.joblib\n",
      "\n",
      "ðŸ”„ Exporting all scalers into one file...\n",
      "âœ“ Exported 10 scalers to models/consolidated/scaler_price_dict.joblib\n",
      "\n",
      "ðŸ”„ Exporting LSTM models (Individual .h5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Exported 10 LSTM models\n"
     ]
    }
   ],
   "source": [
    "# STEP 10: EXPORT MODELS, PREDICTIONS, AND METRICS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 10: EXPORT XGBOOST AND LSTM MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import os\n",
    "\n",
    "os.makedirs('models/xgb_models', exist_ok=True)\n",
    "os.makedirs('models/lstm_models', exist_ok=True)\n",
    "os.makedirs('models/scalers', exist_ok=True)\n",
    "os.makedirs('output', exist_ok=True)\n",
    "os.makedirs('output/plots', exist_ok=True)\n",
    "\n",
    "# Export XGBoost models\n",
    "print(\"\\nðŸ”„ Exporting XGBoost models...\")\n",
    "for item, model in xgb_models_dict.items():\n",
    "    safe_name = item.replace(' ', '_').replace('/', '_')\n",
    "    model_path = os.path.join('models', 'xgb_models', f\"{safe_name}.joblib\")\n",
    "    joblib.dump(model, model_path)\n",
    "print(f\"âœ“ Exported {len(xgb_models_dict)} XGBoost models\")\n",
    "\n",
    "# Export LSTM models\n",
    "print(\"\\nðŸ”„ Exporting LSTM models...\")\n",
    "for item, model in lstm_models_dict.items():\n",
    "    safe_name = item.replace(' ', '_').replace('/', '_')\n",
    "    model_path = os.path.join('models', 'lstm_models', f\"{safe_name}_lstm.h5\")\n",
    "    model.save(model_path)\n",
    "print(f\"âœ“ Exported {len(lstm_models_dict)} LSTM models\")\n",
    "\n",
    "# Export scalers\n",
    "print(\"\\nðŸ”„ Exporting scalers...\")\n",
    "for item, scaler in scaler_price_dict.items():\n",
    "    safe_name = item.replace(' ', '_').replace('/', '_')\n",
    "    scaler_path = os.path.join('models', 'scalers', f\"{safe_name}_scaler.joblib\")\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "print(f\"âœ“ Exported {len(scaler_price_dict)} scalers\")\n",
    "\n",
    "# Export predictions from both models\n",
    "print(\"\\nðŸ”„ Exporting predictions...\")\n",
    "xgb_preds_list = []\n",
    "lstm_preds_list = []\n",
    "\n",
    "for category in categories:\n",
    "    # XGBoost predictions\n",
    "    if item in item_data_dict:\n",
    "        data = item_data_dict[item]\n",
    "        y_pred = xgb_models_dict[item].predict(data['X_test'])\n",
    "        xgb_preds_list.append(pd.DataFrame({\n",
    "            'item': item,\n",
    "            'actual': data['y_test'],\n",
    "            'predicted': y_pred,\n",
    "            'model': 'XGBoost'\n",
    "        }))\n",
    "    \n",
    "    # LSTM predictions\n",
    "    if item in lstm_sequence_dict:\n",
    "        data = lstm_sequence_dict[item]\n",
    "        y_pred = lstm_models_dict[item].predict(data['X_test_seq'], verbose=0).flatten()\n",
    "        lstm_preds_list.append(pd.DataFrame({\n",
    "            'item': item,\n",
    "            'actual': data['y_test_seq'],\n",
    "            'predicted': y_pred,\n",
    "            'model': 'LSTM'\n",
    "        }))\n",
    "\n",
    "xgb_preds_df = pd.concat(xgb_preds_list, ignore_index=True) if xgb_preds_list else pd.DataFrame()\n",
    "lstm_preds_df = pd.concat(lstm_preds_list, ignore_index=True) if lstm_preds_list else pd.DataFrame()\n",
    "all_preds_df = pd.concat([xgb_preds_df, lstm_preds_df], ignore_index=True)\n",
    "\n",
    "all_preds_df.to_csv(os.path.join('output', 'predictions.csv'), index=False)\n",
    "print(f\"âœ“ Saved {len(all_preds_df)} predictions to output/predictions.csv\")\n",
    "\n",
    "# Export metrics\n",
    "print(\"\\nðŸ”„ Exporting metrics...\")\n",
    "xgb_metrics_list = []\n",
    "lstm_metrics_list = []\n",
    "\n",
    "for item in model_metrics.keys():\n",
    "    m = model_metrics[item]\n",
    "    xgb_metrics_list.append({\n",
    "        'item': item,\n",
    "        'model': 'XGBoost',\n",
    "        'test_r2': m['test_r2'],\n",
    "        'rmse': m['rmse'],\n",
    "        'mae': m['mae'],\n",
    "        'mape': m['mape']\n",
    "    })\n",
    "\n",
    "for item in lstm_metrics.keys():\n",
    "    m = lstm_metrics[item]\n",
    "    lstm_metrics_list.append({\n",
    "        'item': item,\n",
    "        'model': 'LSTM',\n",
    "        'test_r2': m['test_r2'],\n",
    "        'rmse': m['rmse'],\n",
    "        'mae': m['mae'],\n",
    "        'mape': m['mape']\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(xgb_metrics_list + lstm_metrics_list)\n",
    "metrics_df.to_csv(os.path.join('output', 'metrics.csv'), index=False)\n",
    "print(f\"âœ“ Saved metrics to output/metrics.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPORT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"âœ“ XGBoost models: models/xgb_models/\")\n",
    "print(f\"âœ“ LSTM models: models/lstm_models/\")\n",
    "print(f\"âœ“ Scalers: models/scalers/\")\n",
    "print(f\"âœ“ Predictions: output/predictions.csv\")\n",
    "print(f\"âœ“ Metrics: output/metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5a12bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 11: VISUALIZATIONS\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metrics_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# RÂ² Comparison\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m metrics_pivot \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics_df\u001b[49m\u001b[38;5;241m.\u001b[39mpivot(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem\u001b[39m\u001b[38;5;124m'\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_r2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m metrics_pivot\u001b[38;5;241m.\u001b[39mplot(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m, ax\u001b[38;5;241m=\u001b[39maxes[\u001b[38;5;241m0\u001b[39m], color\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#1f77b4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#ff7f0e\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     12\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest RÂ² Comparison\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, fontweight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbold\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metrics_df' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1296x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 11: VISUALIZATIONS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 11: VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Plot 1: Model Performance Comparison (RÂ², RMSE, MAE)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# RÂ² Comparison\n",
    "metrics_pivot = metrics_df.pivot(index='item', columns='model', values='test_r2')\n",
    "metrics_pivot.plot(kind='bar', ax=axes[0], color=['#1f77b4', '#ff7f0e'])\n",
    "axes[0].set_title('Test RÂ² Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('RÂ² Score')\n",
    "axes[0].legend(title='Model')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# RMSE Comparison\n",
    "metrics_pivot = metrics_df.pivot(index='item', columns='model', values='rmse')\n",
    "metrics_pivot.plot(kind='bar', ax=axes[1], color=['#1f77b4', '#ff7f0e'])\n",
    "axes[1].set_title('RMSE Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('RMSE (units)')\n",
    "axes[1].legend(title='Model')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MAE Comparison\n",
    "metrics_pivot = metrics_df.pivot(index='item', columns='model', values='mae')\n",
    "metrics_pivot.plot(kind='bar', ax=axes[2], color=['#1f77b4', '#ff7f0e'])\n",
    "axes[2].set_title('MAE Comparison (Confidence Interval)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('MAE (units)')\n",
    "axes[2].legend(title='Model')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join('output', 'plots', 'model_comparison.png'), dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Saved model_comparison.png\")\n",
    "\n",
    "# Plot 2: Actual vs Predicted (for each model)\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, item in enumerate(categories):\n",
    "    # XGBoost\n",
    "    xgb_data = all_preds_df[(all_preds_df['item'] == item) & (all_preds_df['model'] == 'XGBoost')]\n",
    "    if not xgb_data.empty:\n",
    "        axes[idx].scatter(xgb_data['actual'], xgb_data['predicted'], alpha=0.6, label='XGBoost', s=50)\n",
    "    \n",
    "    # LSTM\n",
    "    lstm_data = all_preds_df[(all_preds_df['item'] == item) & (all_preds_df['model'] == 'LSTM')]\n",
    "    if not lstm_data.empty:\n",
    "        axes[idx].scatter(lstm_data['actual'], lstm_data['predicted'], alpha=0.6, label='LSTM', s=50)\n",
    "    \n",
    "    # y=x line\n",
    "    data_combined = pd.concat([xgb_data, lstm_data])\n",
    "    if not data_combined.empty:\n",
    "        mins = min(data_combined['actual'].min(), data_combined['predicted'].min())\n",
    "        maxs = max(data_combined['actual'].max(), data_combined['predicted'].max())\n",
    "        axes[idx].plot([mins, maxs], [mins, maxs], 'r--', alpha=0.5, linewidth=2)\n",
    "        axes[idx].set_xlim(mins - 5, maxs + 5)\n",
    "        axes[idx].set_ylim(mins - 5, maxs + 5)\n",
    "    \n",
    "    axes[idx].set_title(item, fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Actual')\n",
    "    axes[idx].set_ylabel('Predicted')\n",
    "    axes[idx].legend(fontsize=8)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join('output', 'plots', 'actual_vs_predicted.png'), dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Saved actual_vs_predicted.png\")\n",
    "\n",
    "# Plot 3: Metrics Heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "metrics_pivot = metrics_df.pivot(index='item', columns='model', values='test_r2')\n",
    "sns.heatmap(metrics_pivot, annot=True, fmt='.3f', cmap='RdYlGn', ax=ax, cbar_kws={'label': 'RÂ² Score'})\n",
    "ax.set_title('Model Performance Heatmap (RÂ² Scores)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join('output', 'plots', 'performance_heatmap.png'), dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ“ Saved performance_heatmap.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUALIZATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"âœ“ model_comparison.png â€” RÂ², RMSE, MAE by item and model\")\n",
    "print(\"âœ“ actual_vs_predicted.png â€” Scatter plots for all items\")\n",
    "print(\"âœ“ performance_heatmap.png â€” RÂ² scores heatmap\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bc4a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
